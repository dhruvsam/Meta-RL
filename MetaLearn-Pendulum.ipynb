{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow.contrib.layers as layers\n",
    "from mujoco_py import load_model_from_xml, MjSim, MjViewer\n",
    "import policies\n",
    "import value_functions as vfuncs\n",
    "import utils_pg as utils\n",
    "\n",
    "# Environment setup\n",
    "#env = \"CartPole-v0\"\n",
    "env=\"InvertedPendulum-v2\"\n",
    "\n",
    "#env = \"HalfCheetah-v2\"\n",
    "\n",
    "# discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "# observation_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "max_ep_len = 1000\n",
    "num_traj = 10\n",
    "#traj_length = max_ep_len*(observation_dim + 2)\n",
    "latent_size = 25\n",
    "use_baseline = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward network (multi-layer-perceptron, or mlp)\n",
    "\n",
    "def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "    '''\n",
    "    Build a feed forward network\n",
    "    '''\n",
    "    Input = mlp_input\n",
    "    with tf.variable_scope(scope):\n",
    "        # Dense Layers\n",
    "        for i in range(n_layers-1):\n",
    "            dense = tf.layers.dense(inputs = Input, units = size, activation = tf.nn.relu, bias_initializer=tf.constant_initializer(1.0))\n",
    "            Input = dense\n",
    "        # Fully Connected Layer\n",
    "        out = layers.fully_connected(inputs = Input, num_outputs = output_size, activation_fn=output_activation)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner():\n",
    "    def __init__(self, env, max_ep_len, num_traj,latent_size ):\n",
    "        self.env = gym.make(env)\n",
    "        self.discrete = isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.num_traj = num_traj\n",
    "        self.traj_length = self.max_ep_len*(self.observation_dim + 1 + self.env.action_space.shape[0]) # TO Change\n",
    "        self.use_baseline = True\n",
    "        self.latent_size = latent_size\n",
    "        self.feature_size = self.observation_dim + 1 + self.env.action_space.shape[0]\n",
    "        self.lr = 3e-2\n",
    "        self.num_layers = 1\n",
    "        self.layers_size = 16\n",
    "        self.num_mdps = 10\n",
    "        self.model = self.env.env.model\n",
    "        self.sim = MjSim(self.model)\n",
    "\n",
    "        # build model\n",
    "        self.ConstructGraph()\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        self.observation_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        if(self.discrete):\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.int32, shape=(None))\n",
    "            self.action_placeholder_exploit = tf.placeholder(tf.int32, shape=(None))\n",
    "        else:\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "            self.action_placeholder_exploit= tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "\n",
    "        self.baseline_target_placeholder = tf.placeholder(tf.float32, shape= None)\n",
    "        self.advantage_placeholder_explore = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        #self.encoder_input_placeholder = tf.placeholder(tf.float32, shape= (self.num_traj,self.traj_length))\n",
    "        self.encoder_input_placeholder = tf.placeholder(tf.float32, [None, None, self.feature_size])\n",
    "        self.decoder_input_placeholder = tf.placeholder(tf.float32, shape= (1,self.latent_size))\n",
    "        self.sequence_length_placeholder = tf.placeholder(tf.int32, [None, ])\n",
    "        \n",
    "        self.observation_placeholder_exploit = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        #TODO\n",
    "        self.advantage_placeholder_exploit = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "    def build_policy_explore(self, scope = \"policy_explore\"):\n",
    "        if (self.discrete):\n",
    "            self.action_logits = build_mlp(self.observation_placeholder_explore,self.action_dim,scope = scope,n_layers=self.num_layers,size = self.layers_size,output_activation=None)\n",
    "            self.explore_action = tf.multinomial(self.action_logits,1)\n",
    "            self.explore_action = tf.squeeze(self.explore_action, axis=1)\n",
    "            self.explore_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.action_logits, labels = self.action_placeholder_explore)\n",
    "\n",
    "        else:   \n",
    "            action_means = build_mlp(self.observation_placeholder_explore,self.action_dim,scope,n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"log_std\", [self.action_dim])\n",
    "            self.explore_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            self.explore_logprob =  mvn.log_prob(value = self.action_placeholder_explore, name='log_prob')\n",
    "\n",
    "    \n",
    "    \n",
    "    def build_policy_exploit(self, scope = \"policy_exploit\"):\n",
    "        if(self.discrete):\n",
    "            #self.exploit_action_logits = (tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W1) + self.d_B1), self.d_W2) + self.d_B2),self.d_W3) + self.d_B3)\n",
    "            self.exploit_action_logits = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            self.exploit_action = tf.multinomial(self.exploit_action_logits,1)\n",
    "            self.exploit_action = tf.squeeze(self.exploit_action, axis=1)\n",
    "            self.exploit_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.exploit_action_logits, labels = self.action_placeholder_exploit)\n",
    "        else:\n",
    "            action_means = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"exploit_log_prob\", [self.action_dim])\n",
    "            self.exploit_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            self.exploit_logprob =  mvn.log_prob(value = self.action_placeholder_exploit, name='exploit_log_prob')\n",
    "\n",
    "        #self.loss_grads_exploit = self.exploit_logprob * self.advantage_placeholder_exploit\n",
    "        \n",
    "    def NNEncoder(self, scope = \"NNEncoder\"):\n",
    "        self.Z = build_mlp(self.encoder_input_placeholder,self.latent_size,scope = scope,n_layers=3,size = 60,output_activation=None)\n",
    "    \n",
    " \n",
    "\n",
    "    # input [num_traj, length, features (obs + action + reward) ]\n",
    "    def LSTMEncoder(self, scope = \"LSTMEncoder\"):\n",
    "        self.hidden_size = 64\n",
    "        initializer = tf.random_uniform_initializer(-1, 1)\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, self.feature_size, initializer=initializer)\n",
    "        cell_out = tf.contrib.rnn.OutputProjectionWrapper(cell, self.latent_size)\n",
    "        self.output, _ = tf.nn.dynamic_rnn(cell_out,self.encoder_input_placeholder,self.sequence_length_placeholder,dtype=tf.float32,)\n",
    "        batch_size = tf.shape(self.output)[0]\n",
    "        max_length = tf.shape(self.output)[1]\n",
    "        out_size = int(self.output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (self.sequence_length_placeholder - 1)\n",
    "        flat = tf.reshape(self.output, [-1, out_size])\n",
    "        self.Z = tf.reduce_mean(tf.gather(flat, index), axis = 0)\n",
    "        \n",
    "    \n",
    "    def Decoder(self, decoder_out_dim, scope):\n",
    "        return build_mlp(self.decoder_input_placeholder,decoder_out_dim,scope = scope,n_layers=1,size = 16,output_activation=None)\n",
    "    \n",
    "    \n",
    "    def sample_paths_explore(self, env,mdp_num,Test = False, num_episodes = None):\n",
    "        paths = []\n",
    "        self.length = []\n",
    "        episode_rewards = []\n",
    "        self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        env.env.gravity = 5 + 2*mdp_num\n",
    "        for i in range(self.num_traj):\n",
    "            pad = False\n",
    "            state = env.reset()\n",
    "            new_states,states,new_actions, actions, rewards = [], [], [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                if (pad):\n",
    "                    states.append([0]*self.observation_dim)\n",
    "                    if(self.discrete):\n",
    "                        actions.append(0)\n",
    "                    else:\n",
    "                        actions.append([0]*self.action_dim)\n",
    "                    rewards.append(0)\n",
    "                else:\n",
    "                    states.append(state)\n",
    "                    new_states.append(state)\n",
    "                    action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    actions.append(action)\n",
    "                    new_actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "                    episode_reward += reward\n",
    "                    if (done or step == self.max_ep_len-1):\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        self.length.append(step + 1)\n",
    "                        pad = True  \n",
    "\n",
    "            #print(\"explore\",np.array(actions))\n",
    "            path = {\"new_obs\" : np.array(new_states),\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\"new_acs\" : np.array(new_actions),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def sample_paths_exploit(self, env,Z,mdp_num,Test = False, num_episodes = None):\n",
    "        self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        paths = []\n",
    "        num = 0\n",
    "        episode_rewards = []\n",
    "        for i in range(self.num_traj):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.sess.run(self.exploit_action, feed_dict={self.observation_placeholder_exploit : state[None], self.decoder_input_placeholder: Z})[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "            #print(\"exploit\",np.array(actions))\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    " \n",
    "        #print(\"exploit success: \", num)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def get_returns(self,paths, explore = False):\n",
    "        all_returns = []\n",
    "        m = 0\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            if(explore):\n",
    "                length = self.length[m]\n",
    "            else:\n",
    "                length = len(rewards)\n",
    "            #print(self.length[m])\n",
    "            for i in range(length):\n",
    "                path_returns = 0\n",
    "                k = 0\n",
    "                for j in range(i,length):\n",
    "                    path_returns = path_returns + rewards[j]*(1)**k\n",
    "                    k = k+1\n",
    "                returns.append(path_returns)\n",
    "            all_returns.append(returns)\n",
    "            m+=1\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "    \n",
    "    def stack_trajectories(self,paths):\n",
    "        trajectories = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            states = path[\"observation\"]\n",
    "            action = path[\"action\"]\n",
    "            SAR = []\n",
    "            for i in range(len(states)):\n",
    "                SAR.append(list(states[i]) + list(action[i]) + [rewards[i]])\n",
    "            trajectories.append(SAR)\n",
    "\n",
    "        return np.array(trajectories)\n",
    "    \n",
    "    def addBaseline(self):\n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "\n",
    "    def calculate_advantage(self,returns, observations):\n",
    "        if (self.use_baseline):\n",
    "            baseline = self.sess.run(self.baseline, {input_placeholder:observations})\n",
    "            adv = returns - baseline\n",
    "            adv = (adv - np.mean(adv))/np.std(adv)\n",
    "        else:\n",
    "            adv = returns\n",
    "        return adv\n",
    "    \n",
    "\n",
    "        \n",
    "    def ConstructGraph(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.add_placeholders()\n",
    "        \n",
    "        self.build_policy_explore()\n",
    "        self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "        self.loss_grads_explore = -self.explore_logprob * self.advantage_placeholder_explore\n",
    "        self.tvars_explore = tf.trainable_variables()\n",
    "        self.gradients_explore = tf.gradients(self.explore_policy_loss,self.tvars_explore)\n",
    "        \n",
    "        #self.addBaseline()\n",
    "        \n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=1, size = 16,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "        \n",
    "        #Encoder LSTM\n",
    "        self.LSTMEncoder()\n",
    "        \n",
    "        self.decoder_len = 16\n",
    "        #decoder weights\n",
    "        #self.d_W1 = self.Decoder(scope = \"W1\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W2 = self.Decoder(scope = \"W2\", decoder_out_dim = self.decoder_len*self.decoder_len)\n",
    "        #self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.decoder_len*action_dim)\n",
    "        self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.observation_dim*self.action_dim)\n",
    "        \n",
    "        #self.d_W1 = ((self.d_W1 - (tf.reduce_max(self.d_W1) + tf.reduce_min(self.d_W1))/2)/(tf.reduce_max(self.d_W1) - tf.reduce_min(self.d_W1)))*2 \n",
    "        #self.d_W2 = ((self.d_W2 - (tf.reduce_max(self.d_W2) + tf.reduce_min(self.d_W2))/2)/(tf.reduce_max(self.d_W2) - tf.reduce_min(self.d_W2)))*2 \n",
    "        #self.d_W3 = ((self.d_W3 - (tf.reduce_max(self.d_W3) + tf.reduce_min(self.d_W3))/2)/(tf.reduce_max(self.d_W3) - tf.reduce_min(self.d_W3)))*2 \n",
    "        \n",
    "        #self.d_W1 = tf.reshape(self.d_W1, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W2 = tf.reshape(self.d_W2, [self.decoder_len, self.decoder_len])\n",
    "        #self.d_W3 = tf.reshape(self.d_W3, [self.decoder_len, self.action_dim])\n",
    "        self.d_W3 = tf.reshape(self.d_W3, [self.observation_dim, self.action_dim])\n",
    "        \n",
    "        # decoder output bias\n",
    "        #self.d_B1 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B1\"), [self.decoder_len])\n",
    "        #self.d_B2 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B2\"), [self.decoder_len])\n",
    "        self.d_B3 = tf.reshape(self.Decoder(decoder_out_dim = self.action_dim, scope = \"B3\"), [self.action_dim])\n",
    "        #self.d_B1 = ((self.d_B1 - (tf.reduce_max(self.d_B1) + tf.reduce_min(self.d_B1))/2)/(tf.reduce_max(self.d_B1) - tf.reduce_min(self.d_B1)))*2 \n",
    "        #self.d_B2 = ((self.d_B2 - (tf.reduce_max(self.d_B2) + tf.reduce_min(self.d_B2))/2)/(tf.reduce_max(self.d_B2) - tf.reduce_min(self.d_B2)))*2 \n",
    "        \n",
    "        \n",
    "        # exploit policy\n",
    "        self.build_policy_exploit()\n",
    "        #self.d = [self.d_W1, self.d_B1, self.d_W2, self.d_B2, self.d_W3, self.d_B3]\n",
    "        self.exploit_policy_loss = -tf.reduce_sum(self.exploit_logprob * self.advantage_placeholder_exploit)\n",
    "        self.d = [self.d_W3, self.d_B3]\n",
    "        self.gradients_exploit = tf.gradients(self.exploit_policy_loss,self.d)\n",
    "        \n",
    "        # train encoder and decoder\n",
    "        adam_optimizer_exploit =  tf.train.AdamOptimizer(self.lr*0.3)\n",
    "        self.output_train_op = adam_optimizer_exploit.minimize(self.exploit_policy_loss)\n",
    "        # train original network\n",
    "        adam_optimizer_explore = tf.train.AdamOptimizer(self.lr)\n",
    "        self.input_train_op = adam_optimizer_explore.minimize(self.explore_policy_loss)\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.ConstructGraph()\n",
    "        # create tf session\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # initiliaze all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def train_step(self): \n",
    "        # sample num_traj*num_MDPs\n",
    "        for i in range(self.num_mdps):\n",
    "            explore_paths, explore_rewards = self.sample_paths_explore(self.env,i)\n",
    "            self.scores_eval_explore = self.scores_eval_explore + explore_rewards\n",
    "            observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "            new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "            new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "            actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "            rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "            returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "            #print(returns_explore)\n",
    "            print(\"average reward explore: MDP\", i, np.sum(explore_rewards)/num_traj, len(explore_rewards))\n",
    "\n",
    "#             baseline_explore = self.sess.run(self.baseline, {self.observation_placeholder_explore:new_observations_explore})\n",
    "#             adv = returns_explore - np.squeeze(baseline_explore)\n",
    "#             advantages_explore = (adv - np.mean(adv))/np.std(adv)\n",
    "\n",
    "\n",
    "#             # update the baseline\n",
    "\n",
    "#             self.sess.run(self.update_baseline_op, {self.observation_placeholder_explore:new_observations_explore, \n",
    "#                                            self.baseline_target_placeholder : returns_explore})\n",
    "\n",
    "            # calculate explore gradients\n",
    "    #         grads_explore = self.sess.run(self.gradients_explore, feed_dict={\n",
    "    #                     self.observation_placeholder_explore : observations_explore,\n",
    "    #                     self.action_placeholder_explore : actions_explore,\n",
    "    #                     self.advantage_placeholder_explore : returns_explore})\n",
    "            #print(\"explore\",grads_explore )\n",
    "            # form trajectory matrix\n",
    "\n",
    "            M = np.array(self.stack_trajectories(explore_paths))\n",
    "\n",
    "\n",
    "            #encoder LSTM\n",
    "            Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "            Z = np.reshape(Z,[1,len(Z)])\n",
    "            #print(\"Z\",Z)\n",
    "            #print(self.sess.run(self.d, feed_dict = {self.decoder_input_placeholder: Z}))\n",
    "            #print(\"d\",self.d)\n",
    "            # sample paths\n",
    "            tvars = tf.trainable_variables()\n",
    "            tvars_vals = self.sess.run(tvars[-5:-1])\n",
    "            #print(tvars_vals)\n",
    "            for j in range(2):\n",
    "                exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,i)\n",
    "                # get observations, actions and rewards\n",
    "                observations_exploit = np.concatenate([path[\"observation\"] for path in exploit_paths])\n",
    "                actions_exploit = np.concatenate([path[\"action\"] for path in exploit_paths])\n",
    "                rewards_exploit = np.concatenate([path[\"reward\"] for path in exploit_paths])\n",
    "                returns_exploit = self.get_returns(exploit_paths)\n",
    "                print(\"average reward exploit: MDP\", i, np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "                self.scores_eval_exploit = self.scores_eval_exploit + exploit_rewards\n",
    "\n",
    "\n",
    "            # exploit grads\n",
    "    #         grads_exploit = self.sess.run(self.gradients_exploit,feed_dict={\n",
    "    #                     self.observation_placeholder_exploit : observations_exploit,\n",
    "    #                     self.action_placeholder_exploit : actions_exploit,\n",
    "    #                     self.advantage_placeholder_exploit : returns_exploit,\n",
    "    #                     self.decoder_input_placeholder: Z})\n",
    "\n",
    "            #train encoder and decoder network\n",
    "                self.sess.run(self.output_train_op, feed_dict={\n",
    "                                self.observation_placeholder_exploit : observations_exploit,\n",
    "                                self.action_placeholder_exploit : actions_exploit,\n",
    "                                self.advantage_placeholder_exploit : returns_exploit,\n",
    "                                self.decoder_input_placeholder: Z})\n",
    "\n",
    "            # find advantage for input network\n",
    "    #         advantage_explore = 0\n",
    "    #         for i in range(len(grads_exploit)):\n",
    "    #             l1 = grads_exploit[i]\n",
    "    #             l2 = grads_explore[i]\n",
    "    #             advantage_explore = advantage_explore + np.matmul(l1.flatten(), l2.flatten())\n",
    "\n",
    "            # train input policy\n",
    "            self.sess.run(self.input_train_op, feed_dict={\n",
    "                            self.observation_placeholder_explore : new_observations_explore,\n",
    "                            self.action_placeholder_explore : new_actions_explore,\n",
    "                            self.advantage_placeholder_explore : returns_explore})\n",
    "    \n",
    "    def test(self):\n",
    "        explore_paths, explore_rewards = self.sample_paths_explore(self.env, Test = True)\n",
    "        M = self.stack_trajectories(explore_paths)\n",
    "        Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        # sample paths\n",
    "        exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z, Test = True)\n",
    "        print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.device(\"/gpu:0\") as dev:\n",
    "            self.initialize()\n",
    "            num_epochs = 200\n",
    "            self.scores_eval_explore = [] \n",
    "            self.scores_eval_exploit = [] \n",
    "            for epoch in range(num_epochs):\n",
    "                print(\"epoch number: \", epoch)\n",
    "                self.train_step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "a = MetaLearner(env, max_ep_len, num_traj, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:  0\n",
      "average reward explore: MDP 0 18.7 10\n",
      "average reward exploit: MDP 0 7.2 10\n",
      "average reward exploit: MDP 0 6.3 10\n",
      "average reward explore: MDP 1 11.3 10\n",
      "average reward exploit: MDP 1 12.1 10\n",
      "average reward exploit: MDP 1 11.0 10\n",
      "average reward explore: MDP 2 13.4 10\n",
      "average reward exploit: MDP 2 9.2 10\n",
      "average reward exploit: MDP 2 10.0 10\n",
      "average reward explore: MDP 3 11.4 10\n",
      "average reward exploit: MDP 3 6.0 10\n",
      "average reward exploit: MDP 3 6.0 10\n",
      "average reward explore: MDP 4 16.9 10\n",
      "average reward exploit: MDP 4 17.1 10\n",
      "average reward exploit: MDP 4 21.7 10\n",
      "average reward explore: MDP 5 14.3 10\n",
      "average reward exploit: MDP 5 17.8 10\n",
      "average reward exploit: MDP 5 22.6 10\n",
      "average reward explore: MDP 6 16.4 10\n",
      "average reward exploit: MDP 6 21.8 10\n",
      "average reward exploit: MDP 6 29.4 10\n",
      "average reward explore: MDP 7 16.2 10\n",
      "average reward exploit: MDP 7 12.1 10\n",
      "average reward exploit: MDP 7 10.2 10\n",
      "average reward explore: MDP 8 12.8 10\n",
      "average reward exploit: MDP 8 16.6 10\n",
      "average reward exploit: MDP 8 13.6 10\n",
      "average reward explore: MDP 9 17.7 10\n",
      "average reward exploit: MDP 9 20.6 10\n",
      "average reward exploit: MDP 9 26.1 10\n",
      "epoch number:  1\n",
      "average reward explore: MDP 0 23.4 10\n",
      "average reward exploit: MDP 0 20.5 10\n",
      "average reward exploit: MDP 0 23.4 10\n",
      "average reward explore: MDP 1 21.2 10\n",
      "average reward exploit: MDP 1 29.0 10\n",
      "average reward exploit: MDP 1 27.8 10\n",
      "average reward explore: MDP 2 15.1 10\n",
      "average reward exploit: MDP 2 37.0 10\n",
      "average reward exploit: MDP 2 42.7 10\n",
      "average reward explore: MDP 3 22.7 10\n",
      "average reward exploit: MDP 3 26.6 10\n",
      "average reward exploit: MDP 3 27.8 10\n",
      "average reward explore: MDP 4 16.1 10\n",
      "average reward exploit: MDP 4 41.6 10\n",
      "average reward exploit: MDP 4 37.4 10\n",
      "average reward explore: MDP 5 16.7 10\n",
      "average reward exploit: MDP 5 27.0 10\n",
      "average reward exploit: MDP 5 30.2 10\n",
      "average reward explore: MDP 6 25.0 10\n",
      "average reward exploit: MDP 6 50.9 10\n",
      "average reward exploit: MDP 6 44.9 10\n",
      "average reward explore: MDP 7 16.8 10\n",
      "average reward exploit: MDP 7 19.8 10\n",
      "average reward exploit: MDP 7 24.5 10\n",
      "average reward explore: MDP 8 21.9 10\n",
      "average reward exploit: MDP 8 34.9 10\n",
      "average reward exploit: MDP 8 47.4 10\n",
      "average reward explore: MDP 9 18.9 10\n",
      "average reward exploit: MDP 9 38.6 10\n",
      "average reward exploit: MDP 9 48.8 10\n",
      "epoch number:  2\n",
      "average reward explore: MDP 0 37.9 10\n",
      "average reward exploit: MDP 0 67.1 10\n",
      "average reward exploit: MDP 0 71.7 10\n",
      "average reward explore: MDP 1 24.4 10\n",
      "average reward exploit: MDP 1 66.9 10\n",
      "average reward exploit: MDP 1 60.7 10\n",
      "average reward explore: MDP 2 25.6 10\n",
      "average reward exploit: MDP 2 56.2 10\n",
      "average reward exploit: MDP 2 58.7 10\n",
      "average reward explore: MDP 3 26.7 10\n",
      "average reward exploit: MDP 3 47.0 10\n",
      "average reward exploit: MDP 3 49.9 10\n",
      "average reward explore: MDP 4 27.9 10\n",
      "average reward exploit: MDP 4 29.5 10\n",
      "average reward exploit: MDP 4 29.9 10\n",
      "average reward explore: MDP 5 23.7 10\n",
      "average reward exploit: MDP 5 47.7 10\n",
      "average reward exploit: MDP 5 53.0 10\n",
      "average reward explore: MDP 6 22.4 10\n",
      "average reward exploit: MDP 6 94.0 10\n",
      "average reward exploit: MDP 6 100.2 10\n",
      "average reward explore: MDP 7 24.4 10\n",
      "average reward exploit: MDP 7 123.6 10\n",
      "average reward exploit: MDP 7 148.0 10\n",
      "average reward explore: MDP 8 20.2 10\n",
      "average reward exploit: MDP 8 36.1 10\n",
      "average reward exploit: MDP 8 34.9 10\n",
      "average reward explore: MDP 9 23.3 10\n",
      "average reward exploit: MDP 9 39.8 10\n",
      "average reward exploit: MDP 9 40.8 10\n",
      "epoch number:  3\n",
      "average reward explore: MDP 0 33.0 10\n",
      "average reward exploit: MDP 0 54.7 10\n",
      "average reward exploit: MDP 0 47.7 10\n",
      "average reward explore: MDP 1 37.2 10\n",
      "average reward exploit: MDP 1 56.3 10\n",
      "average reward exploit: MDP 1 55.5 10\n",
      "average reward explore: MDP 2 33.3 10\n",
      "average reward exploit: MDP 2 51.6 10\n",
      "average reward exploit: MDP 2 55.0 10\n",
      "average reward explore: MDP 3 43.4 10\n",
      "average reward exploit: MDP 3 54.3 10\n",
      "average reward exploit: MDP 3 53.5 10\n",
      "average reward explore: MDP 4 31.3 10\n",
      "average reward exploit: MDP 4 60.8 10\n",
      "average reward exploit: MDP 4 61.0 10\n",
      "average reward explore: MDP 5 28.9 10\n",
      "average reward exploit: MDP 5 721.6 10\n",
      "average reward exploit: MDP 5 470.8 10\n",
      "average reward explore: MDP 6 23.6 10\n",
      "average reward exploit: MDP 6 55.2 10\n",
      "average reward exploit: MDP 6 57.8 10\n",
      "average reward explore: MDP 7 26.4 10\n",
      "average reward exploit: MDP 7 45.8 10\n",
      "average reward exploit: MDP 7 46.9 10\n",
      "average reward explore: MDP 8 26.3 10\n",
      "average reward exploit: MDP 8 47.0 10\n",
      "average reward exploit: MDP 8 49.4 10\n",
      "average reward explore: MDP 9 28.9 10\n",
      "average reward exploit: MDP 9 156.2 10\n",
      "average reward exploit: MDP 9 175.6 10\n",
      "epoch number:  4\n",
      "average reward explore: MDP 0 48.0 10\n",
      "average reward exploit: MDP 0 107.8 10\n",
      "average reward exploit: MDP 0 115.3 10\n",
      "average reward explore: MDP 1 47.1 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bde7b6456197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-512115ec728e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch number: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-512115ec728e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m#print(tvars_vals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mexploit_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploit_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_paths_exploit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;31m# get observations, actions and rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mobservations_exploit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexploit_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-512115ec728e>\u001b[0m in \u001b[0;36msample_paths_exploit\u001b[0;34m(self, env, Z, mdp_num, Test, num_episodes)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploit_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_placeholder_exploit\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_input_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1125\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;34m\"\"\"The `Operation` that produces this tensor as an output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_paths, explore_rewards = a.sample_paths_explore(a.env,20)\n",
    "observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "returns_explore = a.get_returns(explore_paths, explore = True)\n",
    "M = np.array(a.stack_trajectories(explore_paths))\n",
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: MDP 30.95 400\n"
     ]
    }
   ],
   "source": [
    " print(\"average reward explore: MDP\", np.sum(explore_rewards)/num_traj, len(explore_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 2.8275 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward exploit: MDP\", np.mean(exploit_rewards), len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_traj = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path  = \"results/\" + env + \"/\"\n",
    "plot_output  = output_path + \"scores.png\"\n",
    "export_plot(a.scores_eval_exploit, \"Output Policy Score\", a.env, env + \"scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = []\n",
    "for i in range(len(a.scores_eval_exploit)):\n",
    "    if(i%40 == 0):\n",
    "        exp.append(a.scores_eval_exploit[i])\n",
    "exp2 = []\n",
    "for i in range(len(a.scores_eval_explore)):\n",
    "    if(i%20 == 0):\n",
    "        exp2.append(a.scores_eval_explore[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(range(len(exp)), exp, label = \"Exploit Policy Train\")\n",
    "plt.plot(range(len(exp2)), exp2, label = \"Explore Policy Train\")\n",
    "plt.xlabel(\"Training Episodes\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(env)\n",
    "plt.legend()\n",
    "plt.savefig(\"InvPen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(range(6), [0,943,965,956,981,989] ,label = \"Exploit Policy Test\")\n",
    "plt.ylim((0,1000))\n",
    "plt.xlabel(\"Training Episodes\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(env)\n",
    "plt.legend()\n",
    "plt.savefig(\"InvPentest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
