{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Include Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "\n",
    "# Environment setup\n",
    "#env = \"CartPole-v0\"\n",
    "env=\"InvertedPendulum-v2\"\n",
    "\n",
    "# discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "# observation_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "max_ep_len = 1000\n",
    "num_traj = 1000\n",
    "#traj_length = max_ep_len*(observation_dim + 2)\n",
    "latent_size = 25\n",
    "use_baseline = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward network (multi-layer-perceptron, or mlp)\n",
    "\n",
    "def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "    '''\n",
    "    Build a feed forward network\n",
    "    '''\n",
    "    Input = mlp_input\n",
    "    with tf.variable_scope(scope):\n",
    "        # Dense Layers\n",
    "        for i in range(n_layers-1):\n",
    "            dense = tf.layers.dense(inputs = Input, units = size, activation = tf.nn.relu, bias_initializer=tf.constant_initializer(1.0))\n",
    "            Input = dense\n",
    "        # Fully Connected Layer\n",
    "        out = layers.fully_connected(inputs = Input, num_outputs = output_size, activation_fn=output_activation)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner():\n",
    "    def __init__(self, env, max_ep_len, num_traj,latent_size ):\n",
    "        self.env = gym.make(env)\n",
    "        self.discrete = isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.num_traj = num_traj\n",
    "        self.traj_length = self.max_ep_len*(self.observation_dim + 2) # TO Change\n",
    "        self.use_baseline = True\n",
    "        self.latent_size = latent_size\n",
    "        self.feature_size = self.observation_dim + 2\n",
    "        self.lr = 3e-2\n",
    "        self.num_layers = 1\n",
    "        self.layers_size = 16\n",
    "        # build model\n",
    "        self.ConstructGraph()\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        self.observation_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        if(self.discrete):\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.int32, shape=(None))\n",
    "            self.action_placeholder_exploit = tf.placeholder(tf.int32, shape=(None))\n",
    "        else:\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "            self.action_placeholder_exploit= tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "\n",
    "        self.baseline_target_placeholder = tf.placeholder(tf.float32, shape= None)\n",
    "        self.advantage_placeholder_explore = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        #self.encoder_input_placeholder = tf.placeholder(tf.float32, shape= (self.num_traj,self.traj_length))\n",
    "        self.encoder_input_placeholder = tf.placeholder(tf.float32, [None, None, self.feature_size])\n",
    "        self.decoder_input_placeholder = tf.placeholder(tf.float32, shape= (1,self.latent_size))\n",
    "        self.sequence_length_placeholder = tf.placeholder(tf.int32, [None, ])\n",
    "        \n",
    "        self.observation_placeholder_exploit = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        #TODO\n",
    "        self.advantage_placeholder_exploit = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "    def build_policy_explore(self, scope = \"policy_explore\"):\n",
    "        if (self.discrete):\n",
    "            self.action_logits = build_mlp(self.observation_placeholder_explore,self.action_dim,scope = scope,n_layers=self.num_layers,size = self.layers_size,output_activation=None)\n",
    "            self.explore_action = tf.multinomial(self.action_logits,1)\n",
    "            self.explore_action = tf.squeeze(self.explore_action, axis=1)\n",
    "            self.explore_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.action_logits, labels = self.action_placeholder_explore)\n",
    "\n",
    "        else:   \n",
    "            action_means = build_mlp(self.observation_placeholder_explore,self.action_dim,scope,n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"log_std\", [self.action_dim])\n",
    "            self.explore_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            self.explore_logprob =  mvn.log_prob(value = self.action_placeholder_explore, name='log_prob')\n",
    "\n",
    "    \n",
    "    \n",
    "    def build_policy_exploit(self, scope = \"policy_exploit\"):\n",
    "        if(self.discrete):\n",
    "            #self.exploit_action_logits = (tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W1) + self.d_B1), self.d_W2) + self.d_B2),self.d_W3) + self.d_B3)\n",
    "            self.exploit_action_logits = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            self.exploit_action = tf.multinomial(self.exploit_action_logits,1)\n",
    "            self.exploit_action = tf.squeeze(self.exploit_action, axis=1)\n",
    "            self.exploit_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.exploit_action_logits, labels = self.action_placeholder_exploit)\n",
    "        else:\n",
    "            action_means = tf.matmul(self.observation_placeholder_exploit,self.d_W3) #+ self.d_B3\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"exploit_log_prob\", [self.action_dim])\n",
    "            self.exploit_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            self.exploit_logprob =  mvn.log_prob(value = self.action_placeholder_exploit, name='exploit_log_prob')\n",
    "\n",
    "        #self.loss_grads_exploit = self.exploit_logprob * self.advantage_placeholder_exploit\n",
    "        \n",
    "    def NNEncoder(self, scope = \"NNEncoder\"):\n",
    "        self.Z = build_mlp(self.encoder_input_placeholder,self.latent_size,scope = scope,n_layers=3,size = 60,output_activation=None)\n",
    "    \n",
    " \n",
    "\n",
    "    # input [num_traj, length, features (obs + action + reward) ]\n",
    "    def LSTMEncoder(self, scope = \"LSTMEncoder\"):\n",
    "        self.hidden_size = 64\n",
    "        initializer = tf.random_uniform_initializer(-1, 1)\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, self.feature_size, initializer=initializer)\n",
    "        cell_out = tf.contrib.rnn.OutputProjectionWrapper(cell, self.latent_size)\n",
    "        self.output, _ = tf.nn.dynamic_rnn(cell_out,self.encoder_input_placeholder,self.sequence_length_placeholder,dtype=tf.float32,)\n",
    "        batch_size = tf.shape(self.output)[0]\n",
    "        max_length = tf.shape(self.output)[1]\n",
    "        out_size = int(self.output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (self.sequence_length_placeholder - 1)\n",
    "        flat = tf.reshape(self.output, [-1, out_size])\n",
    "        self.Z = tf.reduce_mean(tf.gather(flat, index), axis = 0)\n",
    "        \n",
    "    \n",
    "    def Decoder(self, decoder_out_dim, scope):\n",
    "        return build_mlp(self.decoder_input_placeholder,decoder_out_dim,scope = scope,n_layers=3,size = 60,output_activation=None)\n",
    "    \n",
    "    \n",
    "    def sample_paths_explore(self, env,Test = False, num_episodes = None):\n",
    "        paths = []\n",
    "        self.length = []\n",
    "        episode_rewards = []\n",
    "        for i in range(self.num_traj):\n",
    "#             if(Test == True):\n",
    "#                 # env.gravity = 17 # Cartpole\n",
    "#             else:\n",
    "#                 if(i%20 == 0):\n",
    "#                     #env.gravity = 5 + np.random.rand(1)*10 # Cartpole\n",
    "            pad = False\n",
    "            state = env.reset()\n",
    "            new_states,states,new_actions, actions, rewards = [], [], [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                if (pad):\n",
    "                    states.append([0]*self.observation_dim)\n",
    "                    if(self.discrete):\n",
    "                        actions.append(0)\n",
    "                    else:\n",
    "                        actions.append([0]*self.action_dim)\n",
    "                    rewards.append(0)\n",
    "                else:\n",
    "                    states.append(state)\n",
    "                    new_states.append(state)\n",
    "                    action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    actions.append(action)\n",
    "                    new_actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "                    episode_reward += reward\n",
    "                    if (done or step == self.max_ep_len-1):\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        self.length.append(step + 1)\n",
    "                        pad = True  \n",
    "\n",
    "            #print(\"explore\",np.array(actions))\n",
    "            path = {\"new_obs\" : np.array(new_states),\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\"new_acs\" : np.array(new_actions),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def sample_paths_exploit(self, env,Z,Test = False, num_episodes = None):\n",
    "        paths = []\n",
    "        num = 0\n",
    "        episode_rewards = []\n",
    "        for i in range(self.num_traj):\n",
    "#             if(Test == True):\n",
    "#                 env.gravity = 17\n",
    "#             else:\n",
    "#                 if(i%20 == 0):\n",
    "#                     env.gravity = 5 + np.random.rand(1)*10\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.sess.run(self.exploit_action, feed_dict={self.observation_placeholder_exploit : state[None], self.decoder_input_placeholder: Z})[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "            #print(\"exploit\",np.array(actions))\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    " \n",
    "        #print(\"exploit success: \", num)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def get_returns(self,paths, explore = False):\n",
    "        all_returns = []\n",
    "        m = 0\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            if(explore):\n",
    "                length = self.length[m]\n",
    "            else:\n",
    "                length = len(rewards)\n",
    "            #print(self.length[m])\n",
    "            for i in range(length):\n",
    "                path_returns = 0\n",
    "                k = 0\n",
    "                for j in range(i,length):\n",
    "                    path_returns = path_returns + rewards[j]*(1)**k\n",
    "                    k = k+1\n",
    "                returns.append(path_returns)\n",
    "            all_returns.append(returns)\n",
    "            m+=1\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "    \n",
    "    def stack_trajectories(self,paths):\n",
    "        trajectories = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            states = path[\"observation\"]\n",
    "            action = path[\"action\"]\n",
    "            SAR = []\n",
    "            for i in range(len(states)):\n",
    "                SAR.append(list(states[i]) + [action[i]] + [rewards[i]])\n",
    "            trajectories.append(SAR)\n",
    "\n",
    "        return np.array(trajectories)\n",
    "    \n",
    "    def addBaseline(self):\n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "\n",
    "    def calculate_advantage(self,returns, observations):\n",
    "        if (self.use_baseline):\n",
    "            baseline = self.sess.run(self.baseline, {input_placeholder:observations})\n",
    "            adv = returns - baseline\n",
    "            adv = (adv - np.mean(adv))/np.std(adv)\n",
    "        else:\n",
    "            adv = returns\n",
    "        return adv\n",
    "    \n",
    "\n",
    "        \n",
    "    def ConstructGraph(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.add_placeholders()\n",
    "        \n",
    "        self.build_policy_explore()\n",
    "        self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "        self.loss_grads_explore = -self.explore_logprob * self.advantage_placeholder_explore\n",
    "        self.tvars_explore = tf.trainable_variables()\n",
    "        self.gradients_explore = tf.gradients(self.explore_policy_loss,self.tvars_explore)\n",
    "        \n",
    "        #self.addBaseline()\n",
    "        \n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=1, size = 16,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "        \n",
    "        #Encoder LSTM\n",
    "        self.LSTMEncoder()\n",
    "        \n",
    "        self.decoder_len = 16\n",
    "        #decoder weights\n",
    "        #self.d_W1 = self.Decoder(scope = \"W1\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W2 = self.Decoder(scope = \"W2\", decoder_out_dim = self.decoder_len*self.decoder_len)\n",
    "        #self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.decoder_len*action_dim)\n",
    "        self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.observation_dim*self.action_dim)\n",
    "        \n",
    "        #self.d_W1 = ((self.d_W1 - (tf.reduce_max(self.d_W1) + tf.reduce_min(self.d_W1))/2)/(tf.reduce_max(self.d_W1) - tf.reduce_min(self.d_W1)))*2 \n",
    "        #self.d_W2 = ((self.d_W2 - (tf.reduce_max(self.d_W2) + tf.reduce_min(self.d_W2))/2)/(tf.reduce_max(self.d_W2) - tf.reduce_min(self.d_W2)))*2 \n",
    "        self.d_W3 = ((self.d_W3 - (tf.reduce_max(self.d_W3) + tf.reduce_min(self.d_W3))/2)/(tf.reduce_max(self.d_W3) - tf.reduce_min(self.d_W3)))*2 \n",
    "        \n",
    "        #self.d_W1 = tf.reshape(self.d_W1, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W2 = tf.reshape(self.d_W2, [self.decoder_len, self.decoder_len])\n",
    "        #self.d_W3 = tf.reshape(self.d_W3, [self.decoder_len, self.action_dim])\n",
    "        self.d_W3 = tf.reshape(self.d_W3, [self.observation_dim, self.action_dim])\n",
    "        \n",
    "        # decoder output bias\n",
    "        #self.d_B1 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B1\"), [self.decoder_len])\n",
    "        #self.d_B2 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B2\"), [self.decoder_len])\n",
    "        self.d_B3 = tf.reshape(self.Decoder(decoder_out_dim = self.action_dim, scope = \"B3\"), [self.action_dim])\n",
    "        #self.d_B1 = ((self.d_B1 - (tf.reduce_max(self.d_B1) + tf.reduce_min(self.d_B1))/2)/(tf.reduce_max(self.d_B1) - tf.reduce_min(self.d_B1)))*2 \n",
    "        #self.d_B2 = ((self.d_B2 - (tf.reduce_max(self.d_B2) + tf.reduce_min(self.d_B2))/2)/(tf.reduce_max(self.d_B2) - tf.reduce_min(self.d_B2)))*2 \n",
    "        \n",
    "        \n",
    "        # exploit policy\n",
    "        self.build_policy_exploit()\n",
    "        #self.d = [self.d_W1, self.d_B1, self.d_W2, self.d_B2, self.d_W3, self.d_B3]\n",
    "        self.exploit_policy_loss = -tf.reduce_sum(self.exploit_logprob * self.advantage_placeholder_exploit)\n",
    "        self.d = [self.d_W3, self.d_B3]\n",
    "        self.gradients_exploit = tf.gradients(self.exploit_policy_loss,self.d)\n",
    "        \n",
    "        # train encoder and decoder\n",
    "        adam_optimizer_exploit =  tf.train.AdamOptimizer(self.lr*5)\n",
    "        self.output_train_op = adam_optimizer_exploit.minimize(self.exploit_policy_loss)\n",
    "        # train original network\n",
    "        adam_optimizer_explore = tf.train.AdamOptimizer(self.lr)\n",
    "        self.input_train_op = adam_optimizer_explore.minimize(self.explore_policy_loss)\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.ConstructGraph()\n",
    "        # create tf session\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # initiliaze all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def train_step(self):\n",
    "        explore_paths, explore_rewards = self.sample_paths_explore(self.env)\n",
    "        observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "        new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "        new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "        actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "        rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "        returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "        #print(returns_explore)\n",
    "        print(\"average reward explore\", np.sum(explore_rewards)/num_traj, len(explore_rewards))\n",
    "        \n",
    "#         baseline_explore = self.sess.run(self.baseline, {self.observation_placeholder_explore:observations_explore})\n",
    "#         adv = returns_explore - np.squeeze(baseline_explore)\n",
    "#         advantages_explore = (adv - np.mean(adv))/np.std(adv)\n",
    "\n",
    "        \n",
    "#         # update the baseline\n",
    "\n",
    "#         self.sess.run(self.update_baseline_op, {self.observation_placeholder_explore:observations_explore, \n",
    "#                                        self.baseline_target_placeholder : returns_explore})\n",
    "\n",
    "        # calculate explore gradients\n",
    "#         grads_explore = self.sess.run(self.gradients_explore, feed_dict={\n",
    "#                     self.observation_placeholder_explore : observations_explore,\n",
    "#                     self.action_placeholder_explore : actions_explore,\n",
    "#                     self.advantage_placeholder_explore : returns_explore})\n",
    "        #print(\"explore\",grads_explore )\n",
    "        # form trajectory matrix\n",
    "        \n",
    "        M = np.array(self.stack_trajectories(explore_paths))\n",
    "\n",
    "        \n",
    "        #encoder LSTM\n",
    "        Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        #print(Z)\n",
    "        #print(self.sess.run(self.d, feed_dict = {self.decoder_input_placeholder: Z}))\n",
    "        # sample paths\n",
    "        \n",
    "        for i in range(5):\n",
    "            exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z)\n",
    "            # get observations, actions and rewards\n",
    "            observations_exploit = np.concatenate([path[\"observation\"] for path in exploit_paths])\n",
    "            actions_exploit = np.concatenate([path[\"action\"] for path in exploit_paths])\n",
    "            rewards_exploit = np.concatenate([path[\"reward\"] for path in exploit_paths])\n",
    "            returns_exploit = self.get_returns(exploit_paths)\n",
    "            print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "        \n",
    "        \n",
    "        # exploit grads\n",
    "#         grads_exploit = self.sess.run(self.gradients_exploit,feed_dict={\n",
    "#                     self.observation_placeholder_exploit : observations_exploit,\n",
    "#                     self.action_placeholder_exploit : actions_exploit,\n",
    "#                     self.advantage_placeholder_exploit : returns_exploit,\n",
    "#                     self.decoder_input_placeholder: Z})\n",
    "\n",
    "        #train encoder and decoder network\n",
    "            self.sess.run(self.output_train_op, feed_dict={\n",
    "                            self.observation_placeholder_exploit : observations_exploit,\n",
    "                            self.action_placeholder_exploit : actions_exploit,\n",
    "                            self.advantage_placeholder_exploit : returns_exploit,\n",
    "                            self.decoder_input_placeholder: Z})\n",
    "        \n",
    "        # find advantage for input network\n",
    "#         advantage_explore = 0\n",
    "#         for i in range(len(grads_exploit)):\n",
    "#             l1 = grads_exploit[i]\n",
    "#             l2 = grads_explore[i]\n",
    "#             advantage_explore = advantage_explore + np.matmul(l1.flatten(), l2.flatten())\n",
    "\n",
    "        # train input policy\n",
    "        self.sess.run(self.input_train_op, feed_dict={\n",
    "                        self.observation_placeholder_explore : new_observations_explore,\n",
    "                        self.action_placeholder_explore : new_actions_explore,\n",
    "                        self.advantage_placeholder_explore : returns_explore})\n",
    "    \n",
    "    def test(self):\n",
    "        explore_paths, explore_rewards = self.sample_paths_explore(self.env, Test = True)\n",
    "        M = self.stack_trajectories(explore_paths)\n",
    "        Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        # sample paths\n",
    "        exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z, Test = True)\n",
    "        print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "        \n",
    "    def train(self):\n",
    "        self.initialize()\n",
    "        num_epochs = 200\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"epoch number: \", epoch)\n",
    "            self.train_step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "a = MetaLearner(env, max_ep_len, num_traj, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:  0\n",
      "average reward explore 6.08 100\n",
      "average reward exploit 8.34 100\n",
      "average reward exploit 31.92 100\n",
      "average reward exploit 30.83 100\n",
      "average reward exploit 29.82 100\n",
      "average reward exploit 33.82 100\n",
      "epoch number:  1\n",
      "average reward explore 6.46 100\n",
      "average reward exploit 37.06 100\n",
      "average reward exploit 38.06 100\n",
      "average reward exploit 38.87 100\n",
      "average reward exploit 38.73 100\n",
      "average reward exploit 40.7 100\n",
      "epoch number:  2\n",
      "average reward explore 6.69 100\n",
      "average reward exploit 39.47 100\n",
      "average reward exploit 36.87 100\n",
      "average reward exploit 38.65 100\n",
      "average reward exploit 42.2 100\n",
      "average reward exploit 37.64 100\n",
      "epoch number:  3\n",
      "average reward explore 7.39 100\n",
      "average reward exploit 36.49 100\n",
      "average reward exploit 36.7 100\n",
      "average reward exploit 38.33 100\n",
      "average reward exploit 38.57 100\n",
      "average reward exploit 42.41 100\n",
      "epoch number:  4\n",
      "average reward explore 8.05 100\n",
      "average reward exploit 43.35 100\n",
      "average reward exploit 47.79 100\n",
      "average reward exploit 47.86 100\n",
      "average reward exploit 47.72 100\n",
      "average reward exploit 48.24 100\n",
      "epoch number:  5\n",
      "average reward explore 8.75 100\n",
      "average reward exploit 44.97 100\n",
      "average reward exploit 46.72 100\n",
      "average reward exploit 49.29 100\n",
      "average reward exploit 48.53 100\n",
      "average reward exploit 47.41 100\n",
      "epoch number:  6\n",
      "average reward explore 10.1 100\n",
      "average reward exploit 49.39 100\n",
      "average reward exploit 48.8 100\n",
      "average reward exploit 48.45 100\n",
      "average reward exploit 48.2 100\n",
      "average reward exploit 48.79 100\n",
      "epoch number:  7\n",
      "average reward explore 9.96 100\n",
      "average reward exploit 50.81 100\n",
      "average reward exploit 49.98 100\n",
      "average reward exploit 51.72 100\n",
      "average reward exploit 50.45 100\n",
      "average reward exploit 52.57 100\n",
      "epoch number:  8\n",
      "average reward explore 12.28 100\n",
      "average reward exploit 50.22 100\n",
      "average reward exploit 51.87 100\n",
      "average reward exploit 51.3 100\n",
      "average reward exploit 53.09 100\n",
      "average reward exploit 51.62 100\n",
      "epoch number:  9\n",
      "average reward explore 14.62 100\n",
      "average reward exploit 52.87 100\n",
      "average reward exploit 47.33 100\n",
      "average reward exploit 51.32 100\n",
      "average reward exploit 51.18 100\n",
      "average reward exploit 48.36 100\n",
      "epoch number:  10\n",
      "average reward explore 15.44 100\n",
      "average reward exploit 47.66 100\n",
      "average reward exploit 47.64 100\n",
      "average reward exploit 48.58 100\n",
      "average reward exploit 50.27 100\n",
      "average reward exploit 52.38 100\n",
      "epoch number:  11\n",
      "average reward explore 18.26 100\n",
      "average reward exploit 51.74 100\n",
      "average reward exploit 51.24 100\n",
      "average reward exploit 51.57 100\n",
      "average reward exploit 50.89 100\n",
      "average reward exploit 49.54 100\n",
      "epoch number:  12\n",
      "average reward explore 22.1 100\n",
      "average reward exploit 48.72 100\n",
      "average reward exploit 48.59 100\n",
      "average reward exploit 50.12 100\n",
      "average reward exploit 46.88 100\n",
      "average reward exploit 47.79 100\n",
      "epoch number:  13\n",
      "average reward explore 26.86 100\n",
      "average reward exploit 50.07 100\n",
      "average reward exploit 46.44 100\n",
      "average reward exploit 48.01 100\n",
      "average reward exploit 46.88 100\n",
      "average reward exploit 47.3 100\n",
      "epoch number:  14\n",
      "average reward explore 31.97 100\n",
      "average reward exploit 49.43 100\n",
      "average reward exploit 47.58 100\n",
      "average reward exploit 49.43 100\n",
      "average reward exploit 50.28 100\n",
      "average reward exploit 49.42 100\n",
      "epoch number:  15\n",
      "average reward explore 36.61 100\n",
      "average reward exploit 49.77 100\n",
      "average reward exploit 51.78 100\n",
      "average reward exploit 49.3 100\n",
      "average reward exploit 49.92 100\n",
      "average reward exploit 48.46 100\n",
      "epoch number:  16\n",
      "average reward explore 40.05 100\n",
      "average reward exploit 48.77 100\n",
      "average reward exploit 49.45 100\n",
      "average reward exploit 48.03 100\n",
      "average reward exploit 46.83 100\n",
      "average reward exploit 48.3 100\n",
      "epoch number:  17\n",
      "average reward explore 48.21 100\n",
      "average reward exploit 47.27 100\n",
      "average reward exploit 51.28 100\n",
      "average reward exploit 48.84 100\n",
      "average reward exploit 51.0 100\n",
      "average reward exploit 50.51 100\n",
      "epoch number:  18\n",
      "average reward explore 49.01 100\n",
      "average reward exploit 52.7 100\n",
      "average reward exploit 53.27 100\n",
      "average reward exploit 50.68 100\n",
      "average reward exploit 50.9 100\n",
      "average reward exploit 53.37 100\n",
      "epoch number:  19\n",
      "average reward explore 55.0 100\n",
      "average reward exploit 52.92 100\n",
      "average reward exploit 54.34 100\n",
      "average reward exploit 52.88 100\n",
      "average reward exploit 52.46 100\n",
      "average reward exploit 52.76 100\n",
      "epoch number:  20\n",
      "average reward explore 58.3 100\n",
      "average reward exploit 52.96 100\n",
      "average reward exploit 53.4 100\n",
      "average reward exploit 50.58 100\n",
      "average reward exploit 50.89 100\n",
      "average reward exploit 52.82 100\n",
      "epoch number:  21\n",
      "average reward explore 72.3 100\n",
      "average reward exploit 54.44 100\n",
      "average reward exploit 54.24 100\n",
      "average reward exploit 53.41 100\n",
      "average reward exploit 50.47 100\n",
      "average reward exploit 49.93 100\n",
      "epoch number:  22\n",
      "average reward explore 83.59 100\n",
      "average reward exploit 51.71 100\n",
      "average reward exploit 51.84 100\n",
      "average reward exploit 49.82 100\n",
      "average reward exploit 51.76 100\n",
      "average reward exploit 50.71 100\n",
      "epoch number:  23\n",
      "average reward explore 96.25 100\n",
      "average reward exploit 52.51 100\n",
      "average reward exploit 50.17 100\n",
      "average reward exploit 52.09 100\n",
      "average reward exploit 54.4 100\n",
      "average reward exploit 55.05 100\n",
      "epoch number:  24\n",
      "average reward explore 114.83 100\n",
      "average reward exploit 51.94 100\n",
      "average reward exploit 51.41 100\n",
      "average reward exploit 52.44 100\n",
      "average reward exploit 52.22 100\n",
      "average reward exploit 52.32 100\n",
      "epoch number:  25\n",
      "average reward explore 134.07 100\n",
      "average reward exploit 52.45 100\n",
      "average reward exploit 51.8 100\n",
      "average reward exploit 52.09 100\n",
      "average reward exploit 52.23 100\n",
      "average reward exploit 52.72 100\n",
      "epoch number:  26\n",
      "average reward explore 164.71 100\n",
      "average reward exploit 53.22 100\n",
      "average reward exploit 54.7 100\n",
      "average reward exploit 53.65 100\n",
      "average reward exploit 52.5 100\n",
      "average reward exploit 52.93 100\n",
      "epoch number:  27\n",
      "average reward explore 210.32 100\n",
      "average reward exploit 52.97 100\n",
      "average reward exploit 55.18 100\n",
      "average reward exploit 52.42 100\n",
      "average reward exploit 50.58 100\n",
      "average reward exploit 50.92 100\n",
      "epoch number:  28\n",
      "average reward explore 285.63 100\n",
      "average reward exploit 50.64 100\n",
      "average reward exploit 52.55 100\n",
      "average reward exploit 51.72 100\n",
      "average reward exploit 51.45 100\n",
      "average reward exploit 51.61 100\n",
      "epoch number:  29\n",
      "average reward explore 283.23 100\n",
      "average reward exploit 50.43 100\n",
      "average reward exploit 51.54 100\n",
      "average reward exploit 52.12 100\n",
      "average reward exploit 53.02 100\n",
      "average reward exploit 55.6 100\n",
      "epoch number:  30\n",
      "average reward explore 385.76 100\n",
      "average reward exploit 54.8 100\n",
      "average reward exploit 53.66 100\n",
      "average reward exploit 51.32 100\n",
      "average reward exploit 50.81 100\n",
      "average reward exploit 52.36 100\n",
      "epoch number:  31\n",
      "average reward explore 366.87 100\n",
      "average reward exploit 51.99 100\n",
      "average reward exploit 51.47 100\n",
      "average reward exploit 49.93 100\n",
      "average reward exploit 49.49 100\n",
      "average reward exploit 48.08 100\n",
      "epoch number:  32\n",
      "average reward explore 548.46 100\n",
      "average reward exploit 49.83 100\n",
      "average reward exploit 46.77 100\n",
      "average reward exploit 46.39 100\n",
      "average reward exploit 47.55 100\n",
      "average reward exploit 44.99 100\n",
      "epoch number:  33\n",
      "average reward explore 425.27 100\n",
      "average reward exploit 45.37 100\n",
      "average reward exploit 45.9 100\n",
      "average reward exploit 45.05 100\n",
      "average reward exploit 45.09 100\n",
      "average reward exploit 41.8 100\n",
      "epoch number:  34\n",
      "average reward explore 766.1 100\n",
      "average reward exploit 43.8 100\n",
      "average reward exploit 41.65 100\n",
      "average reward exploit 42.35 100\n",
      "average reward exploit 41.65 100\n",
      "average reward exploit 40.78 100\n",
      "epoch number:  35\n",
      "average reward explore 607.34 100\n",
      "average reward exploit 39.48 100\n",
      "average reward exploit 41.99 100\n",
      "average reward exploit 40.29 100\n",
      "average reward exploit 39.21 100\n",
      "average reward exploit 39.34 100\n",
      "epoch number:  36\n",
      "average reward explore 653.29 100\n",
      "average reward exploit 40.22 100\n",
      "average reward exploit 38.74 100\n",
      "average reward exploit 38.61 100\n",
      "average reward exploit 37.43 100\n",
      "average reward exploit 38.21 100\n",
      "epoch number:  37\n",
      "average reward explore 688.37 100\n",
      "average reward exploit 38.91 100\n",
      "average reward exploit 40.45 100\n",
      "average reward exploit 39.47 100\n",
      "average reward exploit 36.59 100\n",
      "average reward exploit 38.0 100\n",
      "epoch number:  38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore 800.77 100\n",
      "average reward exploit 37.29 100\n",
      "average reward exploit 38.2 100\n",
      "average reward exploit 36.35 100\n",
      "average reward exploit 35.37 100\n",
      "average reward exploit 35.8 100\n",
      "epoch number:  39\n",
      "average reward explore 950.78 100\n",
      "average reward exploit 35.68 100\n",
      "average reward exploit 34.54 100\n",
      "average reward exploit 34.24 100\n",
      "average reward exploit 34.18 100\n",
      "average reward exploit 35.32 100\n",
      "epoch number:  40\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 36.44 100\n",
      "average reward exploit 36.86 100\n",
      "average reward exploit 38.96 100\n",
      "average reward exploit 40.98 100\n",
      "average reward exploit 41.64 100\n",
      "epoch number:  41\n",
      "average reward explore 993.18 100\n",
      "average reward exploit 40.51 100\n",
      "average reward exploit 42.57 100\n",
      "average reward exploit 42.78 100\n",
      "average reward exploit 42.29 100\n",
      "average reward exploit 44.28 100\n",
      "epoch number:  42\n",
      "average reward explore 985.93 100\n",
      "average reward exploit 43.33 100\n",
      "average reward exploit 44.93 100\n",
      "average reward exploit 45.61 100\n",
      "average reward exploit 46.26 100\n",
      "average reward exploit 47.69 100\n",
      "epoch number:  43\n",
      "average reward explore 583.06 100\n",
      "average reward exploit 48.62 100\n",
      "average reward exploit 49.54 100\n",
      "average reward exploit 51.21 100\n",
      "average reward exploit 53.31 100\n",
      "average reward exploit 54.69 100\n",
      "epoch number:  44\n",
      "average reward explore 901.06 100\n",
      "average reward exploit 50.75 100\n",
      "average reward exploit 50.5 100\n",
      "average reward exploit 51.87 100\n",
      "average reward exploit 51.77 100\n",
      "average reward exploit 50.46 100\n",
      "epoch number:  45\n",
      "average reward explore 999.56 100\n",
      "average reward exploit 53.98 100\n",
      "average reward exploit 52.08 100\n",
      "average reward exploit 53.05 100\n",
      "average reward exploit 52.93 100\n",
      "average reward exploit 54.18 100\n",
      "epoch number:  46\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 53.64 100\n",
      "average reward exploit 50.06 100\n",
      "average reward exploit 49.93 100\n",
      "average reward exploit 52.78 100\n",
      "average reward exploit 51.26 100\n",
      "epoch number:  47\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 49.66 100\n",
      "average reward exploit 49.96 100\n",
      "average reward exploit 48.62 100\n",
      "average reward exploit 50.02 100\n",
      "average reward exploit 48.53 100\n",
      "epoch number:  48\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 47.96 100\n",
      "average reward exploit 46.16 100\n",
      "average reward exploit 47.3 100\n",
      "average reward exploit 45.22 100\n",
      "average reward exploit 45.87 100\n",
      "epoch number:  49\n",
      "average reward explore 998.3 100\n",
      "average reward exploit 46.86 100\n",
      "average reward exploit 49.18 100\n",
      "average reward exploit 48.76 100\n",
      "average reward exploit 46.15 100\n",
      "average reward exploit 47.88 100\n",
      "epoch number:  50\n",
      "average reward explore 973.82 100\n",
      "average reward exploit 47.7 100\n",
      "average reward exploit 48.41 100\n",
      "average reward exploit 47.1 100\n",
      "average reward exploit 46.78 100\n",
      "average reward exploit 47.28 100\n",
      "epoch number:  51\n",
      "average reward explore 873.66 100\n",
      "average reward exploit 47.49 100\n",
      "average reward exploit 47.05 100\n",
      "average reward exploit 46.94 100\n",
      "average reward exploit 47.83 100\n",
      "average reward exploit 46.28 100\n",
      "epoch number:  52\n",
      "average reward explore 857.95 100\n",
      "average reward exploit 45.2 100\n",
      "average reward exploit 45.24 100\n",
      "average reward exploit 47.21 100\n",
      "average reward exploit 45.71 100\n",
      "average reward exploit 47.17 100\n",
      "epoch number:  53\n",
      "average reward explore 891.65 100\n",
      "average reward exploit 46.52 100\n",
      "average reward exploit 46.76 100\n",
      "average reward exploit 48.74 100\n",
      "average reward exploit 47.58 100\n",
      "average reward exploit 49.24 100\n",
      "epoch number:  54\n",
      "average reward explore 928.32 100\n",
      "average reward exploit 48.78 100\n",
      "average reward exploit 47.89 100\n",
      "average reward exploit 50.4 100\n",
      "average reward exploit 50.04 100\n",
      "average reward exploit 49.79 100\n",
      "epoch number:  55\n",
      "average reward explore 964.86 100\n",
      "average reward exploit 48.99 100\n",
      "average reward exploit 47.17 100\n",
      "average reward exploit 48.62 100\n",
      "average reward exploit 47.57 100\n",
      "average reward exploit 47.56 100\n",
      "epoch number:  56\n",
      "average reward explore 997.31 100\n",
      "average reward exploit 49.51 100\n",
      "average reward exploit 50.95 100\n",
      "average reward exploit 51.54 100\n",
      "average reward exploit 50.95 100\n",
      "average reward exploit 55.15 100\n",
      "epoch number:  57\n",
      "average reward explore 996.08 100\n",
      "average reward exploit 56.49 100\n",
      "average reward exploit 55.36 100\n",
      "average reward exploit 50.92 100\n",
      "average reward exploit 52.32 100\n",
      "average reward exploit 51.19 100\n",
      "epoch number:  58\n",
      "average reward explore 996.68 100\n",
      "average reward exploit 53.83 100\n",
      "average reward exploit 51.47 100\n",
      "average reward exploit 53.23 100\n",
      "average reward exploit 52.22 100\n",
      "average reward exploit 50.33 100\n",
      "epoch number:  59\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 50.89 100\n",
      "average reward exploit 52.38 100\n",
      "average reward exploit 50.27 100\n",
      "average reward exploit 50.05 100\n",
      "average reward exploit 48.65 100\n",
      "epoch number:  60\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 48.51 100\n",
      "average reward exploit 48.78 100\n",
      "average reward exploit 48.97 100\n",
      "average reward exploit 48.16 100\n",
      "average reward exploit 49.64 100\n",
      "epoch number:  61\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 48.37 100\n",
      "average reward exploit 52.16 100\n",
      "average reward exploit 49.73 100\n",
      "average reward exploit 51.43 100\n",
      "average reward exploit 53.25 100\n",
      "epoch number:  62\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 52.2 100\n",
      "average reward exploit 52.56 100\n",
      "average reward exploit 50.45 100\n",
      "average reward exploit 51.32 100\n",
      "average reward exploit 53.18 100\n",
      "epoch number:  63\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 52.6 100\n",
      "average reward exploit 50.61 100\n",
      "average reward exploit 50.41 100\n",
      "average reward exploit 50.09 100\n",
      "average reward exploit 50.83 100\n",
      "epoch number:  64\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 51.05 100\n",
      "average reward exploit 49.86 100\n",
      "average reward exploit 49.69 100\n",
      "average reward exploit 50.58 100\n",
      "average reward exploit 49.82 100\n",
      "epoch number:  65\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 51.17 100\n",
      "average reward exploit 51.64 100\n",
      "average reward exploit 49.95 100\n",
      "average reward exploit 51.54 100\n",
      "average reward exploit 50.76 100\n",
      "epoch number:  66\n",
      "average reward explore 997.09 100\n",
      "average reward exploit 49.32 100\n",
      "average reward exploit 49.61 100\n",
      "average reward exploit 49.73 100\n",
      "average reward exploit 51.95 100\n",
      "average reward exploit 51.42 100\n",
      "epoch number:  67\n",
      "average reward explore 983.08 100\n",
      "average reward exploit 50.04 100\n",
      "average reward exploit 50.02 100\n",
      "average reward exploit 51.31 100\n",
      "average reward exploit 52.32 100\n",
      "average reward exploit 48.81 100\n",
      "epoch number:  68\n",
      "average reward explore 995.05 100\n",
      "average reward exploit 50.6 100\n",
      "average reward exploit 52.7 100\n",
      "average reward exploit 51.18 100\n",
      "average reward exploit 51.02 100\n",
      "average reward exploit 50.18 100\n",
      "epoch number:  69\n",
      "average reward explore 987.49 100\n",
      "average reward exploit 52.76 100\n",
      "average reward exploit 50.87 100\n",
      "average reward exploit 52.16 100\n",
      "average reward exploit 50.58 100\n",
      "average reward exploit 51.2 100\n",
      "epoch number:  70\n",
      "average reward explore 982.88 100\n",
      "average reward exploit 48.74 100\n",
      "average reward exploit 48.44 100\n",
      "average reward exploit 50.49 100\n",
      "average reward exploit 49.54 100\n",
      "average reward exploit 47.79 100\n",
      "epoch number:  71\n",
      "average reward explore 995.36 100\n",
      "average reward exploit 49.63 100\n",
      "average reward exploit 48.55 100\n",
      "average reward exploit 47.63 100\n",
      "average reward exploit 48.84 100\n",
      "average reward exploit 46.58 100\n",
      "epoch number:  72\n",
      "average reward explore 1000.0 100\n",
      "average reward exploit 49.3 100\n",
      "average reward exploit 47.19 100\n",
      "average reward exploit 48.82 100\n",
      "average reward exploit 45.76 100\n",
      "average reward exploit 49.26 100\n",
      "epoch number:  73\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
