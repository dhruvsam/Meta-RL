{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Include Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow.contrib.layers as layers\n",
    "from mujoco_py import load_model_from_xml, MjSim, MjViewer\n",
    "import policies\n",
    "import value_functions as vfuncs\n",
    "import utils_pg as utils\n",
    "\n",
    "# Environment setup\n",
    "#env = \"CartPole-v0\"\n",
    "#env=\"InvertedPendulum-v2\"\n",
    "env = \"Walker2d-v2\"\n",
    "\n",
    "# discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "# observation_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "max_ep_len = 1000\n",
    "num_traj = 30\n",
    "#traj_length = max_ep_len*(observation_dim + 2)\n",
    "latent_size = 150\n",
    "use_baseline = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward network (multi-layer-perceptron, or mlp)\n",
    "\n",
    "def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "    '''\n",
    "    Build a feed forward network\n",
    "    '''\n",
    "    Input = mlp_input\n",
    "    with tf.variable_scope(scope):\n",
    "        # Dense Layers\n",
    "        for i in range(n_layers-1):\n",
    "            dense = tf.layers.dense(inputs = Input, units = size, activation = tf.nn.relu, bias_initializer=tf.constant_initializer(1.0))\n",
    "            Input = dense\n",
    "        # Fully Connected Layer\n",
    "        out = layers.fully_connected(inputs = Input, num_outputs = output_size, activation_fn=output_activation)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner():\n",
    "    def __init__(self, env, max_ep_len, num_traj,latent_size ):\n",
    "        self.env = gym.make(env)\n",
    "        self.discrete = isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.num_traj = num_traj\n",
    "        self.traj_length = self.max_ep_len*(self.observation_dim + 2) # TO Change\n",
    "        self.use_baseline = True\n",
    "        self.latent_size = latent_size\n",
    "        self.feature_size = self.observation_dim + 1 + self.env.action_space.shape[0] # HC\n",
    "        self.lr = 3e-3\n",
    "        self.num_layers = 2\n",
    "        self.layers_size = 32\n",
    "        self.num_mdps = 10\n",
    "        self.model = self.env.env.model\n",
    "        self.sim = MjSim(self.model)\n",
    "        self.gamma = 0.97\n",
    "        self.decoder_len = 32\n",
    "\n",
    "        # build model\n",
    "        self.ConstructGraph()\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        self.observation_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        if(self.discrete):\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.int32, shape=(None))\n",
    "            self.action_placeholder_exploit = tf.placeholder(tf.int32, shape=(None))\n",
    "        else:\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "            self.action_placeholder_exploit= tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "\n",
    "        self.baseline_target_placeholder = tf.placeholder(tf.float32, shape= None)\n",
    "        self.advantage_placeholder_explore = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        #self.encoder_input_placeholder = tf.placeholder(tf.float32, shape= (self.num_traj,self.traj_length))\n",
    "        self.encoder_input_placeholder = tf.placeholder(tf.float32, [None, None, self.feature_size])\n",
    "        self.decoder_input_placeholder = tf.placeholder(tf.float32, shape= (1,self.latent_size))\n",
    "        self.sequence_length_placeholder = tf.placeholder(tf.int32, [None, ])\n",
    "        \n",
    "        self.observation_placeholder_exploit = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        #TODO\n",
    "        self.advantage_placeholder_exploit = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "    def build_policy_explore(self, scope = \"policy_explore\"):\n",
    "        if (self.discrete):\n",
    "            self.action_logits = build_mlp(self.observation_placeholder_explore,self.action_dim,scope = scope,n_layers=self.num_layers,size = self.layers_size,output_activation=None)\n",
    "            self.explore_action = tf.multinomial(self.action_logits,1)\n",
    "            self.explore_action = tf.squeeze(self.explore_action, axis=1)\n",
    "            self.explore_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.action_logits, labels = self.action_placeholder_explore)\n",
    "\n",
    "        else:   \n",
    "            action_means = build_mlp(self.observation_placeholder_explore,self.action_dim,scope,n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            #log_std = tf.get_variable(\"log_std\", [self.action_dim])\n",
    "            log_std = tf.get_variable(\n",
    "                'log_std',\n",
    "                shape=[1, self.action_dim],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer(),\n",
    "                trainable=True\n",
    "            )\n",
    "            log_std = tf.log(tf.exp(log_std) + 1.)\n",
    "            log_std = tf.tile(log_std,\n",
    "                                   [tf.shape(action_means)[0], 1])\n",
    "            self.explore_action = tf.random_normal((1,),\n",
    "                                                   action_means,\n",
    "                                                   log_std)\n",
    "            dist = tf.contrib.distributions.MultivariateNormalDiag(action_means, log_std)\n",
    "            self.explore_logprob = dist.prob(self.action_placeholder_explore, name = 'log_prob')\n",
    "            #self.explore_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            #mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            #self.explore_logprob =  mvn.log_prob(value = self.action_placeholder_explore, name='log_prob')\n",
    "\n",
    "    \n",
    "    \n",
    "    def build_policy_exploit(self, scope = \"policy_exploit\"):\n",
    "        if(self.discrete):\n",
    "            #self.exploit_action_logits = (tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W1) + self.d_B1), self.d_W2) + self.d_B2),self.d_W3) + self.d_B3)\n",
    "            #self.exploit_action_logits = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            self.exploit_action_logits = tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W2) + self.d_B2), self.d_W3) + self.d_B3\n",
    "            self.exploit_action = tf.multinomial(self.exploit_action_logits,1)\n",
    "            self.exploit_action = tf.squeeze(self.exploit_action, axis=1)\n",
    "            self.exploit_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.exploit_action_logits, labels = self.action_placeholder_exploit)\n",
    "        else:\n",
    "            action_means = tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W2) + self.d_B2), self.d_W3) + self.d_B3\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"exploit_log_prob\", [self.action_dim])\n",
    "            self.exploit_action = action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            self.exploit_logprob = utils.gauss_log_prob(mu=action_means, logstd=log_std, x=self.action_placeholder_exploit)\n",
    "            \n",
    "#             init = tf.constant(np.random.rand(1, 2))\n",
    "#             log_std = tf.get_variable(\"exploit_log_prob\", [self.action_dim])\n",
    "#             self.exploit_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "#             mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "#             self.exploit_logprob =  mvn.log_prob(value = self.action_placeholder_exploit, name='exploit_log_prob')\n",
    "\n",
    "        #self.loss_grads_exploit = self.exploit_logprob * self.advantage_placeholder_exploit\n",
    "        \n",
    "    def NNEncoder(self, scope = \"NNEncoder\"):\n",
    "        self.Z = build_mlp(self.encoder_input_placeholder,self.latent_size,scope = scope,n_layers=3,size = 60,output_activation=None)\n",
    "    \n",
    " \n",
    "\n",
    "    # input [num_traj, length, features (obs + action + reward) ]\n",
    "    def LSTMEncoder(self, scope = \"LSTMEncoder\"):\n",
    "        self.hidden_size = 64\n",
    "        initializer = tf.random_uniform_initializer(-1, 1)\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, self.feature_size, initializer=initializer)\n",
    "        cell_out = tf.contrib.rnn.OutputProjectionWrapper(cell, self.latent_size)\n",
    "        self.output, _ = tf.nn.dynamic_rnn(cell_out,self.encoder_input_placeholder,self.sequence_length_placeholder,dtype=tf.float32,)\n",
    "        batch_size = tf.shape(self.output)[0]\n",
    "        max_length = tf.shape(self.output)[1]\n",
    "        out_size = int(self.output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (self.sequence_length_placeholder - 1)\n",
    "        flat = tf.reshape(self.output, [-1, out_size])\n",
    "        self.Z = tf.reduce_mean(tf.gather(flat, index), axis = 0)\n",
    "        \n",
    "    \n",
    "    def Decoder(self, decoder_out_dim, scope):\n",
    "        return build_mlp(self.decoder_input_placeholder,decoder_out_dim,scope = scope,n_layers=1,size = 64,output_activation=None)\n",
    "    \n",
    "    \n",
    "    def sample_paths_explore(self, env,mdp_num,Test = False, num_episodes = None):\n",
    "        paths = []\n",
    "        self.length = []\n",
    "        episode_rewards = []\n",
    "        self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        for i in range(self.num_traj):\n",
    "            pad = False\n",
    "            state = env.reset()\n",
    "            new_states,states,new_actions, actions, rewards,new_rewards = [], [], [], [], [],[]\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                if (pad):\n",
    "                    states.append([0]*self.observation_dim)\n",
    "                    if(self.discrete):\n",
    "                        actions.append(0)\n",
    "                    else:\n",
    "                        actions.append([0]*self.action_dim)\n",
    "                    rewards.append(0)\n",
    "                else:\n",
    "                    states.append(state)\n",
    "                    new_states.append(state)\n",
    "                    #action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\n",
    "                    action = self.policyfn.sample_action(state) #NEW\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    actions.append(action)\n",
    "                    new_actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "                    new_rewards.append(reward)\n",
    "                    episode_reward += reward\n",
    "                    if (done or step == self.max_ep_len-1):\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        self.length.append(step + 1)\n",
    "                        pad = True \n",
    "                        \n",
    "            episode_rewards.append(episode_reward)            \n",
    "            #print(\"explore\",np.array(actions))\n",
    "            path = {\"new_obs\" : np.array(new_states),\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\"new_acs\" : np.array(new_actions),\n",
    "                                \"action\" : np.array(actions),\"new_rewards\":np.array(new_rewards)}\n",
    "            paths.append(path)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def sample_paths_exploit(self, env,Z,mdp_num,Test = False, num_episodes = None):\n",
    "        self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        \n",
    "        paths = []\n",
    "        num = 0\n",
    "        episode_rewards = []\n",
    "        for i in range(self.num_traj):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.sess.run(self.exploit_action, feed_dict={self.observation_placeholder_exploit : state[None], self.decoder_input_placeholder: Z})[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "            #print(\"exploit\",np.array(actions))\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    " \n",
    "        #print(\"exploit success: \", num)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def get_returns(self,paths, explore = False):\n",
    "        all_returns = []\n",
    "        m = 0\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            if(explore):\n",
    "                length = self.length[m]\n",
    "            else:\n",
    "                length = len(rewards)\n",
    "            for i in range(length):\n",
    "                path_returns = 0\n",
    "                k = 0\n",
    "                for j in range(i,length):\n",
    "                    path_returns = path_returns + rewards[j]*(self.gamma)**k\n",
    "                    k = k+1\n",
    "                returns.append(path_returns)\n",
    "            all_returns.append(returns)\n",
    "            m+=1\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "    \n",
    "    def stack_trajectories(self,paths):\n",
    "        trajectories = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            states = path[\"observation\"]\n",
    "            action = path[\"action\"]\n",
    "            \n",
    "            SAR = []\n",
    "            for i in range(len(states)):\n",
    "                SAR.append(list(states[i]) + list(action[i]) + [rewards[i]])\n",
    "            trajectories.append(SAR)\n",
    "        return np.array(trajectories)\n",
    "    \n",
    "    def addBaseline(self):\n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "\n",
    "    def calculate_advantage(self,returns, observations):\n",
    "        if (self.use_baseline):\n",
    "            baseline = self.sess.run(self.baseline, {input_placeholder:observations})\n",
    "            adv = returns - baseline\n",
    "            adv = (adv - np.mean(adv))/np.std(adv)\n",
    "        else:\n",
    "            adv = returns\n",
    "        return adv\n",
    "    \n",
    "\n",
    "        \n",
    "    def ConstructGraph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.vf = vfuncs.NnValueFunction(session=self.sess,ob_dim=self.observation_dim)\n",
    "        self.policyfn = policies.GaussianPolicy(self.sess,self.observation_dim, self.action_dim)\n",
    "        \n",
    "        self.add_placeholders()\n",
    "        \n",
    "#         self.add_placeholders()\n",
    "        \n",
    "#         self.build_policy_explore()\n",
    "#         self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "#         self.loss_grads_explore = -self.explore_logprob * self.advantage_placeholder_explore\n",
    "#         self.tvars_explore = tf.trainable_variables()\n",
    "#         self.gradients_explore = tf.gradients(self.explore_policy_loss,self.tvars_explore)\n",
    "        \n",
    "#         #self.addBaseline()\n",
    "        \n",
    "#         self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=1, size = 16,output_activation=None)\n",
    "#         self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "#         baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "#         self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "        \n",
    "#         #Encoder LSTM\n",
    "        self.LSTMEncoder()\n",
    "        \n",
    "        \n",
    "        #decoder weights\n",
    "        #self.d_W1 = self.Decoder(scope = \"W1\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        self.d_W2 = self.Decoder(scope = \"W2\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.decoder_len*action_dim)\n",
    "        self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.decoder_len*self.action_dim)\n",
    "        \n",
    "        #self.d_W1 = ((self.d_W1 - (tf.reduce_max(self.d_W1) + tf.reduce_min(self.d_W1))/2)/(tf.reduce_max(self.d_W1) - tf.reduce_min(self.d_W1)))*2 \n",
    "        #self.d_W2 = ((self.d_W2 - (tf.reduce_max(self.d_W2) + tf.reduce_min(self.d_W2))/2)/(tf.reduce_max(self.d_W2) - tf.reduce_min(self.d_W2)))*2 \n",
    "        #self.d_W3 = ((self.d_W3 - (tf.reduce_max(self.d_W3) + tf.reduce_min(self.d_W3))/2)/(tf.reduce_max(self.d_W3) - tf.reduce_min(self.d_W3)))*2 \n",
    "        \n",
    "        #self.d_W1 = tf.reshape(self.d_W1, [self.observation_dim, self.decoder_len])\n",
    "        self.d_W2 = tf.reshape(self.d_W2, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W3 = tf.reshape(self.d_W3, [self.decoder_len, self.action_dim])\n",
    "        self.d_W3 = tf.reshape(self.d_W3, [self.decoder_len, self.action_dim])\n",
    "        \n",
    "        # decoder output bias\n",
    "        #self.d_B1 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B1\"), [self.decoder_len])\n",
    "        self.d_B2 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B2\"), [self.decoder_len])\n",
    "        self.d_B3 = tf.reshape(self.Decoder(decoder_out_dim = self.action_dim, scope = \"B3\"), [self.action_dim])\n",
    "        #self.d_B1 = ((self.d_B1 - (tf.reduce_max(self.d_B1) + tf.reduce_min(self.d_B1))/2)/(tf.reduce_max(self.d_B1) - tf.reduce_min(self.d_B1)))*2 \n",
    "        #self.d_B2 = ((self.d_B2 - (tf.reduce_max(self.d_B2) + tf.reduce_min(self.d_B2))/2)/(tf.reduce_max(self.d_B2) - tf.reduce_min(self.d_B2)))*2 \n",
    "        \n",
    "        \n",
    "        # exploit policy\n",
    "        self.build_policy_exploit()\n",
    "        #self.d = [self.d_W1, self.d_B1, self.d_W2, self.d_B2, self.d_W3, self.d_B3]\n",
    "        self.exploit_policy_loss = -tf.reduce_sum(self.exploit_logprob * self.advantage_placeholder_exploit)\n",
    "        self.d = [self.d_W3, self.d_B3]\n",
    "        self.gradients_exploit = tf.gradients(self.exploit_policy_loss,self.d)\n",
    "        \n",
    "        # train encoder and decoder\n",
    "        adam_optimizer_exploit =  tf.train.AdamOptimizer(self.lr*0.3)\n",
    "        self.output_train_op = adam_optimizer_exploit.minimize(self.exploit_policy_loss)\n",
    "        # train original network\n",
    "#         adam_optimizer_explore = tf.train.AdamOptimizer(self.lr)\n",
    "#         self.input_train_op = adam_optimizer_explore.minimize(self.explore_policy_loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.ConstructGraph()\n",
    "#         # create tf session\n",
    "#         self.sess = tf.Session()\n",
    "        \n",
    "#         # initiliaze all variables\n",
    "#         init = tf.global_variables_initializer()\n",
    "#         self.sess.run(init)\n",
    "    \n",
    "    def train_step(self): \n",
    "        # sample num_traj*num_MDPs\n",
    "        for i in range(self.num_mdps):\n",
    "            explore_paths, explore_rewards = self.sample_paths_explore(self.env,i)\n",
    "            observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "            new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "            new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "            actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "            rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "            \n",
    "            vtargs, vpreds, advantages_explore = [], [], []\n",
    "            for path in explore_paths:\n",
    "                rew_t = path[\"new_rewards\"]\n",
    "                return_t = utils.discount(rew_t, self.gamma)\n",
    "                vpred_t = self.vf.predict(path[\"new_obs\"])\n",
    "                adv_t = return_t - vpred_t\n",
    "                advantages_explore.append(adv_t)\n",
    "                vtargs.append(return_t)\n",
    "                vpreds.append(vpred_t)\n",
    "                \n",
    "                \n",
    "            #returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "            advantages_explore = np.concatenate(advantages_explore)\n",
    "            advantages_explore = (advantages_explore - advantages_explore.mean()) / (advantages_explore.std() + 1e-8)\n",
    "            vtarg_n = np.concatenate(vtargs)\n",
    "            vpred_n = np.concatenate(vpreds)\n",
    "            #update baseline\n",
    "            #print(advantages_explore.shape,new_observations_explore.shape)\n",
    "            self.vf.fit(new_observations_explore, vtarg_n)\n",
    "            \n",
    "            surr_loss, oldmean_na, oldlogstd_a = self.policyfn.update_policy(\n",
    "                    new_observations_explore, new_actions_explore, advantages_explore, self.lr)\n",
    "            \n",
    "            print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "            \n",
    "            #print(returns_explore)\n",
    "#             print(\"average reward explore: MDP\", i, np.sum(explore_rewards)/num_traj, len(explore_rewards))\n",
    "\n",
    "#             baseline_explore = self.sess.run(self.baseline, {self.observation_placeholder_explore:new_observations_explore})\n",
    "#             adv = returns_explore - np.squeeze(baseline_explore)\n",
    "#             advantages_explore = (adv - np.mean(adv))/(np.std(adv) + 1e-12)\n",
    "\n",
    "\n",
    "#             # update the baseline\n",
    "\n",
    "#             self.sess.run(self.update_baseline_op, {self.observation_placeholder_explore:new_observations_explore, \n",
    "#                                            self.baseline_target_placeholder : returns_explore})\n",
    "\n",
    "            # calculate explore gradients\n",
    "    #         grads_explore = self.sess.run(self.gradients_explore, feed_dict={\n",
    "    #                     self.observation_placeholder_explore : observations_explore,\n",
    "    #                     self.action_placeholder_explore : actions_explore,\n",
    "    #                     self.advantage_placeholder_explore : returns_explore})\n",
    "            #print(\"explore\",grads_explore )\n",
    "            # form trajectory matrix\n",
    "\n",
    "            M = np.array(self.stack_trajectories(explore_paths))\n",
    "\n",
    "\n",
    "            #encoder LSTM\n",
    "            Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "            Z = np.reshape(Z,[1,len(Z)])\n",
    "#             #print(\"Z\",Z)\n",
    "#             #print(self.sess.run(self.d, feed_dict = {self.decoder_input_placeholder: Z}))\n",
    "#             #print(\"d\",self.d)\n",
    "#             # sample paths\n",
    "#             tvars = tf.trainable_variables()\n",
    "#             tvars_vals = self.sess.run(tvars[-5:-1])\n",
    "#             #print(tvars_vals)\n",
    "            for j in range(10):\n",
    "                exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,i)\n",
    "                # get observations, actions and rewards\n",
    "                observations_exploit = np.concatenate([path[\"observation\"] for path in exploit_paths])\n",
    "                actions_exploit = np.concatenate([path[\"action\"] for path in exploit_paths])\n",
    "                rewards_exploit = np.concatenate([path[\"reward\"] for path in exploit_paths])\n",
    "                returns_exploit = self.get_returns(exploit_paths)\n",
    "                print(\"average reward exploit: MDP\", i, np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "\n",
    "\n",
    "#             # exploit grads\n",
    "#     #         grads_exploit = self.sess.run(self.gradients_exploit,feed_dict={\n",
    "#     #                     self.observation_placeholder_exploit : observations_exploit,\n",
    "#     #                     self.action_placeholder_exploit : actions_exploit,\n",
    "#     #                     self.advantage_placeholder_exploit : returns_exploit,\n",
    "#     #                     self.decoder_input_placeholder: Z})\n",
    "\n",
    "            #train encoder and decoder network\n",
    "                self.sess.run(self.output_train_op, feed_dict={\n",
    "                                self.observation_placeholder_exploit : observations_exploit,\n",
    "                                self.action_placeholder_exploit : actions_exploit,\n",
    "                                self.advantage_placeholder_exploit : returns_exploit,\n",
    "                                self.decoder_input_placeholder: Z})\n",
    "\n",
    "            # find advantage for input network\n",
    "    #         advantage_explore = 0\n",
    "    #         for i in range(len(grads_exploit)):\n",
    "    #             l1 = grads_exploit[i]\n",
    "    #             l2 = grads_explore[i]\n",
    "    #             advantage_explore = advantage_explore + np.matmul(l1.flatten(), l2.flatten())\n",
    "\n",
    "            # train input policy\n",
    "#             self.sess.run(self.input_train_op, feed_dict={\n",
    "#                             self.observation_placeholder_explore : new_observations_explore,\n",
    "#                             self.action_placeholder_explore : new_actions_explore,\n",
    "#                             self.advantage_placeholder_explore : advantages_explore})\n",
    "    \n",
    "    def test(self):\n",
    "        explore_paths, explore_rewards = self.sample_paths_explore(self.env,20*np.random.rand())\n",
    "        print(\"gravity: \",self.sim.model.opt.gravity )\n",
    "        M = self.stack_trajectories(explore_paths)\n",
    "        Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        # sample paths\n",
    "        exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,20*np.random.rand())\n",
    "        print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "        \n",
    "    def train(self):\n",
    "        self.initialize()\n",
    "        num_epochs = 200\n",
    "        for epoch in range(num_epochs):\n",
    "            self.num_traj = 20\n",
    "            print(\"epoch number: \", epoch)\n",
    "            self.train_step()\n",
    "            print(\"evaluating on new MDPs\")\n",
    "            self.num_traj = 400\n",
    "            self.test()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "a = MetaLearner(env, max_ep_len, num_traj, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:  1.395116126629723\n",
      "average reward explore: -5.488783135846341\n",
      "average reward exploit: MDP 0 1.5905597665299116 30\n",
      "average reward exploit: MDP 0 17.1553250658701 30\n",
      "average reward exploit: MDP 0 86.83620566025076 30\n",
      "average reward exploit: MDP 0 142.14817155650337 30\n",
      "average reward exploit: MDP 0 170.1525105850891 30\n",
      "average reward exploit: MDP 0 129.2077222192028 30\n",
      "average reward exploit: MDP 0 181.7407743927591 30\n",
      "average reward exploit: MDP 0 183.03349666281827 30\n",
      "average reward exploit: MDP 0 199.71808455589414 30\n",
      "average reward exploit: MDP 0 157.14819411366383 30\n",
      "average reward explore: -5.356999262968356\n",
      "average reward exploit: MDP 1 202.015328782422 30\n",
      "average reward exploit: MDP 1 176.5828675038157 30\n",
      "average reward exploit: MDP 1 193.76201843382665 30\n",
      "average reward exploit: MDP 1 192.52030197731168 30\n",
      "average reward exploit: MDP 1 187.8272439473706 30\n",
      "average reward exploit: MDP 1 211.5207268739771 30\n",
      "average reward exploit: MDP 1 205.27475380611105 30\n",
      "average reward exploit: MDP 1 244.59451204372652 30\n",
      "average reward exploit: MDP 1 213.98532875354422 30\n",
      "average reward exploit: MDP 1 255.3827724770649 30\n",
      "average reward explore: -5.547427755992986\n",
      "average reward exploit: MDP 2 179.93306171103117 30\n",
      "average reward exploit: MDP 2 214.16931583311765 30\n",
      "average reward exploit: MDP 2 175.85091176689065 30\n",
      "average reward exploit: MDP 2 104.83572093661637 30\n",
      "average reward exploit: MDP 2 59.65090597199623 30\n",
      "average reward exploit: MDP 2 100.32603270848901 30\n",
      "average reward exploit: MDP 2 63.4205176009427 30\n",
      "average reward exploit: MDP 2 33.91988725312588 30\n",
      "average reward exploit: MDP 2 39.51709304441042 30\n",
      "average reward exploit: MDP 2 48.44477650233611 30\n",
      "average reward explore: -5.179645198122857\n",
      "average reward exploit: MDP 3 108.02535773759627 30\n",
      "average reward exploit: MDP 3 129.50713908820276 30\n",
      "average reward exploit: MDP 3 116.92430995885866 30\n",
      "average reward exploit: MDP 3 153.8196370768015 30\n",
      "average reward exploit: MDP 3 186.70897126627628 30\n",
      "average reward exploit: MDP 3 188.34452652333897 30\n",
      "average reward exploit: MDP 3 161.27371294267675 30\n",
      "average reward exploit: MDP 3 183.19413080359175 30\n",
      "average reward exploit: MDP 3 164.30008517979869 30\n",
      "average reward exploit: MDP 3 191.20555559613362 30\n",
      "average reward explore: -4.883235667757547\n",
      "average reward exploit: MDP 4 195.24037616435092 30\n",
      "average reward exploit: MDP 4 210.63343044159546 30\n",
      "average reward exploit: MDP 4 227.22386369645332 30\n",
      "average reward exploit: MDP 4 180.80508653459344 30\n",
      "average reward exploit: MDP 4 201.38301803218192 30\n",
      "average reward exploit: MDP 4 75.74569879230371 30\n",
      "average reward exploit: MDP 4 40.988787323566484 30\n",
      "average reward exploit: MDP 4 25.846601184256276 30\n",
      "average reward exploit: MDP 4 51.52663272092809 30\n",
      "average reward exploit: MDP 4 120.89455542746893 30\n",
      "average reward explore: -4.404599605251079\n",
      "average reward exploit: MDP 5 129.73312203432687 30\n",
      "average reward exploit: MDP 5 198.94762469882946 30\n",
      "average reward exploit: MDP 5 131.031775460748 30\n",
      "average reward exploit: MDP 5 131.33766806931533 30\n",
      "average reward exploit: MDP 5 93.32445730380991 30\n",
      "average reward exploit: MDP 5 104.74754234363688 30\n",
      "average reward exploit: MDP 5 77.23536833562089 30\n",
      "average reward exploit: MDP 5 88.22452583745172 30\n",
      "average reward exploit: MDP 5 70.02612921177555 30\n",
      "average reward exploit: MDP 5 102.72919214541045 30\n",
      "average reward explore: -3.851049484420746\n",
      "average reward exploit: MDP 6 123.18538282016917 30\n",
      "average reward exploit: MDP 6 103.1139324493688 30\n",
      "average reward exploit: MDP 6 54.30308141536655 30\n",
      "average reward exploit: MDP 6 97.61044872574818 30\n",
      "average reward exploit: MDP 6 91.75930001596313 30\n",
      "average reward exploit: MDP 6 71.92103174296106 30\n",
      "average reward exploit: MDP 6 55.0145290173295 30\n",
      "average reward exploit: MDP 6 102.73174713994314 30\n",
      "average reward exploit: MDP 6 42.501155150634986 30\n",
      "average reward exploit: MDP 6 42.66750036819484 30\n",
      "average reward explore: -3.295911309232336\n",
      "average reward exploit: MDP 7 91.47874244853999 30\n",
      "average reward exploit: MDP 7 162.0900583006075 30\n",
      "average reward exploit: MDP 7 121.3129506529467 30\n",
      "average reward exploit: MDP 7 91.36268909737308 30\n",
      "average reward exploit: MDP 7 76.24993760072476 30\n",
      "average reward exploit: MDP 7 79.85400752233711 30\n",
      "average reward exploit: MDP 7 86.59716279025987 30\n",
      "average reward exploit: MDP 7 75.99533155411629 30\n",
      "average reward exploit: MDP 7 87.8406316842434 30\n",
      "average reward exploit: MDP 7 82.01283215510944 30\n",
      "average reward explore: -3.000970070992817\n",
      "average reward exploit: MDP 8 106.66069712048571 30\n",
      "average reward exploit: MDP 8 119.08509438505652 30\n",
      "average reward exploit: MDP 8 84.26474054968587 30\n",
      "average reward exploit: MDP 8 66.01278326571868 30\n",
      "average reward exploit: MDP 8 119.50655071095481 30\n",
      "average reward exploit: MDP 8 131.84587577858645 30\n",
      "average reward exploit: MDP 8 161.88796053948442 30\n",
      "average reward exploit: MDP 8 126.05636859520524 30\n",
      "average reward exploit: MDP 8 104.36942579916496 30\n",
      "average reward exploit: MDP 8 144.63933434236776 30\n",
      "average reward explore: -2.655288915944983\n",
      "average reward exploit: MDP 9 160.11969639903478 30\n",
      "average reward exploit: MDP 9 191.16135655640042 30\n",
      "average reward exploit: MDP 9 253.84619370857 30\n",
      "average reward exploit: MDP 9 227.0807640581217 30\n",
      "average reward exploit: MDP 9 230.37374877197465 30\n",
      "average reward exploit: MDP 9 175.1731554426732 30\n",
      "average reward exploit: MDP 9 74.63515076402173 30\n",
      "average reward exploit: MDP 9 15.976516636639031 30\n",
      "average reward exploit: MDP 9 6.833180497682359 30\n",
      "average reward exploit: MDP 9 8.654329089293908 30\n",
      "epoch number:  5.477201888747824\n",
      "average reward explore: -1.2590772773701757\n",
      "average reward exploit: MDP 0 2.9207167045023312 30\n",
      "average reward exploit: MDP 0 3.8917949154370337 30\n",
      "average reward exploit: MDP 0 3.7209823264632 30\n",
      "average reward exploit: MDP 0 1.8200329729612719 30\n",
      "average reward exploit: MDP 0 7.330266958754572 30\n",
      "average reward exploit: MDP 0 12.924343432873268 30\n",
      "average reward exploit: MDP 0 11.963696165825175 30\n",
      "average reward exploit: MDP 0 21.545443081517444 30\n",
      "average reward exploit: MDP 0 47.78643535579864 30\n",
      "average reward exploit: MDP 0 85.4966858844942 30\n",
      "average reward explore: -1.5907751598819957\n",
      "average reward exploit: MDP 1 139.08287458573207 30\n",
      "average reward exploit: MDP 1 162.92622300525775 30\n",
      "average reward exploit: MDP 1 189.42952023241546 30\n",
      "average reward exploit: MDP 1 191.003137902963 30\n",
      "average reward exploit: MDP 1 204.18666482531106 30\n",
      "average reward exploit: MDP 1 196.60311755708787 30\n",
      "average reward exploit: MDP 1 183.0038208997757 30\n",
      "average reward exploit: MDP 1 172.69832164656225 30\n",
      "average reward exploit: MDP 1 186.98922769786552 30\n",
      "average reward exploit: MDP 1 159.42184811351913 30\n",
      "average reward explore: 0.20568344908197306\n",
      "average reward exploit: MDP 2 205.17874281730317 30\n",
      "average reward exploit: MDP 2 269.1812908209233 30\n",
      "average reward exploit: MDP 2 320.6693826286498 30\n",
      "average reward exploit: MDP 2 307.2331154889168 30\n",
      "average reward exploit: MDP 2 324.16809341309647 30\n",
      "average reward exploit: MDP 2 286.3498015243826 30\n",
      "average reward exploit: MDP 2 267.43450496357787 30\n",
      "average reward exploit: MDP 2 225.42958740747514 30\n",
      "average reward exploit: MDP 2 173.7240814884826 30\n",
      "average reward exploit: MDP 2 96.56502281888193 30\n",
      "average reward explore: 1.1180392046364191\n",
      "average reward exploit: MDP 3 21.047050001216725 30\n",
      "average reward exploit: MDP 3 81.46149114790133 30\n",
      "average reward exploit: MDP 3 199.07130712273772 30\n",
      "average reward exploit: MDP 3 269.64027585122534 30\n",
      "average reward exploit: MDP 3 250.28928765674752 30\n",
      "average reward exploit: MDP 3 215.43606180995818 30\n",
      "average reward exploit: MDP 3 147.98441040737933 30\n",
      "average reward exploit: MDP 3 126.45054032406216 30\n",
      "average reward exploit: MDP 3 57.900505958507466 30\n",
      "average reward exploit: MDP 3 88.98256020843216 30\n",
      "average reward explore: 1.15390420859959\n",
      "average reward exploit: MDP 4 99.85670840213828 30\n",
      "average reward exploit: MDP 4 134.39926392901785 30\n",
      "average reward exploit: MDP 4 159.53543445336388 30\n",
      "average reward exploit: MDP 4 166.5937609643073 30\n",
      "average reward exploit: MDP 4 177.2772800449483 30\n",
      "average reward exploit: MDP 4 190.3607727739436 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 4 158.0796445116462 30\n",
      "average reward exploit: MDP 4 176.16103879579595 30\n",
      "average reward exploit: MDP 4 186.14003711215182 30\n",
      "average reward exploit: MDP 4 224.19566163583403 30\n",
      "average reward explore: 2.5670357411539557\n",
      "average reward exploit: MDP 5 172.23473234512267 30\n",
      "average reward exploit: MDP 5 220.72044538074368 30\n",
      "average reward exploit: MDP 5 228.39437130763923 30\n",
      "average reward exploit: MDP 5 270.77009365556887 30\n",
      "average reward exploit: MDP 5 254.62280360550469 30\n",
      "average reward exploit: MDP 5 259.7743693467103 30\n",
      "average reward exploit: MDP 5 225.8414250127018 30\n",
      "average reward exploit: MDP 5 233.60561609063635 30\n",
      "average reward exploit: MDP 5 276.4665549402762 30\n",
      "average reward exploit: MDP 5 235.31082776684133 30\n",
      "average reward explore: 2.941228125609221\n",
      "average reward exploit: MDP 6 211.8695466752353 30\n",
      "average reward exploit: MDP 6 168.44631628062322 30\n",
      "average reward exploit: MDP 6 135.43488518876254 30\n",
      "average reward exploit: MDP 6 145.71923639465183 30\n",
      "average reward exploit: MDP 6 217.14741646372624 30\n",
      "average reward exploit: MDP 6 215.37469003948524 30\n",
      "average reward exploit: MDP 6 223.62310294042445 30\n",
      "average reward exploit: MDP 6 196.26818364333218 30\n",
      "average reward exploit: MDP 6 245.69956653215917 30\n",
      "average reward exploit: MDP 6 225.4588296296305 30\n",
      "average reward explore: 3.1497733824684926\n",
      "average reward exploit: MDP 7 176.31976591820518 30\n",
      "average reward exploit: MDP 7 229.7712218774616 30\n",
      "average reward exploit: MDP 7 211.8890721661633 30\n",
      "average reward exploit: MDP 7 221.45624427304946 30\n",
      "average reward exploit: MDP 7 184.235676719228 30\n",
      "average reward exploit: MDP 7 199.32904138628763 30\n",
      "average reward exploit: MDP 7 182.42875940800772 30\n",
      "average reward exploit: MDP 7 184.45110082137353 30\n",
      "average reward exploit: MDP 7 179.23032623478568 30\n",
      "average reward exploit: MDP 7 183.29296313080798 30\n",
      "average reward explore: 2.453645795781595\n",
      "average reward exploit: MDP 8 150.52088059289068 30\n",
      "average reward exploit: MDP 8 115.326755944704 30\n",
      "average reward exploit: MDP 8 135.0603536682509 30\n",
      "average reward exploit: MDP 8 116.22988540376409 30\n",
      "average reward exploit: MDP 8 66.18021079897366 30\n",
      "average reward exploit: MDP 8 77.19033717869213 30\n",
      "average reward exploit: MDP 8 86.02618307088865 30\n",
      "average reward exploit: MDP 8 47.10145528522481 30\n",
      "average reward exploit: MDP 8 42.42802932428733 30\n",
      "average reward exploit: MDP 8 32.53958723141461 30\n",
      "average reward explore: 4.595996954790481\n",
      "average reward exploit: MDP 9 4.043974596525509 30\n",
      "average reward exploit: MDP 9 1.4794725421269106 30\n",
      "average reward exploit: MDP 9 -3.6030981328354694 30\n",
      "average reward exploit: MDP 9 38.27924387453302 30\n",
      "average reward exploit: MDP 9 49.02609873099678 30\n",
      "average reward exploit: MDP 9 18.811250807303683 30\n",
      "average reward exploit: MDP 9 64.64363582513607 30\n",
      "average reward exploit: MDP 9 73.07362587509284 30\n",
      "average reward exploit: MDP 9 103.39666342502521 30\n",
      "average reward exploit: MDP 9 138.5618849969923 30\n",
      "epoch number:  5.172050747127247\n",
      "average reward explore: 9.22925829593608\n",
      "average reward exploit: MDP 0 102.02623555113988 30\n",
      "average reward exploit: MDP 0 141.3443916924922 30\n",
      "average reward exploit: MDP 0 117.34856765483883 30\n",
      "average reward exploit: MDP 0 165.0019445971302 30\n",
      "average reward exploit: MDP 0 173.63742999849632 30\n",
      "average reward exploit: MDP 0 139.18404176877428 30\n",
      "average reward exploit: MDP 0 147.26520771551552 30\n",
      "average reward exploit: MDP 0 148.40941544666433 30\n",
      "average reward exploit: MDP 0 174.13761601560057 30\n",
      "average reward exploit: MDP 0 184.02691158052252 30\n",
      "average reward explore: 7.39587507551788\n",
      "average reward exploit: MDP 1 149.4666170873347 30\n",
      "average reward exploit: MDP 1 152.8623987217279 30\n",
      "average reward exploit: MDP 1 159.13181780405 30\n",
      "average reward exploit: MDP 1 171.16358290806696 30\n",
      "average reward exploit: MDP 1 172.30612692082414 30\n",
      "average reward exploit: MDP 1 181.89599554771667 30\n",
      "average reward exploit: MDP 1 169.86208085321684 30\n",
      "average reward exploit: MDP 1 173.27440150278537 30\n",
      "average reward exploit: MDP 1 168.29656359141407 30\n",
      "average reward exploit: MDP 1 159.90554032666927 30\n",
      "average reward explore: 9.56688454971765\n",
      "average reward exploit: MDP 2 238.79070443358688 30\n",
      "average reward exploit: MDP 2 268.5948702536406 30\n",
      "average reward exploit: MDP 2 265.062351074305 30\n",
      "average reward exploit: MDP 2 270.8358701394756 30\n",
      "average reward exploit: MDP 2 261.65669438420355 30\n",
      "average reward exploit: MDP 2 223.43618964798128 30\n",
      "average reward exploit: MDP 2 247.65029425816334 30\n",
      "average reward exploit: MDP 2 238.85776245797754 30\n",
      "average reward exploit: MDP 2 236.62064179834263 30\n",
      "average reward exploit: MDP 2 244.80349869930745 30\n",
      "average reward explore: 10.022172356449012\n",
      "average reward exploit: MDP 3 179.0125777157618 30\n",
      "average reward exploit: MDP 3 205.58660552378123 30\n",
      "average reward exploit: MDP 3 138.02452516859574 30\n",
      "average reward exploit: MDP 3 160.5187089517921 30\n",
      "average reward exploit: MDP 3 135.82445850545503 30\n",
      "average reward exploit: MDP 3 99.36504463649312 30\n",
      "average reward exploit: MDP 3 58.585365999222894 30\n",
      "average reward exploit: MDP 3 31.059714413152207 30\n",
      "average reward exploit: MDP 3 35.27140004408553 30\n",
      "average reward exploit: MDP 3 36.858978735028714 30\n",
      "average reward explore: 13.19679295973877\n",
      "average reward exploit: MDP 4 111.7170985850671 30\n",
      "average reward exploit: MDP 4 163.39236971396377 30\n",
      "average reward exploit: MDP 4 201.02472177950568 30\n",
      "average reward exploit: MDP 4 219.1746039168919 30\n",
      "average reward exploit: MDP 4 210.48718224083825 30\n",
      "average reward exploit: MDP 4 228.85690263416498 30\n",
      "average reward exploit: MDP 4 242.2425674801543 30\n",
      "average reward exploit: MDP 4 164.02215074721917 30\n",
      "average reward exploit: MDP 4 59.268829264049586 30\n",
      "average reward exploit: MDP 4 90.61763274156242 30\n",
      "average reward explore: 14.473923728790343\n",
      "average reward exploit: MDP 5 144.83308206265528 30\n",
      "average reward exploit: MDP 5 167.24596124424318 30\n",
      "average reward exploit: MDP 5 145.21730032670027 30\n",
      "average reward exploit: MDP 5 117.17974314323725 30\n",
      "average reward exploit: MDP 5 126.73129822848483 30\n",
      "average reward exploit: MDP 5 194.30113404988245 30\n",
      "average reward exploit: MDP 5 129.8008729370806 30\n",
      "average reward exploit: MDP 5 128.895502138474 30\n",
      "average reward exploit: MDP 5 156.93228385318523 30\n",
      "average reward exploit: MDP 5 133.6655180313476 30\n",
      "average reward explore: 19.852680476156024\n",
      "average reward exploit: MDP 6 173.54909384642107 30\n",
      "average reward exploit: MDP 6 184.56389518592917 30\n",
      "average reward exploit: MDP 6 169.2587070323294 30\n",
      "average reward exploit: MDP 6 119.49289879971238 30\n",
      "average reward exploit: MDP 6 199.62192709363728 30\n",
      "average reward exploit: MDP 6 137.09093275843333 30\n",
      "average reward exploit: MDP 6 158.4842503404044 30\n",
      "average reward exploit: MDP 6 181.60949894590152 30\n",
      "average reward exploit: MDP 6 166.62167835015683 30\n",
      "average reward exploit: MDP 6 150.1079745302247 30\n",
      "average reward explore: 18.262203385507767\n",
      "average reward exploit: MDP 7 168.68687752648287 30\n",
      "average reward exploit: MDP 7 113.71263002089508 30\n",
      "average reward exploit: MDP 7 143.14067381679405 30\n",
      "average reward exploit: MDP 7 171.12051583741604 30\n",
      "average reward exploit: MDP 7 176.43585925062965 30\n",
      "average reward exploit: MDP 7 156.01879095067642 30\n",
      "average reward exploit: MDP 7 221.17942725571632 30\n",
      "average reward exploit: MDP 7 148.38390645999792 30\n",
      "average reward exploit: MDP 7 229.70598879081518 30\n",
      "average reward exploit: MDP 7 245.5410529277552 30\n",
      "average reward explore: 21.91438385924776\n",
      "average reward exploit: MDP 8 205.4999599011255 30\n",
      "average reward exploit: MDP 8 209.85297482327093 30\n",
      "average reward exploit: MDP 8 245.17439347911875 30\n",
      "average reward exploit: MDP 8 168.57915454557454 30\n",
      "average reward exploit: MDP 8 202.83329597965394 30\n",
      "average reward exploit: MDP 8 194.533580550294 30\n",
      "average reward exploit: MDP 8 178.75652063780944 30\n",
      "average reward exploit: MDP 8 195.16393112845995 30\n",
      "average reward exploit: MDP 8 173.1842170645758 30\n",
      "average reward exploit: MDP 8 212.99205539647258 30\n",
      "average reward explore: 16.099790906914635\n",
      "average reward exploit: MDP 9 158.47089440182842 30\n",
      "average reward exploit: MDP 9 238.6542927832026 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 9 193.65011932416797 30\n",
      "average reward exploit: MDP 9 220.11414674598043 30\n",
      "average reward exploit: MDP 9 217.5687375344928 30\n",
      "average reward exploit: MDP 9 204.28878202017174 30\n",
      "average reward exploit: MDP 9 226.22991324189354 30\n",
      "average reward exploit: MDP 9 233.9468375133545 30\n",
      "average reward exploit: MDP 9 230.32428254106694 30\n",
      "average reward exploit: MDP 9 219.69246684801914 30\n",
      "epoch number:  9.126353885431179\n",
      "average reward explore: 37.652323344901106\n",
      "average reward exploit: MDP 0 277.23670174555696 30\n",
      "average reward exploit: MDP 0 253.85643994982982 30\n",
      "average reward exploit: MDP 0 316.8216154802472 30\n",
      "average reward exploit: MDP 0 223.31710676400698 30\n",
      "average reward exploit: MDP 0 172.7790601461982 30\n",
      "average reward exploit: MDP 0 252.21504909487294 30\n",
      "average reward exploit: MDP 0 191.8534916865112 30\n",
      "average reward exploit: MDP 0 211.23514884879762 30\n",
      "average reward exploit: MDP 0 260.8546283895349 30\n",
      "average reward exploit: MDP 0 238.70505559421386 30\n",
      "average reward explore: 38.56299339892232\n",
      "average reward exploit: MDP 1 287.9747629900458 30\n",
      "average reward exploit: MDP 1 311.73283096541843 30\n",
      "average reward exploit: MDP 1 315.2517528205302 30\n",
      "average reward exploit: MDP 1 309.72027644763165 30\n",
      "average reward exploit: MDP 1 285.54195940559123 30\n",
      "average reward exploit: MDP 1 235.6797483799376 30\n",
      "average reward exploit: MDP 1 146.66177292904513 30\n",
      "average reward exploit: MDP 1 137.06074643414928 30\n",
      "average reward exploit: MDP 1 124.768917379904 30\n",
      "average reward exploit: MDP 1 98.40246272157853 30\n",
      "average reward explore: 42.21438816004358\n",
      "average reward exploit: MDP 2 27.811012212050116 30\n",
      "average reward exploit: MDP 2 85.13381996534098 30\n",
      "average reward exploit: MDP 2 117.73936147293529 30\n",
      "average reward exploit: MDP 2 135.86881856480912 30\n",
      "average reward exploit: MDP 2 227.63684781925895 30\n",
      "average reward exploit: MDP 2 217.14755089272933 30\n",
      "average reward exploit: MDP 2 173.35265975439688 30\n",
      "average reward exploit: MDP 2 191.3551817066585 30\n",
      "average reward exploit: MDP 2 186.9477293979601 30\n",
      "average reward exploit: MDP 2 110.99506416392377 30\n",
      "average reward explore: 37.95130198784702\n",
      "average reward exploit: MDP 3 127.76770171752514 30\n",
      "average reward exploit: MDP 3 86.4822626436552 30\n",
      "average reward exploit: MDP 3 102.09256807739449 30\n",
      "average reward exploit: MDP 3 71.55445989387044 30\n",
      "average reward exploit: MDP 3 114.92352099710554 30\n",
      "average reward exploit: MDP 3 98.09517066124774 30\n",
      "average reward exploit: MDP 3 105.32642900036798 30\n",
      "average reward exploit: MDP 3 130.12366776263204 30\n",
      "average reward exploit: MDP 3 80.8009427681783 30\n",
      "average reward exploit: MDP 3 146.34278197752428 30\n",
      "average reward explore: 37.16644537935396\n",
      "average reward exploit: MDP 4 65.04366388337237 30\n",
      "average reward exploit: MDP 4 56.47552818774079 30\n",
      "average reward exploit: MDP 4 75.72475927365058 30\n",
      "average reward exploit: MDP 4 86.43148996980825 30\n",
      "average reward exploit: MDP 4 99.68687836773726 30\n",
      "average reward exploit: MDP 4 73.11521203851868 30\n",
      "average reward exploit: MDP 4 132.3142403790534 30\n",
      "average reward exploit: MDP 4 104.80667514822304 30\n",
      "average reward exploit: MDP 4 111.15160384218355 30\n",
      "average reward exploit: MDP 4 143.83841017307495 30\n",
      "average reward explore: 40.16629456765959\n",
      "average reward exploit: MDP 5 78.4485322884515 30\n",
      "average reward exploit: MDP 5 58.91726121632936 30\n",
      "average reward exploit: MDP 5 60.02645937011833 30\n",
      "average reward exploit: MDP 5 83.32650737094143 30\n",
      "average reward exploit: MDP 5 48.22211644899675 30\n",
      "average reward exploit: MDP 5 86.09839263293027 30\n",
      "average reward exploit: MDP 5 96.76793912298716 30\n",
      "average reward exploit: MDP 5 99.00291689266439 30\n",
      "average reward exploit: MDP 5 84.14498577179279 30\n",
      "average reward exploit: MDP 5 101.37127757315102 30\n",
      "average reward explore: 28.99239867265459\n",
      "average reward exploit: MDP 6 107.46940008024707 30\n",
      "average reward exploit: MDP 6 131.80113205219075 30\n",
      "average reward exploit: MDP 6 132.35309058932984 30\n",
      "average reward exploit: MDP 6 111.24877495074523 30\n",
      "average reward exploit: MDP 6 148.3006047532503 30\n",
      "average reward exploit: MDP 6 141.73132064498532 30\n",
      "average reward exploit: MDP 6 146.46751866896514 30\n",
      "average reward exploit: MDP 6 130.43469844461282 30\n",
      "average reward exploit: MDP 6 143.6572062030253 30\n",
      "average reward exploit: MDP 6 101.16024968241578 30\n",
      "average reward explore: 47.9586870102482\n",
      "average reward exploit: MDP 7 95.44436575982829 30\n",
      "average reward exploit: MDP 7 96.41813530741946 30\n",
      "average reward exploit: MDP 7 135.03348220441418 30\n",
      "average reward exploit: MDP 7 197.38411094539944 30\n",
      "average reward exploit: MDP 7 165.97326475027597 30\n",
      "average reward exploit: MDP 7 154.55402224583958 30\n",
      "average reward exploit: MDP 7 171.9645389910381 30\n",
      "average reward exploit: MDP 7 169.44952196917035 30\n",
      "average reward exploit: MDP 7 221.93011950272515 30\n",
      "average reward exploit: MDP 7 126.55644165687433 30\n",
      "average reward explore: 85.31274147511861\n",
      "average reward exploit: MDP 8 16.977857353580887 30\n",
      "average reward exploit: MDP 8 18.760934808744597 30\n",
      "average reward exploit: MDP 8 20.550352637423767 30\n",
      "average reward exploit: MDP 8 18.57391523197994 30\n",
      "average reward exploit: MDP 8 26.411197597705364 30\n",
      "average reward exploit: MDP 8 19.103563200098506 30\n",
      "average reward exploit: MDP 8 27.80709542065529 30\n",
      "average reward exploit: MDP 8 21.813794103778168 30\n",
      "average reward exploit: MDP 8 22.195933753472392 30\n",
      "average reward exploit: MDP 8 31.60820887205232 30\n",
      "average reward explore: 55.325548758442686\n",
      "average reward exploit: MDP 9 229.1414150050921 30\n",
      "average reward exploit: MDP 9 263.62077134263313 30\n",
      "average reward exploit: MDP 9 235.82002862613768 30\n",
      "average reward exploit: MDP 9 143.718379031954 30\n",
      "average reward exploit: MDP 9 193.13214315157293 30\n",
      "average reward exploit: MDP 9 168.95859444611662 30\n",
      "average reward exploit: MDP 9 207.71115555765644 30\n",
      "average reward exploit: MDP 9 177.33667060323413 30\n",
      "average reward exploit: MDP 9 201.47977931477027 30\n",
      "average reward exploit: MDP 9 173.06465447174395 30\n",
      "epoch number:  8.700131726864983\n",
      "average reward explore: 76.12857910976267\n",
      "average reward exploit: MDP 0 255.43984085425208 30\n",
      "average reward exploit: MDP 0 263.1740430522314 30\n",
      "average reward exploit: MDP 0 261.55129760482464 30\n",
      "average reward exploit: MDP 0 297.41106134536074 30\n",
      "average reward exploit: MDP 0 274.4681818219548 30\n",
      "average reward exploit: MDP 0 273.03970400410196 30\n",
      "average reward exploit: MDP 0 285.0779031248307 30\n",
      "average reward exploit: MDP 0 241.37445952160508 30\n",
      "average reward exploit: MDP 0 296.00293556425174 30\n",
      "average reward exploit: MDP 0 264.68358021859837 30\n",
      "average reward explore: 57.83682434945828\n",
      "average reward exploit: MDP 1 234.34473948404522 30\n",
      "average reward exploit: MDP 1 223.70834554087017 30\n",
      "average reward exploit: MDP 1 239.98324381022397 30\n",
      "average reward exploit: MDP 1 224.4983763506941 30\n",
      "average reward exploit: MDP 1 277.7693851445021 30\n",
      "average reward exploit: MDP 1 269.0127481235041 30\n",
      "average reward exploit: MDP 1 268.57372530997986 30\n",
      "average reward exploit: MDP 1 273.77135404203665 30\n",
      "average reward exploit: MDP 1 262.8885248998513 30\n",
      "average reward exploit: MDP 1 233.44234614454572 30\n",
      "average reward explore: 71.13066658860718\n",
      "average reward exploit: MDP 2 145.75109837480807 30\n",
      "average reward exploit: MDP 2 176.45304997734092 30\n",
      "average reward exploit: MDP 2 293.97866920076143 30\n",
      "average reward exploit: MDP 2 265.48097304774325 30\n",
      "average reward exploit: MDP 2 247.18485319938924 30\n",
      "average reward exploit: MDP 2 219.49852207120563 30\n",
      "average reward exploit: MDP 2 212.82843504161264 30\n",
      "average reward exploit: MDP 2 179.36557682900963 30\n",
      "average reward exploit: MDP 2 214.54896411851212 30\n",
      "average reward exploit: MDP 2 158.59076313966395 30\n",
      "average reward explore: 46.783716345058224\n",
      "average reward exploit: MDP 3 197.08130594600567 30\n",
      "average reward exploit: MDP 3 174.2852479322713 30\n",
      "average reward exploit: MDP 3 210.46231940315366 30\n",
      "average reward exploit: MDP 3 233.58482826419564 30\n",
      "average reward exploit: MDP 3 250.0033082407329 30\n",
      "average reward exploit: MDP 3 200.0092079556017 30\n",
      "average reward exploit: MDP 3 150.74458443413397 30\n",
      "average reward exploit: MDP 3 167.71666363856232 30\n",
      "average reward exploit: MDP 3 135.21405578061905 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 3 128.86207172209566 30\n",
      "average reward explore: 101.49993546376255\n",
      "average reward exploit: MDP 4 217.7342960477908 30\n",
      "average reward exploit: MDP 4 159.16074480597942 30\n",
      "average reward exploit: MDP 4 197.38151339350878 30\n",
      "average reward exploit: MDP 4 163.49895005600396 30\n",
      "average reward exploit: MDP 4 167.58975489802935 30\n",
      "average reward exploit: MDP 4 184.01839693543175 30\n",
      "average reward exploit: MDP 4 202.88578859891973 30\n",
      "average reward exploit: MDP 4 164.11204544794924 30\n",
      "average reward exploit: MDP 4 189.50910456805343 30\n",
      "average reward exploit: MDP 4 201.1513090305167 30\n",
      "average reward explore: 116.23500634231395\n",
      "average reward exploit: MDP 5 205.8273880370501 30\n",
      "average reward exploit: MDP 5 228.63401583091218 30\n",
      "average reward exploit: MDP 5 170.53096493110584 30\n",
      "average reward exploit: MDP 5 200.18212390023265 30\n",
      "average reward exploit: MDP 5 157.26672989583693 30\n",
      "average reward exploit: MDP 5 218.57769608001203 30\n",
      "average reward exploit: MDP 5 191.3960389288172 30\n",
      "average reward exploit: MDP 5 217.20572811465166 30\n",
      "average reward exploit: MDP 5 215.80408022901273 30\n",
      "average reward exploit: MDP 5 179.6464592727926 30\n",
      "average reward explore: 77.91822027124795\n",
      "average reward exploit: MDP 6 163.1288285611517 30\n",
      "average reward exploit: MDP 6 144.83487099130335 30\n",
      "average reward exploit: MDP 6 128.43780916690187 30\n",
      "average reward exploit: MDP 6 121.37908546836996 30\n",
      "average reward exploit: MDP 6 130.43651035255294 30\n",
      "average reward exploit: MDP 6 145.92619632569003 30\n",
      "average reward exploit: MDP 6 129.95715110356096 30\n",
      "average reward exploit: MDP 6 144.6298621784375 30\n",
      "average reward exploit: MDP 6 133.5672294226753 30\n",
      "average reward exploit: MDP 6 114.76562497860876 30\n",
      "average reward explore: 72.17911194338915\n",
      "average reward exploit: MDP 7 121.0908017066386 30\n",
      "average reward exploit: MDP 7 144.95818767281418 30\n",
      "average reward exploit: MDP 7 129.9074772817051 30\n",
      "average reward exploit: MDP 7 160.61044776552703 30\n",
      "average reward exploit: MDP 7 161.26628324316007 30\n",
      "average reward exploit: MDP 7 161.64987003917886 30\n",
      "average reward exploit: MDP 7 180.95684055771352 30\n",
      "average reward exploit: MDP 7 158.2209157128016 30\n",
      "average reward exploit: MDP 7 172.99823148509373 30\n",
      "average reward exploit: MDP 7 175.87145246651679 30\n",
      "average reward explore: 76.56797434517338\n",
      "average reward exploit: MDP 8 115.52784492049226 30\n",
      "average reward exploit: MDP 8 155.34293627738484 30\n",
      "average reward exploit: MDP 8 140.94629771994477 30\n",
      "average reward exploit: MDP 8 170.03455232625996 30\n",
      "average reward exploit: MDP 8 159.57334900389952 30\n",
      "average reward exploit: MDP 8 143.49998428183522 30\n",
      "average reward exploit: MDP 8 152.35513574907435 30\n",
      "average reward exploit: MDP 8 178.79223384024124 30\n",
      "average reward exploit: MDP 8 161.91907817673516 30\n",
      "average reward exploit: MDP 8 119.24434835280475 30\n",
      "average reward explore: 59.40907800000311\n",
      "average reward exploit: MDP 9 66.46362390016536 30\n",
      "average reward exploit: MDP 9 59.28514455632349 30\n",
      "average reward exploit: MDP 9 89.95731163460657 30\n",
      "average reward exploit: MDP 9 34.816626325196104 30\n",
      "average reward exploit: MDP 9 106.60297621805734 30\n",
      "average reward exploit: MDP 9 86.4220751166617 30\n",
      "average reward exploit: MDP 9 122.58419070971584 30\n",
      "average reward exploit: MDP 9 158.43181517675836 30\n",
      "average reward exploit: MDP 9 86.50455504018191 30\n",
      "average reward exploit: MDP 9 98.67898933820477 30\n",
      "epoch number:  9.0665742015145\n",
      "average reward explore: 131.4365219975358\n",
      "average reward exploit: MDP 0 117.81551938957622 30\n",
      "average reward exploit: MDP 0 124.35334566288343 30\n",
      "average reward exploit: MDP 0 153.05243938756522 30\n",
      "average reward exploit: MDP 0 150.88198192723505 30\n",
      "average reward exploit: MDP 0 176.0988030756715 30\n",
      "average reward exploit: MDP 0 197.35476754324426 30\n",
      "average reward exploit: MDP 0 224.9238105424405 30\n",
      "average reward exploit: MDP 0 245.8850173312684 30\n",
      "average reward exploit: MDP 0 271.09015399049787 30\n",
      "average reward exploit: MDP 0 299.3055407540598 30\n",
      "average reward explore: 101.72803707381257\n",
      "average reward exploit: MDP 1 233.398124294134 30\n",
      "average reward exploit: MDP 1 263.9378174662635 30\n",
      "average reward exploit: MDP 1 187.52786909792727 30\n",
      "average reward exploit: MDP 1 168.19365348485226 30\n",
      "average reward exploit: MDP 1 204.91870368474034 30\n",
      "average reward exploit: MDP 1 204.16655311910708 30\n",
      "average reward exploit: MDP 1 180.90367940351766 30\n",
      "average reward exploit: MDP 1 200.88263979692564 30\n",
      "average reward exploit: MDP 1 204.66345294604693 30\n",
      "average reward exploit: MDP 1 186.57873256261362 30\n",
      "average reward explore: 101.97618799165254\n",
      "average reward exploit: MDP 2 157.9207707045349 30\n",
      "average reward exploit: MDP 2 186.58704219301893 30\n",
      "average reward exploit: MDP 2 240.0957732903659 30\n",
      "average reward exploit: MDP 2 243.9075612113894 30\n",
      "average reward exploit: MDP 2 198.91794209580289 30\n",
      "average reward exploit: MDP 2 233.1525791596035 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bde7b6456197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0850773e16af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch number: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0850773e16af>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;31m#             #print(tvars_vals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mexploit_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploit_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_paths_exploit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;31m# get observations, actions and rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mobservations_exploit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexploit_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0850773e16af>\u001b[0m in \u001b[0;36msample_paths_exploit\u001b[0;34m(self, env, Z, mdp_num, Test, num_episodes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploit_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_placeholder_exploit\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_input_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:  0\n",
      "average reward explore: 104.09820173365365\n",
      "average reward exploit: MDP 0 166.86632145751955 20\n",
      "average reward exploit: MDP 0 183.4032122924228 20\n",
      "average reward exploit: MDP 0 154.09359760237987 20\n",
      "average reward exploit: MDP 0 93.58866717512761 20\n",
      "average reward exploit: MDP 0 81.28630678202339 20\n",
      "average reward exploit: MDP 0 99.49544866039983 20\n",
      "average reward exploit: MDP 0 51.91129211021658 20\n",
      "average reward exploit: MDP 0 76.25835887783205 20\n",
      "average reward exploit: MDP 0 63.82478373073333 20\n",
      "average reward exploit: MDP 0 62.52060226816711 20\n",
      "average reward explore: 106.61991186249125\n",
      "average reward exploit: MDP 1 0.4813188188888587 20\n",
      "average reward exploit: MDP 1 0.9281975846860616 20\n",
      "average reward exploit: MDP 1 -0.595528252342379 20\n",
      "average reward exploit: MDP 1 -1.0613044349057945 20\n",
      "average reward exploit: MDP 1 -1.0804372160095206 20\n",
      "average reward exploit: MDP 1 -0.10899359941873948 20\n",
      "average reward exploit: MDP 1 -2.7097089327946833 20\n",
      "average reward exploit: MDP 1 -3.987689309893603 20\n",
      "average reward exploit: MDP 1 -3.134593231983366 20\n",
      "average reward exploit: MDP 1 -11.476079384980688 20\n",
      "average reward explore: 81.6981032674743\n",
      "average reward exploit: MDP 2 1.9964541659992183 20\n",
      "average reward exploit: MDP 2 6.563918096487205 20\n",
      "average reward exploit: MDP 2 41.41159150158054 20\n",
      "average reward exploit: MDP 2 40.19183996844688 20\n",
      "average reward exploit: MDP 2 132.33144156660086 20\n",
      "average reward exploit: MDP 2 80.72969597938125 20\n",
      "average reward exploit: MDP 2 151.06131187751035 20\n",
      "average reward exploit: MDP 2 130.91482957449577 20\n",
      "average reward exploit: MDP 2 115.54062032985954 20\n",
      "average reward exploit: MDP 2 113.0096139583889 20\n",
      "average reward explore: 91.5930622405144\n",
      "average reward exploit: MDP 3 127.32368004609081 20\n",
      "average reward exploit: MDP 3 138.20685933524356 20\n",
      "average reward exploit: MDP 3 122.08967938815515 20\n",
      "average reward exploit: MDP 3 124.30134416363559 20\n",
      "average reward exploit: MDP 3 122.22295444703683 20\n",
      "average reward exploit: MDP 3 138.86702763079373 20\n",
      "average reward exploit: MDP 3 127.36894104642528 20\n",
      "average reward exploit: MDP 3 117.6254279859809 20\n",
      "average reward exploit: MDP 3 130.67177334871943 20\n",
      "average reward exploit: MDP 3 111.5021840766373 20\n",
      "average reward explore: 117.63394896857683\n",
      "average reward exploit: MDP 4 111.2898288543403 20\n",
      "average reward exploit: MDP 4 90.32455898519925 20\n",
      "average reward exploit: MDP 4 93.31241704366013 20\n",
      "average reward exploit: MDP 4 62.23863486723213 20\n",
      "average reward exploit: MDP 4 97.96453587955446 20\n",
      "average reward exploit: MDP 4 87.2093940661011 20\n",
      "average reward exploit: MDP 4 103.4277151514228 20\n",
      "average reward exploit: MDP 4 149.03313943863301 20\n",
      "average reward exploit: MDP 4 110.52715578694003 20\n",
      "average reward exploit: MDP 4 104.17279722125959 20\n",
      "average reward explore: 169.64292875821232\n",
      "average reward exploit: MDP 5 67.86797415504641 20\n",
      "average reward exploit: MDP 5 83.94976541967999 20\n",
      "average reward exploit: MDP 5 79.62247874838151 20\n",
      "average reward exploit: MDP 5 105.47768472482967 20\n",
      "average reward exploit: MDP 5 147.51930994621898 20\n",
      "average reward exploit: MDP 5 133.96454343281363 20\n",
      "average reward exploit: MDP 5 129.46956402187223 20\n",
      "average reward exploit: MDP 5 127.19714933607466 20\n",
      "average reward exploit: MDP 5 130.63713140285628 20\n",
      "average reward exploit: MDP 5 139.823002077659 20\n",
      "average reward explore: 162.28043836691495\n",
      "average reward exploit: MDP 6 137.556763436363 20\n",
      "average reward exploit: MDP 6 149.62872786594892 20\n",
      "average reward exploit: MDP 6 126.60776865954463 20\n",
      "average reward exploit: MDP 6 118.41974447030772 20\n",
      "average reward exploit: MDP 6 123.78982067646271 20\n",
      "average reward exploit: MDP 6 137.12841360114518 20\n",
      "average reward exploit: MDP 6 130.3263707240746 20\n",
      "average reward exploit: MDP 6 100.46290602463468 20\n",
      "average reward exploit: MDP 6 123.86198726700619 20\n",
      "average reward exploit: MDP 6 86.02169344260409 20\n",
      "average reward explore: 164.57629034363148\n",
      "average reward exploit: MDP 7 104.62511115911515 20\n",
      "average reward exploit: MDP 7 107.62182241306992 20\n",
      "average reward exploit: MDP 7 100.21144885969994 20\n",
      "average reward exploit: MDP 7 97.24726241502194 20\n",
      "average reward exploit: MDP 7 111.65267840306471 20\n",
      "average reward exploit: MDP 7 105.5646998504469 20\n",
      "average reward exploit: MDP 7 98.74773790144924 20\n",
      "average reward exploit: MDP 7 141.84802666238227 20\n",
      "average reward exploit: MDP 7 70.37402665999419 20\n",
      "average reward exploit: MDP 7 101.86074997816291 20\n",
      "average reward explore: 155.99492244246207\n",
      "average reward exploit: MDP 8 137.89048415904628 20\n",
      "average reward exploit: MDP 8 90.60907747413306 20\n",
      "average reward exploit: MDP 8 135.10088893474352 20\n",
      "average reward exploit: MDP 8 133.92589358211094 20\n",
      "average reward exploit: MDP 8 132.57430847717714 20\n",
      "average reward exploit: MDP 8 114.84871953850897 20\n",
      "average reward exploit: MDP 8 103.97878365779667 20\n",
      "average reward exploit: MDP 8 120.1707855259468 20\n",
      "average reward exploit: MDP 8 112.34766662885798 20\n",
      "average reward exploit: MDP 8 115.87879265395834 20\n",
      "average reward explore: 152.48281078233214\n",
      "average reward exploit: MDP 9 107.9071290086277 20\n",
      "average reward exploit: MDP 9 91.40149297061672 20\n",
      "average reward exploit: MDP 9 107.58944921531489 20\n",
      "average reward exploit: MDP 9 101.06592138919386 20\n",
      "average reward exploit: MDP 9 77.66367232181234 20\n",
      "average reward exploit: MDP 9 82.39196805686927 20\n",
      "average reward exploit: MDP 9 93.2724519205291 20\n",
      "average reward exploit: MDP 9 101.78694891813923 20\n",
      "average reward exploit: MDP 9 91.69831859025356 20\n",
      "average reward exploit: MDP 9 89.10042277172899 20\n",
      "epoch number:  1\n",
      "average reward explore: 255.8530486829026\n",
      "average reward exploit: MDP 0 132.89394850966494 20\n",
      "average reward exploit: MDP 0 196.25548497380433 20\n",
      "average reward exploit: MDP 0 156.73445537764715 20\n",
      "average reward exploit: MDP 0 142.92811400874626 20\n",
      "average reward exploit: MDP 0 188.91366145158548 20\n",
      "average reward exploit: MDP 0 192.00007033158298 20\n",
      "average reward exploit: MDP 0 163.81549687771857 20\n",
      "average reward exploit: MDP 0 190.83219210808343 20\n",
      "average reward exploit: MDP 0 209.27616510472748 20\n",
      "average reward exploit: MDP 0 159.88708897583567 20\n",
      "average reward explore: 241.59491369709536\n",
      "average reward exploit: MDP 1 101.99159685632804 20\n",
      "average reward exploit: MDP 1 113.9608694343611 20\n",
      "average reward exploit: MDP 1 110.76546105791229 20\n",
      "average reward exploit: MDP 1 131.85501685346173 20\n",
      "average reward exploit: MDP 1 130.13500204297242 20\n",
      "average reward exploit: MDP 1 152.11744960645802 20\n",
      "average reward exploit: MDP 1 147.66956411232894 20\n",
      "average reward exploit: MDP 1 161.1692914394165 20\n",
      "average reward exploit: MDP 1 103.85787347016476 20\n",
      "average reward exploit: MDP 1 111.91922873277127 20\n",
      "average reward explore: 228.55931811223763\n",
      "average reward exploit: MDP 2 124.94298556647259 20\n",
      "average reward exploit: MDP 2 129.17800072129234 20\n",
      "average reward exploit: MDP 2 106.82082898289927 20\n",
      "average reward exploit: MDP 2 101.38202609392123 20\n",
      "average reward exploit: MDP 2 115.41786442840335 20\n",
      "average reward exploit: MDP 2 82.12065759724244 20\n",
      "average reward exploit: MDP 2 95.64636898299968 20\n",
      "average reward exploit: MDP 2 71.93394054982679 20\n",
      "average reward exploit: MDP 2 92.65398315644171 20\n",
      "average reward exploit: MDP 2 79.13318506226389 20\n",
      "average reward explore: 257.7206672685358\n",
      "average reward exploit: MDP 3 102.98439230767639 20\n",
      "average reward exploit: MDP 3 88.89408098660036 20\n",
      "average reward exploit: MDP 3 108.05417553547225 20\n",
      "average reward exploit: MDP 3 62.33884669960607 20\n",
      "average reward exploit: MDP 3 143.18657702547958 20\n",
      "average reward exploit: MDP 3 85.13443914477367 20\n",
      "average reward exploit: MDP 3 115.3696151472317 20\n",
      "average reward exploit: MDP 3 111.2355581881633 20\n",
      "average reward exploit: MDP 3 98.53779979701764 20\n",
      "average reward exploit: MDP 3 122.05122207833672 20\n",
      "average reward explore: 242.2382919096317\n",
      "average reward exploit: MDP 4 74.17522031608816 20\n",
      "average reward exploit: MDP 4 89.14245445401818 20\n",
      "average reward exploit: MDP 4 87.53521966460106 20\n",
      "average reward exploit: MDP 4 88.51644439367846 20\n",
      "average reward exploit: MDP 4 90.25643441708552 20\n",
      "average reward exploit: MDP 4 90.28016034007263 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 4 99.59348091036624 20\n",
      "average reward exploit: MDP 4 131.11327910811931 20\n",
      "average reward exploit: MDP 4 121.39283105828756 20\n",
      "average reward exploit: MDP 4 98.82237610135637 20\n",
      "average reward explore: 283.65726497216275\n",
      "average reward exploit: MDP 5 105.83176213826464 20\n",
      "average reward exploit: MDP 5 108.51197548030278 20\n",
      "average reward exploit: MDP 5 94.36767179666133 20\n",
      "average reward exploit: MDP 5 99.27896949530863 20\n",
      "average reward exploit: MDP 5 113.12966335067223 20\n",
      "average reward exploit: MDP 5 79.4179255851247 20\n",
      "average reward exploit: MDP 5 109.05871493498407 20\n",
      "average reward exploit: MDP 5 80.1146796514033 20\n",
      "average reward exploit: MDP 5 103.13950772259408 20\n",
      "average reward exploit: MDP 5 87.51761412972165 20\n",
      "average reward explore: 253.89450899737895\n",
      "average reward exploit: MDP 6 105.15216594323141 20\n",
      "average reward exploit: MDP 6 71.72861705894293 20\n",
      "average reward exploit: MDP 6 115.8543856039681 20\n",
      "average reward exploit: MDP 6 77.10598170436482 20\n",
      "average reward exploit: MDP 6 97.5317105662639 20\n",
      "average reward exploit: MDP 6 98.6713419342772 20\n",
      "average reward exploit: MDP 6 126.15704740325097 20\n",
      "average reward exploit: MDP 6 123.85959900486222 20\n",
      "average reward exploit: MDP 6 79.52520783150727 20\n",
      "average reward exploit: MDP 6 85.71671845535889 20\n",
      "average reward explore: 306.5408290714581\n",
      "average reward exploit: MDP 7 81.35584028728667 20\n",
      "average reward exploit: MDP 7 80.14424776089909 20\n",
      "average reward exploit: MDP 7 68.41067785251963 20\n",
      "average reward exploit: MDP 7 87.37302478872651 20\n",
      "average reward exploit: MDP 7 43.35909353172209 20\n",
      "average reward exploit: MDP 7 39.81687353288506 20\n",
      "average reward exploit: MDP 7 57.23584902676394 20\n",
      "average reward exploit: MDP 7 97.97306076344054 20\n",
      "average reward exploit: MDP 7 78.66660946619852 20\n",
      "average reward exploit: MDP 7 76.49062073764115 20\n",
      "average reward explore: 234.03762687777504\n",
      "average reward exploit: MDP 8 66.5999281596728 20\n",
      "average reward exploit: MDP 8 38.456877018905494 20\n",
      "average reward exploit: MDP 8 73.47378267848072 20\n",
      "average reward exploit: MDP 8 70.34778619215426 20\n",
      "average reward exploit: MDP 8 57.39183745946832 20\n",
      "average reward exploit: MDP 8 57.520980673318576 20\n",
      "average reward exploit: MDP 8 35.239231838710246 20\n",
      "average reward exploit: MDP 8 54.794577528128 20\n",
      "average reward exploit: MDP 8 65.48580848847877 20\n",
      "average reward exploit: MDP 8 35.3231148582127 20\n",
      "average reward explore: 186.86697562018676\n",
      "average reward exploit: MDP 9 89.60026911185763 20\n",
      "average reward exploit: MDP 9 41.42367457023786 20\n",
      "average reward exploit: MDP 9 39.157648162126875 20\n",
      "average reward exploit: MDP 9 46.95331961207221 20\n",
      "average reward exploit: MDP 9 59.367573505695916 20\n",
      "average reward exploit: MDP 9 62.809050320871805 20\n",
      "average reward exploit: MDP 9 62.195666682471035 20\n",
      "average reward exploit: MDP 9 30.59286545737317 20\n",
      "average reward exploit: MDP 9 44.87226698066932 20\n",
      "average reward exploit: MDP 9 57.06609261628628 20\n",
      "epoch number:  2\n",
      "average reward explore: 229.79649260824627\n",
      "average reward exploit: MDP 0 154.5086183935687 20\n",
      "average reward exploit: MDP 0 152.29193850091735 20\n",
      "average reward exploit: MDP 0 189.6552524704149 20\n",
      "average reward exploit: MDP 0 181.30611989281527 20\n",
      "average reward exploit: MDP 0 178.6439245854498 20\n",
      "average reward exploit: MDP 0 191.4091611300317 20\n",
      "average reward exploit: MDP 0 176.39631593049546 20\n",
      "average reward exploit: MDP 0 155.23857467559142 20\n",
      "average reward exploit: MDP 0 188.00936606599632 20\n",
      "average reward exploit: MDP 0 157.74798374405498 20\n",
      "average reward explore: 339.2663018517914\n",
      "average reward exploit: MDP 1 152.3803482306438 20\n",
      "average reward exploit: MDP 1 150.95262974754635 20\n",
      "average reward exploit: MDP 1 162.46692873987297 20\n",
      "average reward exploit: MDP 1 159.0787002757707 20\n",
      "average reward exploit: MDP 1 135.5346656878921 20\n",
      "average reward exploit: MDP 1 138.67033729541512 20\n",
      "average reward exploit: MDP 1 103.77808619480354 20\n",
      "average reward exploit: MDP 1 95.97511047413873 20\n",
      "average reward exploit: MDP 1 109.22957425465215 20\n",
      "average reward exploit: MDP 1 124.12800858743415 20\n",
      "average reward explore: 230.6282319813632\n",
      "average reward exploit: MDP 2 93.71773396670577 20\n",
      "average reward exploit: MDP 2 69.41667858013251 20\n",
      "average reward exploit: MDP 2 71.98401470458695 20\n",
      "average reward exploit: MDP 2 56.703077862647575 20\n",
      "average reward exploit: MDP 2 62.93907963038111 20\n",
      "average reward exploit: MDP 2 46.28733191727398 20\n",
      "average reward exploit: MDP 2 74.0968926823847 20\n",
      "average reward exploit: MDP 2 41.07109962926366 20\n",
      "average reward exploit: MDP 2 45.25293316400468 20\n",
      "average reward exploit: MDP 2 56.567583901731425 20\n",
      "average reward explore: 255.76784906010556\n",
      "average reward exploit: MDP 3 9.904414927317157 20\n",
      "average reward exploit: MDP 3 -0.9829701968433416 20\n",
      "average reward exploit: MDP 3 9.254020175781973 20\n",
      "average reward exploit: MDP 3 0.07064467126036042 20\n",
      "average reward exploit: MDP 3 8.300152539481884 20\n",
      "average reward exploit: MDP 3 -5.237153119098164 20\n",
      "average reward exploit: MDP 3 5.626659457363321 20\n",
      "average reward exploit: MDP 3 8.186969132636944 20\n",
      "average reward exploit: MDP 3 15.53999442038938 20\n",
      "average reward exploit: MDP 3 6.841703127816865 20\n",
      "average reward explore: 296.03210055885245\n",
      "average reward exploit: MDP 4 -1.1789385446896261 20\n",
      "average reward exploit: MDP 4 6.115087166106769 20\n",
      "average reward exploit: MDP 4 9.038414726953908 20\n",
      "average reward exploit: MDP 4 9.213808888358704 20\n",
      "average reward exploit: MDP 4 16.5430152110651 20\n",
      "average reward exploit: MDP 4 29.8281111966082 20\n",
      "average reward exploit: MDP 4 20.81506878582609 20\n",
      "average reward exploit: MDP 4 23.74655093286831 20\n",
      "average reward exploit: MDP 4 42.071113492994655 20\n",
      "average reward exploit: MDP 4 27.86041859752545 20\n",
      "average reward explore: 309.44924437025304\n",
      "average reward exploit: MDP 5 34.62727002014548 20\n",
      "average reward exploit: MDP 5 35.88016230960939 20\n",
      "average reward exploit: MDP 5 43.025131282772186 20\n",
      "average reward exploit: MDP 5 56.87680514738209 20\n",
      "average reward exploit: MDP 5 43.18585714627293 20\n",
      "average reward exploit: MDP 5 48.46399241397588 20\n",
      "average reward exploit: MDP 5 56.45945264278529 20\n",
      "average reward exploit: MDP 5 39.62810875507353 20\n",
      "average reward exploit: MDP 5 36.29574401699991 20\n",
      "average reward exploit: MDP 5 44.82859404090349 20\n",
      "average reward explore: 255.32936732426933\n",
      "average reward exploit: MDP 6 68.4794139625803 20\n",
      "average reward exploit: MDP 6 72.8700960626086 20\n",
      "average reward exploit: MDP 6 77.17956078579978 20\n",
      "average reward exploit: MDP 6 71.27287691783887 20\n",
      "average reward exploit: MDP 6 81.01512783145401 20\n",
      "average reward exploit: MDP 6 107.3379703596058 20\n",
      "average reward exploit: MDP 6 75.38328574702054 20\n",
      "average reward exploit: MDP 6 99.67956911993916 20\n",
      "average reward exploit: MDP 6 99.62350685733351 20\n",
      "average reward exploit: MDP 6 111.15958859112283 20\n",
      "average reward explore: 288.6842572776233\n",
      "average reward exploit: MDP 7 97.86628065195404 20\n",
      "average reward exploit: MDP 7 127.29776943909896 20\n",
      "average reward exploit: MDP 7 107.6184089491579 20\n",
      "average reward exploit: MDP 7 99.40354290518754 20\n",
      "average reward exploit: MDP 7 116.88125991928706 20\n",
      "average reward exploit: MDP 7 115.21651414819121 20\n",
      "average reward exploit: MDP 7 109.1141274361939 20\n",
      "average reward exploit: MDP 7 122.30416663190393 20\n",
      "average reward exploit: MDP 7 135.7316101352975 20\n",
      "average reward exploit: MDP 7 130.2146607013494 20\n",
      "average reward explore: 290.9063190481387\n",
      "average reward exploit: MDP 8 129.48764082505713 20\n",
      "average reward exploit: MDP 8 168.6183127349989 20\n",
      "average reward exploit: MDP 8 138.9195662114973 20\n",
      "average reward exploit: MDP 8 149.44565665371115 20\n",
      "average reward exploit: MDP 8 140.08584803292368 20\n",
      "average reward exploit: MDP 8 152.66349633316588 20\n",
      "average reward exploit: MDP 8 133.88648651537358 20\n",
      "average reward exploit: MDP 8 142.68009166728638 20\n",
      "average reward exploit: MDP 8 141.42408834665991 20\n",
      "average reward exploit: MDP 8 143.28013531357865 20\n",
      "average reward explore: 168.2869960422953\n",
      "average reward exploit: MDP 9 115.8066938920703 20\n",
      "average reward exploit: MDP 9 101.28795307644307 20\n",
      "average reward exploit: MDP 9 101.24984005711975 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 9 102.62062753911911 20\n",
      "average reward exploit: MDP 9 140.21430952999145 20\n",
      "average reward exploit: MDP 9 112.25976343496177 20\n",
      "average reward exploit: MDP 9 103.53500006316767 20\n",
      "average reward exploit: MDP 9 90.33887190384088 20\n",
      "average reward exploit: MDP 9 102.09830230642318 20\n",
      "average reward exploit: MDP 9 99.5513367393247 20\n",
      "epoch number:  3\n",
      "average reward explore: 272.3533577812179\n",
      "average reward exploit: MDP 0 207.1527397234521 20\n",
      "average reward exploit: MDP 0 195.22497482762222 20\n",
      "average reward exploit: MDP 0 200.97991327209516 20\n",
      "average reward exploit: MDP 0 190.17917639763306 20\n",
      "average reward exploit: MDP 0 202.6974567696393 20\n",
      "average reward exploit: MDP 0 220.7338009211581 20\n",
      "average reward exploit: MDP 0 184.3872221299447 20\n",
      "average reward exploit: MDP 0 191.7943854967473 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5fdad9ea6bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch number: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0850773e16af>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;31m#             #print(tvars_vals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mexploit_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploit_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_paths_exploit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;31m# get observations, actions and rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mobservations_exploit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexploit_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0850773e16af>\u001b[0m in \u001b[0;36msample_paths_exploit\u001b[0;34m(self, env, Z, mdp_num, Test, num_episodes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploit_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_placeholder_exploit\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_input_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "        print(\"epoch number: \", epoch)\n",
    "        a.train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_paths, explore_rewards = a.sample_paths_explore(a.env,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: 270.30608302184663\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "M = a.stack_trajectories(explore_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit 5081.8546886311515 800\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_traj = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    explore_paths, explore_rewards = a.sample_paths_explore(a.env,30*np.random.rand())\n",
    "    print(\"gravity:\", a.sim.model.opt.gravity)\n",
    "    print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "    M = a.stack_trajectories(explore_paths)\n",
    "    Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "    Z = np.reshape(Z,[1,len(Z)])\n",
    "    exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1)\n",
    "    print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "returns_explore = a.get_returns(explore_paths, explore = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array(a.stack_trajectories(explore_paths))\n",
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"average reward explore: MDP\", np.sum(explore_rewards)/num_traj, len(explore_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average reward exploit: MDP\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_pendulum(env):\n",
    "    m = config['pendulum_mass_min'] + np.random.rand()*(config['pendulum_mass_max'] - config['pendulum_mass_min'])\n",
    "    l = config['pendulum_len_min'] + np.random.rand()*(config['pendulum_len_max'] - config['pendulum_len_min'])\n",
    "    env.m = m\n",
    "    env.l = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_hopper(env):\n",
    "    ts = config['torso_min'] + np.random.rand()*(config['torso_max'] - config['torso_min'])\n",
    "    f = config['friction_min'] + np.random.rand()*(config['friction_max'] - config['friction_min'])\n",
    "    \n",
    "    env.friction = f\n",
    "    env.torso_size = ts\n",
    "    env.apply_env_modifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "cfg_filename = 'hopper-config.yml'\n",
    "with open(cfg_filename,'r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Randomizer(gym.make('Hopper-v2'), randomize_hopper)\n",
    "env.unwrapped.__dict__.keys()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = config['pendulum_mass_min'] + np.random.rand()*(config['pendulum_mass_max'] - config['pendulum_mass_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'Hopper-v2'\n",
    "e = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.unwrapped.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.unwrapped.apply_env_modifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mujoco_py.cymj.PyMjOption at 0x1c23ba8ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sim.model.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
