{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow.contrib.layers as layers\n",
    "from mujoco_py import load_model_from_xml, MjSim, MjViewer\n",
    "import policies\n",
    "import value_functions as vfuncs\n",
    "import utils_pg as utils\n",
    "\n",
    "# Environment setup\n",
    "#env = \"CartPole-v0\"\n",
    "#env=\"InvertedPendulum-v2\"\n",
    "#env = \"Hopper-v2\"\n",
    "env=\"HalfCheetah-v2\"\n",
    "\n",
    "# discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "# observation_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "max_ep_len = 1000\n",
    "num_traj = 10\n",
    "#traj_length = max_ep_len*(observation_dim + 2)\n",
    "latent_size = 25\n",
    "use_baseline = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feed forward network (multi-layer-perceptron, or mlp)\n",
    "# def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "#     '''\n",
    "#     Build a feed forward network\n",
    "#     '''\n",
    "#     Input = mlp_input\n",
    "#     with tf.variable_scope(scope):\n",
    "#         # Dense Layers\n",
    "#         for i in range(n_layers-1):\n",
    "#             dense = tf.layers.dense(inputs = Input, units = size, activation = tf.nn.relu, bias_initializer=tf.constant_initializer(1.0))\n",
    "#             Input = dense\n",
    "#         # Fully Connected Layer\n",
    "#         out = layers.fully_connected(inputs = Input, num_outputs = output_size, activation_fn=output_activation)\n",
    "#     return out\n",
    "\n",
    "def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "\n",
    "        hidden1 = layers.fully_connected(mlp_input,\n",
    "                num_outputs=32,\n",
    "                weights_initializer=layers.xavier_initializer(uniform=True),\n",
    "                activation_fn=tf.nn.relu)\n",
    "        hidden2 = layers.fully_connected(hidden1,\n",
    "                num_outputs=32,\n",
    "                weights_initializer=layers.xavier_initializer(uniform=True),\n",
    "                activation_fn=tf.nn.relu)\n",
    "        mean_na = layers.fully_connected(hidden2,\n",
    "                num_outputs=output_size,\n",
    "                weights_initializer=layers.xavier_initializer(uniform=True),\n",
    "                activation_fn=None)\n",
    "\n",
    "        return mean_na "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner():\n",
    "    def __init__(self, env, max_ep_len, num_traj,latent_size ):\n",
    "        self.env = gym.make(env)\n",
    "        self.discrete = isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.num_traj = num_traj\n",
    "        self.traj_length = self.max_ep_len*(self.observation_dim + 1 + self.env.action_space.shape[0]) # TO Change\n",
    "        self.use_baseline = True\n",
    "        self.latent_size = latent_size\n",
    "        self.feature_size = self.observation_dim + 1 + self.env.action_space.shape[0] # HC\n",
    "        self.lr = 3e-2\n",
    "        self.num_layers = 1\n",
    "        self.layers_size = 16\n",
    "        self.num_mdps = 10\n",
    "        self.model = self.env.env.model\n",
    "        self.sim = MjSim(self.model)\n",
    "        self.gamma = .95\n",
    "        self.decoder_len = 16\n",
    "\n",
    "        # build model\n",
    "        #self.ConstructGraph()\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        self.observation_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        if(self.discrete):\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.int32, shape=(None))\n",
    "            self.action_placeholder_exploit = tf.placeholder(tf.int32, shape=(None))\n",
    "        else:\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "            self.action_placeholder_exploit= tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "        \n",
    "        ##########################\n",
    "        self.n     = tf.shape(self.observation_placeholder_explore)[0] \n",
    "        self.oldlogstd_a  = tf.placeholder(name=\"oldlogstd\", shape=[self.action_dim], dtype=tf.float32)\n",
    "        ##########################\n",
    "        \n",
    "        self.baseline_target_placeholder = tf.placeholder(tf.float32, shape= None)\n",
    "        self.advantage_placeholder_explore = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "        #self.encoder_input_placeholder = tf.placeholder(tf.float32, shape= (self.num_traj,self.traj_length))\n",
    "        self.encoder_input_placeholder = tf.placeholder(tf.float32, [None, None, self.feature_size])\n",
    "        self.decoder_input_placeholder = tf.placeholder(tf.float32, shape= (1,self.latent_size))\n",
    "        self.sequence_length_placeholder = tf.placeholder(tf.int32, [None, ])\n",
    "        \n",
    "        self.observation_placeholder_exploit = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        #TODO\n",
    "        self.advantage_placeholder_exploit = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "    def build_policy_explore(self, scope = \"policy_explore\"):\n",
    "        if (self.discrete):\n",
    "            self.action_logits = build_mlp(self.observation_placeholder_explore,self.action_dim,scope = scope,n_layers=self.num_layers,size = self.layers_size,output_activation=None)\n",
    "            self.explore_action = tf.multinomial(self.action_logits,1)\n",
    "            self.explore_action = tf.squeeze(self.explore_action, axis=1)\n",
    "            self.explore_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.action_logits, labels = self.action_placeholder_explore)\n",
    "\n",
    "        else:   \n",
    "            action_means = build_mlp(self.observation_placeholder_explore,self.action_dim,scope,n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "            \n",
    "            self.logstd_a     = tf.get_variable(\"logstd\", [self.action_dim], initializer=tf.zeros_initializer())\n",
    "            self.logstd_na    = tf.ones(shape=(self.n,self.action_dim), dtype=tf.float32) * self.logstd_a\n",
    "            self.oldlogstd_na = tf.ones(shape=(self.n,self.action_dim), dtype=tf.float32) * self.oldlogstd_a\n",
    "            \n",
    "            self.explore_logprob  = utils.gauss_log_prob(mu=action_means, logstd=self.logstd_na, x=self.action_placeholder_explore)\n",
    "            self.explore_action =   action_means + tf.multiply(tf.exp(self.logstd_na),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            \n",
    "#             init = tf.constant(np.random.rand(1, 2))\n",
    "#             log_std = tf.get_variable(\"log_std\", [self.action_dim])\n",
    "#             self.explore_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "#             mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "#             self.explore_logprob =  mvn.log_prob(value = self.action_placeholder_explore, name='log_prob')\n",
    "    \n",
    "    \n",
    "    def build_policy_exploit(self, scope = \"policy_exploit\"):\n",
    "        if(self.discrete):\n",
    "            #self.exploit_action_logits = (tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W1) + self.d_B1), self.d_W2) + self.d_B2),self.d_W3) + self.d_B3)\n",
    "            self.exploit_action_logits = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            self.exploit_action = tf.multinomial(self.exploit_action_logits,1)\n",
    "            self.exploit_action = tf.squeeze(self.exploit_action, axis=1)\n",
    "            self.exploit_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.exploit_action_logits, labels = self.action_placeholder_exploit)\n",
    "        else:\n",
    "            \n",
    "            action_means = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"exploit_log_prob\", [self.action_dim])\n",
    "            self.exploit_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            self.exploit_logprob =  mvn.log_prob(value = self.action_placeholder_exploit, name='exploit_log_prob')\n",
    "\n",
    "        #self.loss_grads_exploit = self.exploit_logprob * self.advantage_placeholder_exploit\n",
    "        \n",
    "    def NNEncoder(self, scope = \"NNEncoder\"):\n",
    "        self.Z = build_mlp(self.encoder_input_placeholder,self.latent_size,scope = scope,n_layers=3,size = 60,output_activation=None)\n",
    "    \n",
    " \n",
    "\n",
    "    # input [num_traj, length, features (obs + action + reward) ]\n",
    "    def LSTMEncoder(self, scope = \"LSTMEncoder\"):\n",
    "        self.hidden_size = 64\n",
    "        initializer = tf.random_uniform_initializer(-1, 1)\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, self.feature_size, initializer=initializer)\n",
    "        cell_out = tf.contrib.rnn.OutputProjectionWrapper(cell, self.latent_size)\n",
    "        self.output, _ = tf.nn.dynamic_rnn(cell_out,self.encoder_input_placeholder,self.sequence_length_placeholder,dtype=tf.float32,)\n",
    "        batch_size = tf.shape(self.output)[0]\n",
    "        max_length = tf.shape(self.output)[1]\n",
    "        out_size = int(self.output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (self.sequence_length_placeholder - 1)\n",
    "        flat = tf.reshape(self.output, [-1, out_size])\n",
    "        self.Z = tf.reduce_mean(tf.gather(flat, index), axis = 0)\n",
    "        \n",
    "    \n",
    "    def Decoder(self, decoder_out_dim, scope):\n",
    "        return build_mlp(self.decoder_input_placeholder,decoder_out_dim,scope = scope,n_layers=1,size = 64,output_activation=None)\n",
    "    \n",
    "    \n",
    "    def sample_paths_explore(self, env,mdp_num,Test = False, num_episodes = None):\n",
    "        paths = []\n",
    "        self.length = []\n",
    "        episode_rewards = []\n",
    "        #self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        for i in range(self.num_traj):\n",
    "            pad = False\n",
    "            state = env.reset()\n",
    "            new_states,states,new_actions, actions, rewards,new_rewards = [], [], [], [], [],[]\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                if (pad):\n",
    "                    states.append([0]*self.observation_dim)\n",
    "                    if(self.discrete):\n",
    "                        actions.append(0)\n",
    "                    else:\n",
    "                        actions.append([0]*self.action_dim)\n",
    "                    rewards.append(0)\n",
    "                else:\n",
    "                    states.append(state)\n",
    "                    new_states.append(state)\n",
    "                    action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\n",
    "                    #action = self.policyfn.sample_action(state) #NEW\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    actions.append(action)\n",
    "                    new_actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "                    new_rewards.append(reward)\n",
    "                    episode_reward += reward\n",
    "                    if (done or step == self.max_ep_len-1):\n",
    "                        self.length.append(step + 1)\n",
    "                        pad = True \n",
    "                        \n",
    "            episode_rewards.append(episode_reward)            \n",
    "            #print(\"explore\",np.array(actions))\n",
    "            path = {\"new_obs\" : np.array(new_states),\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\"new_acs\" : np.array(new_actions),\n",
    "                                \"action\" : np.array(actions),\"new_rewards\":np.array(new_rewards)}\n",
    "            paths.append(path)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def sample_paths_exploit(self, env,Z,mdp_num,Test = False, num_episodes = None):\n",
    "        #self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        \n",
    "        paths = []\n",
    "        num = 0\n",
    "        episode_rewards = []\n",
    "        for i in range(self.num_traj):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.sess.run(self.exploit_action, feed_dict={self.observation_placeholder_exploit : state[None], self.decoder_input_placeholder: Z})[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "            #print(\"exploit\",np.array(actions))\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    " \n",
    "        #print(\"exploit success: \", num)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def get_returns(self,paths, explore = False):\n",
    "        all_returns = []\n",
    "        m = 0\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            if(explore):\n",
    "                length = self.length[m]\n",
    "            else:\n",
    "                length = len(rewards)\n",
    "            for i in range(length):\n",
    "                path_returns = 0\n",
    "                k = 0\n",
    "                for j in range(i,length):\n",
    "                    path_returns = path_returns + rewards[j]*(self.gamma)**k\n",
    "                    k = k+1\n",
    "                returns.append(path_returns)\n",
    "            all_returns.append(returns)\n",
    "            m+=1\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "    \n",
    "    def stack_trajectories(self,paths):\n",
    "        trajectories = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            states = path[\"observation\"]\n",
    "            action = path[\"action\"]\n",
    "            \n",
    "            SAR = []\n",
    "            for i in range(len(states)):\n",
    "                SAR.append(list(states[i]) + list(action[i]) + [rewards[i]])\n",
    "            trajectories.append(SAR)\n",
    "        return np.array(trajectories)\n",
    "    \n",
    "    def addBaseline(self):\n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "\n",
    "    def calculate_advantage(self,returns, observations):\n",
    "        if (self.use_baseline):\n",
    "            baseline = self.sess.run(self.baseline, {input_placeholder:observations})\n",
    "            adv = returns - baseline\n",
    "            adv = (adv - np.mean(adv))/np.std(adv)\n",
    "        else:\n",
    "            adv = returns\n",
    "        return adv\n",
    "    \n",
    "\n",
    "        \n",
    "    def ConstructGraph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.add_placeholders()\n",
    "        \n",
    "        self.build_policy_explore()\n",
    "        self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "        \n",
    "        #self.vf = vfuncs.NnValueFunction(session=self.sess,ob_dim=self.observation_dim)\n",
    "\n",
    "        \n",
    "        \n",
    "#         self.add_placeholders()\n",
    "        \n",
    "#         self.build_policy_explore()\n",
    "#         self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "#         self.loss_grads_explore = -self.explore_logprob * self.advantage_placeholder_explore\n",
    "#         self.tvars_explore = tf.trainable_variables()\n",
    "#         self.gradients_explore = tf.gradients(self.explore_policy_loss,self.tvars_explore)\n",
    "        \n",
    "        self.addBaseline()\n",
    "        \n",
    "#         self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=1, size = 16,output_activation=None)\n",
    "#         self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "#         baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "#         self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "        \n",
    "#         #Encoder LSTM\n",
    "        self.LSTMEncoder()\n",
    "        \n",
    "        \n",
    "        #decoder weights\n",
    "        #self.d_W1 = self.Decoder(scope = \"W1\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W2 = self.Decoder(scope = \"W2\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.decoder_len*action_dim)\n",
    "        self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.observation_dim*self.action_dim)\n",
    "        \n",
    "        #self.d_W1 = ((self.d_W1 - (tf.reduce_max(self.d_W1) + tf.reduce_min(self.d_W1))/2)/(tf.reduce_max(self.d_W1) - tf.reduce_min(self.d_W1)))*2 \n",
    "        #self.d_W2 = ((self.d_W2 - (tf.reduce_max(self.d_W2) + tf.reduce_min(self.d_W2))/2)/(tf.reduce_max(self.d_W2) - tf.reduce_min(self.d_W2)))*2 \n",
    "        #self.d_W3 = ((self.d_W3 - (tf.reduce_max(self.d_W3) + tf.reduce_min(self.d_W3))/2)/(tf.reduce_max(self.d_W3) - tf.reduce_min(self.d_W3)))*2 \n",
    "        \n",
    "        #self.d_W1 = tf.reshape(self.d_W1, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W2 = tf.reshape(self.d_W2, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W3 = tf.reshape(self.d_W3, [self.decoder_len, self.action_dim])\n",
    "        self.d_W3 = tf.reshape(self.d_W3, [self.observation_dim, self.action_dim])\n",
    "        \n",
    "        # decoder output bias\n",
    "        #self.d_B1 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B1\"), [self.decoder_len])\n",
    "        #self.d_B2 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B2\"), [self.decoder_len])\n",
    "        self.d_B3 = tf.reshape(self.Decoder(decoder_out_dim = self.action_dim, scope = \"B3\"), [self.action_dim])\n",
    "        #self.d_B1 = ((self.d_B1 - (tf.reduce_max(self.d_B1) + tf.reduce_min(self.d_B1))/2)/(tf.reduce_max(self.d_B1) - tf.reduce_min(self.d_B1)))*2 \n",
    "        #self.d_B2 = ((self.d_B2 - (tf.reduce_max(self.d_B2) + tf.reduce_min(self.d_B2))/2)/(tf.reduce_max(self.d_B2) - tf.reduce_min(self.d_B2)))*2 \n",
    "        \n",
    "        \n",
    "        # exploit policy\n",
    "        self.build_policy_exploit()\n",
    "        #self.d = [self.d_W1, self.d_B1, self.d_W2, self.d_B2, self.d_W3, self.d_B3]\n",
    "        self.exploit_policy_loss = -tf.reduce_sum(self.exploit_logprob * self.advantage_placeholder_exploit)\n",
    "        self.d = [self.d_W3, self.d_B3]\n",
    "        self.gradients_exploit = tf.gradients(self.exploit_policy_loss,self.d)\n",
    "        \n",
    "        # train encoder and decoder\n",
    "        adam_optimizer_exploit =  tf.train.AdamOptimizer(self.lr*0.3)\n",
    "        self.output_train_op = adam_optimizer_exploit.minimize(self.exploit_policy_loss)\n",
    "        # train original network\n",
    "        adam_optimizer_explore = tf.train.AdamOptimizer(self.lr)\n",
    "        self.input_train_op = adam_optimizer_explore.minimize(self.explore_policy_loss)\n",
    "\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.ConstructGraph()\n",
    "        # create tf session\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # initiliaze all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def train_step(self): \n",
    "        # sample num_traj*num_MDPs\n",
    "        for i in range(self.num_mdps):\n",
    "            explore_paths, explore_rewards = self.sample_paths_explore(self.env,i)\n",
    "            observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "            new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "            new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "            actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "            rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "            returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "            \n",
    "#             vtargs, vpreds, advantages_explore = [], [], []\n",
    "#             for path in explore_paths:\n",
    "#                 rew_t = path[\"new_rewards\"]\n",
    "#                 return_t = utils.discount(rew_t, self.gamma)\n",
    "#                 vpred_t = self.vf.predict(path[\"new_obs\"])\n",
    "#                 adv_t = return_t - vpred_t\n",
    "#                 advantages_explore.append(adv_t)\n",
    "#                 vtargs.append(return_t)\n",
    "#                 vpreds.append(vpred_t)\n",
    "                \n",
    "                \n",
    "#             #returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "#             advantages_explore = np.concatenate(advantages_explore)\n",
    "#             advantages_explore = (advantages_explore - advantages_explore.mean()) / (advantages_explore.std() + 1e-8)\n",
    "#             vtarg_n = np.concatenate(vtargs)\n",
    "#             vpred_n = np.concatenate(vpreds)\n",
    "#             #update baseline\n",
    "#             #print(advantages_explore.shape,new_observations_explore.shape)\n",
    "#             self.vf.fit(new_observations_explore, vtarg_n)\n",
    "            \n",
    "#             surr_loss, oldmean_na, oldlogstd_a = self.policyfn.update_policy(\n",
    "#                     new_observations_explore, new_actions_explore, advantages_explore, self.lr)\n",
    "            \n",
    "            print(\"average reward explore: MDP\", i, np.mean(explore_rewards), len(explore_rewards))\n",
    "            \n",
    "            #print(returns_explore)\n",
    "#             print(\"average reward explore: MDP\", i, np.sum(explore_rewards)/num_traj, len(explore_rewards))\n",
    "\n",
    "            baseline_explore = self.sess.run(self.baseline, {self.observation_placeholder_explore:new_observations_explore})\n",
    "            adv = returns_explore - np.squeeze(baseline_explore)\n",
    "            advantages_explore = (adv - np.mean(adv))/(np.std(adv) + 1e-12)\n",
    "\n",
    "\n",
    "            # update the baseline\n",
    "\n",
    "            self.sess.run(self.update_baseline_op, {self.observation_placeholder_explore:new_observations_explore, \n",
    "                                           self.baseline_target_placeholder : returns_explore})\n",
    "\n",
    "            # calculate explore gradients\n",
    "    #         grads_explore = self.sess.run(self.gradients_explore, feed_dict={\n",
    "    #                     self.observation_placeholder_explore : observations_explore,\n",
    "    #                     self.action_placeholder_explore : actions_explore,\n",
    "    #                     self.advantage_placeholder_explore : returns_explore})\n",
    "            #print(\"explore\",grads_explore )\n",
    "            # form trajectory matrix\n",
    "\n",
    "            M = np.array(self.stack_trajectories(explore_paths))\n",
    "\n",
    "\n",
    "            #encoder LSTM\n",
    "            Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "            Z = np.reshape(Z,[1,len(Z)])\n",
    "#             #print(\"Z\",Z)\n",
    "\n",
    "#             # sample paths\n",
    "#             tvars = tf.trainable_variables()\n",
    "#             tvars_vals = self.sess.run(tvars[-5:-1])\n",
    "#             #print(tvars_vals)\n",
    "            for j in range(2):\n",
    "                exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,i)\n",
    "                # get observations, actions and rewards\n",
    "                observations_exploit = np.concatenate([path[\"observation\"] for path in exploit_paths])\n",
    "                actions_exploit = np.concatenate([path[\"action\"] for path in exploit_paths])\n",
    "                rewards_exploit = np.concatenate([path[\"reward\"] for path in exploit_paths])\n",
    "                returns_exploit = self.get_returns(exploit_paths)\n",
    "                print(\"average reward exploit: MDP\", i, np.mean(exploit_rewards), len(exploit_rewards))\n",
    "\n",
    "\n",
    "#             # exploit grads\n",
    "#     #         grads_exploit = self.sess.run(self.gradients_exploit,feed_dict={\n",
    "#     #                     self.observation_placeholder_exploit : observations_exploit,\n",
    "#     #                     self.action_placeholder_exploit : actions_exploit,\n",
    "#     #                     self.advantage_placeholder_exploit : returns_exploit,\n",
    "#     #                     self.decoder_input_placeholder: Z})\n",
    "\n",
    "            #train encoder and decoder network\n",
    "                self.sess.run(self.output_train_op, feed_dict={\n",
    "                                self.observation_placeholder_exploit : observations_exploit,\n",
    "                                self.action_placeholder_exploit : actions_exploit,\n",
    "                                self.advantage_placeholder_exploit : returns_exploit,\n",
    "                                self.decoder_input_placeholder: Z})\n",
    "\n",
    "            # find advantage for input network\n",
    "    #         advantage_explore = 0\n",
    "    #         for i in range(len(grads_exploit)):\n",
    "    #             l1 = grads_exploit[i]\n",
    "    #             l2 = grads_explore[i]\n",
    "    #             advantage_explore = advantage_explore + np.matmul(l1.flatten(), l2.flatten())\n",
    "\n",
    "            #train input policy\n",
    "            self.sess.run(self.input_train_op, feed_dict={\n",
    "                            self.observation_placeholder_explore : new_observations_explore,\n",
    "                            self.action_placeholder_explore : new_actions_explore,\n",
    "                            self.advantage_placeholder_explore : advantages_explore})\n",
    "    \n",
    "    def test(self):\n",
    "        explore_paths, explore_rewards = self.sample_paths_explore(self.env,20*np.random.rand())\n",
    "        print(\"gravity: \",self.sim.model.opt.gravity )\n",
    "        M = self.stack_trajectories(explore_paths)\n",
    "        Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        # sample paths\n",
    "        exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,20*np.random.rand())\n",
    "        print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "        \n",
    "    def train(self):\n",
    "        self.initialize()\n",
    "        num_epochs = 200\n",
    "        for epoch in range(num_epochs):\n",
    "            #self.num_traj = 20\n",
    "            print(\"epoch number: \", epoch)\n",
    "            self.train_step()\n",
    "#             print(\"evaluating on new MDPs\")\n",
    "#             self.num_traj = 400\n",
    "#             self.test()\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "a = MetaLearner(env, max_ep_len, num_traj, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:  0\n",
      "average reward explore: MDP 0 -5574.111930740368 10\n",
      "average reward exploit: MDP 0 -2351.1058283695043 10\n",
      "average reward exploit: MDP 0 -3047.5409764256055 10\n",
      "average reward explore: MDP 1 -4323.167657057778 10\n",
      "average reward exploit: MDP 1 -1850.3831664525806 10\n",
      "average reward exploit: MDP 1 -2214.5610486962987 10\n",
      "average reward explore: MDP 2 -3463.352826734882 10\n",
      "average reward exploit: MDP 2 -1529.1440568983458 10\n",
      "average reward exploit: MDP 2 -1307.1301946754738 10\n",
      "average reward explore: MDP 3 -3246.340450521749 10\n",
      "average reward exploit: MDP 3 -1638.5933783686603 10\n",
      "average reward exploit: MDP 3 -1220.6853585554527 10\n",
      "average reward explore: MDP 4 -3557.5884335204964 10\n",
      "average reward exploit: MDP 4 -1338.4745818385702 10\n",
      "average reward exploit: MDP 4 -1153.955532476509 10\n",
      "average reward explore: MDP 5 -3062.750710957279 10\n",
      "average reward exploit: MDP 5 -1247.9023401328832 10\n",
      "average reward exploit: MDP 5 -1101.6223169971731 10\n",
      "average reward explore: MDP 6 -2548.5035783158733 10\n",
      "average reward exploit: MDP 6 -1109.5216695765173 10\n",
      "average reward exploit: MDP 6 -925.9546966995182 10\n",
      "average reward explore: MDP 7 -2093.777750147331 10\n",
      "average reward exploit: MDP 7 -1049.6904699285942 10\n",
      "average reward exploit: MDP 7 -924.3240713039835 10\n",
      "average reward explore: MDP 8 -1741.2961029919538 10\n",
      "average reward exploit: MDP 8 -1231.5627016275125 10\n",
      "average reward exploit: MDP 8 -974.918838793081 10\n",
      "average reward explore: MDP 9 -1589.6994010786295 10\n",
      "average reward exploit: MDP 9 -1103.0190739168697 10\n",
      "average reward exploit: MDP 9 -830.1149526306106 10\n",
      "epoch number:  1\n",
      "average reward explore: MDP 0 -1424.2150390075399 10\n",
      "average reward exploit: MDP 0 -1103.804122652084 10\n",
      "average reward exploit: MDP 0 -637.1339425104488 10\n",
      "average reward explore: MDP 1 -1298.4761238305596 10\n",
      "average reward exploit: MDP 1 -681.9670825976896 10\n",
      "average reward exploit: MDP 1 -749.6831117518357 10\n",
      "average reward explore: MDP 2 -1213.8595434692368 10\n",
      "average reward exploit: MDP 2 -944.1966192695018 10\n",
      "average reward exploit: MDP 2 -711.3475294178951 10\n",
      "average reward explore: MDP 3 -1193.8042810847696 10\n",
      "average reward exploit: MDP 3 -609.8408241717254 10\n",
      "average reward exploit: MDP 3 -614.0387074157492 10\n",
      "average reward explore: MDP 4 -1185.8822809340104 10\n",
      "average reward exploit: MDP 4 -788.6023215847678 10\n",
      "average reward exploit: MDP 4 -750.5245889761748 10\n",
      "average reward explore: MDP 5 -1095.12030115888 10\n",
      "average reward exploit: MDP 5 -653.1505092987334 10\n",
      "average reward exploit: MDP 5 -617.3109978775276 10\n",
      "average reward explore: MDP 6 -1057.2130713577974 10\n",
      "average reward exploit: MDP 6 -675.5588634523248 10\n",
      "average reward exploit: MDP 6 -598.8364857188903 10\n",
      "average reward explore: MDP 7 -923.8937327163737 10\n",
      "average reward exploit: MDP 7 -609.5977747713798 10\n",
      "average reward exploit: MDP 7 -642.0783417337773 10\n",
      "average reward explore: MDP 8 -758.2206463059896 10\n",
      "average reward exploit: MDP 8 -461.91462144437025 10\n",
      "average reward exploit: MDP 8 -569.7829009228656 10\n",
      "average reward explore: MDP 9 -720.2183648121211 10\n",
      "average reward exploit: MDP 9 -582.0256259943496 10\n",
      "average reward exploit: MDP 9 -482.58257632074344 10\n",
      "epoch number:  2\n",
      "average reward explore: MDP 0 -740.644849093307 10\n",
      "average reward exploit: MDP 0 -442.9621085545679 10\n",
      "average reward exploit: MDP 0 -487.63787938454254 10\n",
      "average reward explore: MDP 1 -693.1336217503734 10\n",
      "average reward exploit: MDP 1 -457.4444829661873 10\n",
      "average reward exploit: MDP 1 -485.7029931018789 10\n",
      "average reward explore: MDP 2 -625.125483124242 10\n",
      "average reward exploit: MDP 2 -477.3763747529342 10\n",
      "average reward exploit: MDP 2 -379.3151555661951 10\n",
      "average reward explore: MDP 3 -586.5896684853714 10\n",
      "average reward exploit: MDP 3 -321.90075935811853 10\n",
      "average reward exploit: MDP 3 -332.58981255146745 10\n",
      "average reward explore: MDP 4 -625.4214053077477 10\n",
      "average reward exploit: MDP 4 -342.6398002756093 10\n",
      "average reward exploit: MDP 4 -377.4367579821956 10\n",
      "average reward explore: MDP 5 -829.6552192709687 10\n",
      "average reward exploit: MDP 5 -271.2245463398698 10\n",
      "average reward exploit: MDP 5 -352.0241974912848 10\n",
      "average reward explore: MDP 6 -1074.0725643483624 10\n",
      "average reward exploit: MDP 6 -180.96873798081356 10\n",
      "average reward exploit: MDP 6 -200.46457737207075 10\n",
      "average reward explore: MDP 7 -1133.158790010212 10\n",
      "average reward exploit: MDP 7 -394.5065037908484 10\n",
      "average reward exploit: MDP 7 -349.1634280146658 10\n",
      "average reward explore: MDP 8 -933.7916418737935 10\n",
      "average reward exploit: MDP 8 -439.3418042149813 10\n",
      "average reward exploit: MDP 8 -361.7135779022549 10\n",
      "average reward explore: MDP 9 -1213.2378182958532 10\n",
      "average reward exploit: MDP 9 -360.98388495684236 10\n",
      "average reward exploit: MDP 9 -238.45339448565102 10\n",
      "epoch number:  3\n",
      "average reward explore: MDP 0 -1416.6935984576644 10\n",
      "average reward exploit: MDP 0 -214.07416977628336 10\n",
      "average reward exploit: MDP 0 -116.59223793320018 10\n",
      "average reward explore: MDP 1 -1151.1616711010897 10\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    explore_paths, explore_rewards = a.sample_paths_explore(a.env,200*np.random.rand())\n",
    "    print(\"gravity:\", a.sim.model.opt.gravity)\n",
    "    print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "    M = a.stack_trajectories(explore_paths)\n",
    "    Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "    Z = np.reshape(Z,[1,len(Z)])\n",
    "    exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1)\n",
    "    print(\"average reward exploit\", np.mean(exploit_rewards), len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a.lr = 3e-3\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "        print(\"epoch number: \", epoch)\n",
    "        a.train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_traj = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_paths, explore_rewards = a.sample_paths_explore(a.env,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: 151.57677928797034 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward explore:\", np.mean(explore_rewards), len(explore_rewards))\n",
    "M = a.stack_trajectories(explore_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit 18.577710931432883 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_traj = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gravity: [ 0.         0.        -5.4325712]\n",
      "average reward explore 3.5619071147766848\n",
      "average reward exploit 2.7301699712255383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-730f26c155e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mexplore_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplore_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_paths_explore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#print(\"average reward explore:\", np.mean(explore_rewards))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplore_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-fb36fe994242>\u001b[0m in \u001b[0;36msample_paths_explore\u001b[0;34m(self, env, mdp_num, Test, num_episodes)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;31m#action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    maxrev2 = 0\n",
    "    maxrev1 = 0\n",
    "    traj = np.random.rand()*(i+1)\n",
    "    \n",
    "    for i in range(4):\n",
    "        explore_paths, explore_rewards = a.sample_paths_explore(a.env,traj)\n",
    "        #print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "        M = a.stack_trajectories(explore_paths)\n",
    "        Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        exploit_paths, explore_rewards = a.sample_paths_exploit(a.env,Z,traj)\n",
    "        if(np.mean(explore_rewards) > maxrev1):\n",
    "            maxrev1 = np.mean(explore_rewards) / num_traj\n",
    "        if(np.mean(exploit_rewards)>maxrev2):\n",
    "            maxrev2 = np.mean(exploit_rewards) / num_traj\n",
    "    print(\"gravity:\", a.sim.model.opt.gravity)\n",
    "    print(\"average reward explore\", maxrev1)\n",
    "    print(\"average reward exploit\", maxrev2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_paths, explore_rewards = a.sample_paths_explore(a.env,5)\n",
    "observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "returns_explore = a.get_returns(explore_paths, explore = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array(a.stack_trajectories(explore_paths))\n",
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: MDP 20488.85105031593 800\n"
     ]
    }
   ],
   "source": [
    " print(\"average reward explore: MDP\", np.sum(explore_rewards)/num_traj, len(explore_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 644.4290518284578 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward exploit: MDP\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_pendulum(env):\n",
    "    m = config['pendulum_mass_min'] + np.random.rand()*(config['pendulum_mass_max'] - config['pendulum_mass_min'])\n",
    "    l = config['pendulum_len_min'] + np.random.rand()*(config['pendulum_len_max'] - config['pendulum_len_min'])\n",
    "    env.m = m\n",
    "    env.l = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_hopper(env):\n",
    "    ts = config['torso_min'] + np.random.rand()*(config['torso_max'] - config['torso_min'])\n",
    "    f = config['friction_min'] + np.random.rand()*(config['friction_max'] - config['friction_min'])\n",
    "    \n",
    "    env.friction = f\n",
    "    env.torso_size = ts\n",
    "    env.apply_env_modifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "cfg_filename = 'hopper-config.yml'\n",
    "with open(cfg_filename,'r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Randomizer(gym.make('Hopper-v2'), randomize_hopper)\n",
    "env.unwrapped.__dict__.keys()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = config['pendulum_mass_min'] + np.random.rand()*(config['pendulum_mass_max'] - config['pendulum_mass_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'Hopper-v2'\n",
    "e = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.unwrapped.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.unwrapped.apply_env_modifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mujoco_py.cymj.PyMjOption at 0x1c23ba8ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sim.model.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[112.13464748855134,\n",
       " 107.88303586312765,\n",
       " 186.02669267001477,\n",
       " 135.2581011205201,\n",
       " 79.13080731610503,\n",
       " 121.000075027127,\n",
       " 173.12633008893275,\n",
       " 183.049128023303,\n",
       " 178.67252211302042,\n",
       " 87.14624506369364,\n",
       " 194.95350061967267,\n",
       " 99.25651092576477,\n",
       " 71.49508077704641,\n",
       " 88.63821085625852,\n",
       " 102.13457423812075,\n",
       " 158.97240001687823,\n",
       " 116.1121244537115,\n",
       " 107.73547738516186,\n",
       " 139.42624826594056,\n",
       " 66.64122421143784,\n",
       " 180.7967956127941,\n",
       " 81.9917085465781,\n",
       " 86.7164580284039,\n",
       " 216.9255537144621,\n",
       " 196.23999804809637,\n",
       " 106.97407952338774,\n",
       " 211.77799925827094,\n",
       " 138.13634311633172,\n",
       " 146.8420615449942,\n",
       " 128.17126276654236,\n",
       " 74.77819434837676,\n",
       " 183.75160687642273,\n",
       " 72.78452895798674,\n",
       " 120.41099605928771,\n",
       " 101.5334367194092,\n",
       " 168.35981035001126,\n",
       " 69.62303405123133,\n",
       " 121.26107855071695,\n",
       " 158.01986971006684,\n",
       " 200.6322013111029,\n",
       " 71.87886597554524,\n",
       " 186.44100396410624,\n",
       " 90.5373394288748,\n",
       " 162.02200487464742,\n",
       " 171.76682194626335,\n",
       " 64.66835988088545,\n",
       " 93.07526440100999,\n",
       " 166.23306413321885,\n",
       " 105.99062506542288,\n",
       " 143.228189242465,\n",
       " 109.4308873122448,\n",
       " 143.5163139595506,\n",
       " 115.2344747817427,\n",
       " 60.20145334942355,\n",
       " 79.41136565212034,\n",
       " 183.78021364424464,\n",
       " 162.80513615600867,\n",
       " 87.64483751806068,\n",
       " 179.35113237455454,\n",
       " 60.607104642814754,\n",
       " 158.86527932123138,\n",
       " 125.77884930841314,\n",
       " 57.846734325285844,\n",
       " 123.92092940759048,\n",
       " 64.76586275688906,\n",
       " 164.67320276421657,\n",
       " 217.5985315618744,\n",
       " 198.02047584112512,\n",
       " 174.78015485384395,\n",
       " 78.6406920984788,\n",
       " 82.74185193615898,\n",
       " 181.4236702099464,\n",
       " 164.5192539944514,\n",
       " 175.46044451685484,\n",
       " 112.41872354355482,\n",
       " 120.65323070257459,\n",
       " 67.70633670005066,\n",
       " 152.0323454228858,\n",
       " 59.228267657095785,\n",
       " 159.06147132614302,\n",
       " 138.10635790834223,\n",
       " 75.88459957410683,\n",
       " 191.71341643467542,\n",
       " 158.4351805403924,\n",
       " 159.69731797408917,\n",
       " 111.49673121212818,\n",
       " 123.54878963166887,\n",
       " 123.15187865769032,\n",
       " 204.0635926964165,\n",
       " 189.31065397658512,\n",
       " 111.47756666995318,\n",
       " 164.26777641403527,\n",
       " 186.38204078987383,\n",
       " 89.76715796335638,\n",
       " 146.0933463146793,\n",
       " 152.610803218257,\n",
       " 114.77645412380338,\n",
       " 108.81992797106655,\n",
       " 188.25841809794176,\n",
       " 138.05299981344763,\n",
       " 184.15135002671167,\n",
       " 165.51448940409804,\n",
       " 175.52528317336584,\n",
       " 158.04278077734477,\n",
       " 156.84343321851523,\n",
       " 162.88627891367096,\n",
       " 188.41426192056605,\n",
       " 166.88873086055196,\n",
       " 116.24395041654002,\n",
       " 118.9098479167613,\n",
       " 169.21638209609134,\n",
       " 125.60153708455401,\n",
       " 168.97418752440572,\n",
       " 82.57725942062146,\n",
       " 135.01744845514526,\n",
       " 155.72827335098413,\n",
       " 178.80459441292803,\n",
       " 149.18428778521994,\n",
       " 159.1717479508303,\n",
       " 166.51255884406552,\n",
       " 173.43399285166555,\n",
       " 114.84112110685228,\n",
       " 169.42098247486686,\n",
       " 171.53876987424545,\n",
       " 107.16562997614457,\n",
       " 151.592953157463,\n",
       " 160.23711460638478,\n",
       " 162.55928788792957,\n",
       " 139.3097392754981,\n",
       " 114.9124209995656,\n",
       " 56.30767436211967,\n",
       " 116.56671702163149,\n",
       " 111.5149260443636,\n",
       " 163.46270836733547,\n",
       " 209.81775366280502,\n",
       " 175.45418362699255,\n",
       " 176.27783490931438,\n",
       " 99.88984408856093,\n",
       " 170.4815846265511,\n",
       " 140.43457537901952,\n",
       " 160.7437604778002,\n",
       " 101.26544910160446,\n",
       " 186.79100277608921,\n",
       " 64.93570257422888,\n",
       " 127.39573549217515,\n",
       " 184.99808188972617,\n",
       " 155.48670478950834,\n",
       " 68.31059687806311,\n",
       " 116.59860309305891,\n",
       " 170.39817305577597,\n",
       " 180.91296974869903,\n",
       " 152.9999943228542,\n",
       " 55.2057682094539,\n",
       " 123.9159015413261,\n",
       " 153.07469346847233,\n",
       " 158.5662290602995,\n",
       " 167.96563872160917,\n",
       " 94.16104827522985,\n",
       " 164.29140176537447,\n",
       " 162.19781056445146,\n",
       " 66.93100045650992,\n",
       " 60.91080210686061,\n",
       " 155.67670483170477,\n",
       " 174.63819653598944,\n",
       " 175.62832402040308,\n",
       " 113.98475380740328,\n",
       " 151.025130814201,\n",
       " 184.08070546884474,\n",
       " 197.89987483640658,\n",
       " 166.34365956938348,\n",
       " 185.90693339067053,\n",
       " 155.405899623845,\n",
       " 108.73802281684786,\n",
       " 71.92198829803677,\n",
       " 177.67728172973392,\n",
       " 189.19418485702744,\n",
       " 179.90317459982495,\n",
       " 184.84496980886635,\n",
       " 84.99476214441425,\n",
       " 111.45349416019073,\n",
       " 95.96066575397037,\n",
       " 221.89746310923368,\n",
       " 167.29304647107165,\n",
       " 100.45878395802286,\n",
       " 64.92333651823472,\n",
       " 112.27092564166203,\n",
       " 90.06206676145489,\n",
       " 146.39087807704877,\n",
       " 137.81828611991392,\n",
       " 157.07481173296145,\n",
       " 119.59676340646334,\n",
       " 73.47632593638016,\n",
       " 105.2242473701803,\n",
       " 189.47087231516565,\n",
       " 158.89309952832096,\n",
       " 123.24530573630302,\n",
       " 168.74937121889718,\n",
       " 138.6119878719888,\n",
       " 168.0969789849356,\n",
       " 170.62416276935818,\n",
       " 167.754198574978,\n",
       " 108.95616635176627,\n",
       " 194.97695297709222,\n",
       " 194.29747925519328,\n",
       " 178.67396703671469,\n",
       " 165.46589124304427,\n",
       " 90.28994201137101,\n",
       " 188.74143598694346,\n",
       " 125.04081290630428,\n",
       " 154.65044266308902,\n",
       " 102.46636008012295,\n",
       " 100.11554168795548,\n",
       " 170.91308109496129,\n",
       " 155.19047732795164,\n",
       " 102.12738265789064,\n",
       " 73.16181212374306,\n",
       " 170.16072823260023,\n",
       " 148.1590655671541,\n",
       " 189.96344126682635,\n",
       " 121.24110411269352,\n",
       " 124.25682682196863,\n",
       " 137.60212823136854,\n",
       " 123.6499407440728,\n",
       " 144.70326934900717,\n",
       " 157.61354368678892,\n",
       " 165.1887064543368,\n",
       " 150.13792688997142,\n",
       " 170.50968679891525,\n",
       " 107.40474661209676,\n",
       " 187.92630362822896,\n",
       " 171.80751432624095,\n",
       " 87.54920454902833,\n",
       " 187.56078128096428,\n",
       " 126.9021969860226,\n",
       " 61.073377970628364,\n",
       " 79.8378169904285,\n",
       " 184.65590876042876,\n",
       " 138.11240117235025,\n",
       " 114.46582064662607,\n",
       " 202.9490539883215,\n",
       " 84.84737575084107,\n",
       " 119.61693920616204,\n",
       " 161.76096501574867,\n",
       " 73.79217743284339,\n",
       " 165.21248152372328,\n",
       " 195.88894726115467,\n",
       " 179.45762239125025,\n",
       " 119.25894091202768,\n",
       " 165.8499876712191,\n",
       " 163.02155204297222,\n",
       " 114.80144383470349,\n",
       " 172.95707879010018,\n",
       " 138.09567519558797,\n",
       " 78.01488266784014,\n",
       " 111.42568653879442,\n",
       " 78.6547113169232,\n",
       " 196.61864800042588,\n",
       " 163.91173960195496,\n",
       " 90.6724933743234,\n",
       " 109.21782422844883,\n",
       " 191.23924303173578,\n",
       " 179.57993763471634,\n",
       " 100.5618416601382,\n",
       " 146.5140892260237,\n",
       " 140.05267884602674,\n",
       " 187.28481930517745,\n",
       " 163.31293483537556,\n",
       " 91.10761390655676,\n",
       " 151.6882722797837,\n",
       " 170.50280782363396,\n",
       " 108.51150198344183,\n",
       " 187.9751469395834,\n",
       " 137.3987602658892,\n",
       " 152.60378420359584,\n",
       " 129.7790180146226,\n",
       " 184.5950702165,\n",
       " 145.10017115995348,\n",
       " 108.93162191604206,\n",
       " 159.94400042864663,\n",
       " 158.57705956080912,\n",
       " 114.75847535280559,\n",
       " 94.8289213532897,\n",
       " 101.24432769178549,\n",
       " 145.00535960776062,\n",
       " 175.15319391853885,\n",
       " 166.24847041107589,\n",
       " 183.22554145014652,\n",
       " 155.36464560915147,\n",
       " 83.55061695048877,\n",
       " 162.90929343888735,\n",
       " 110.61380141948368,\n",
       " 93.01847339476095,\n",
       " 111.36720440755657,\n",
       " 199.43240560635525,\n",
       " 154.39226877344186,\n",
       " 134.23219811822142,\n",
       " 175.0187659641553,\n",
       " 83.51763676516553,\n",
       " 155.94449481431639,\n",
       " 133.6998695570268,\n",
       " 86.23446070044483,\n",
       " 78.50949156371904,\n",
       " 173.79626581992414,\n",
       " 162.77855082722536,\n",
       " 83.42137359070993,\n",
       " 194.97309237125322,\n",
       " 111.04123889982446,\n",
       " 191.0403270548807,\n",
       " 131.39542236182626,\n",
       " 182.01060562678185,\n",
       " 160.35570079318518,\n",
       " 76.25971014896251,\n",
       " 102.66210583955788,\n",
       " 124.78694717791087,\n",
       " 186.31913235993025,\n",
       " 130.4241857577105,\n",
       " 125.86402233457912,\n",
       " 159.65124698187867,\n",
       " 171.98788428743958,\n",
       " 111.90435314227774,\n",
       " 169.05038055051875,\n",
       " 95.8220871358059,\n",
       " 116.53015500009899,\n",
       " 130.12229729853436,\n",
       " 155.93156973198674,\n",
       " 162.19998363677647,\n",
       " 118.9088840882428,\n",
       " 182.9131958811087,\n",
       " 159.37523039033306,\n",
       " 167.6324162759753,\n",
       " 206.03761570014734,\n",
       " 100.4322031371492,\n",
       " 104.7584895469109,\n",
       " 70.68655794797172,\n",
       " 84.18939568037818,\n",
       " 191.62844514800258,\n",
       " 81.3421780910568,\n",
       " 129.69266941008482,\n",
       " 122.2434285486572,\n",
       " 199.13639090661073,\n",
       " 128.48077611425407,\n",
       " 166.12630325695477,\n",
       " 161.8878810291055,\n",
       " 160.66829947187873,\n",
       " 136.93280377099546,\n",
       " 186.81124019953143,\n",
       " 104.55614575005836,\n",
       " 199.15663354338034,\n",
       " 105.17298089873758,\n",
       " 157.18968585175875,\n",
       " 178.91477795540197,\n",
       " 115.5543658473315,\n",
       " 68.0302818158207,\n",
       " 148.24724283982516,\n",
       " 45.13841070994898,\n",
       " 95.99318929577541,\n",
       " 167.8307533692741,\n",
       " 188.25355467850324,\n",
       " 98.69309220923157,\n",
       " 111.06260591420735,\n",
       " 178.52161755093746,\n",
       " 118.24823952768493,\n",
       " 201.7235873105987,\n",
       " 169.1956912836896,\n",
       " 179.33454365748116,\n",
       " 81.66916522624804,\n",
       " 61.246066543679454,\n",
       " 100.72542334922018,\n",
       " 150.11418318162004,\n",
       " 174.95435490395033,\n",
       " 89.50068796774528,\n",
       " 178.81266509462466,\n",
       " 84.55475555929331,\n",
       " 168.71265549272033,\n",
       " 192.7560020741052,\n",
       " 63.945376219342926,\n",
       " 171.7243591724856,\n",
       " 179.77963215746692,\n",
       " 105.46121172218646,\n",
       " 93.83646669804898,\n",
       " 112.06465270292784,\n",
       " 173.7027056113586,\n",
       " 92.76332763415257,\n",
       " 170.34634102703615,\n",
       " 95.51235672187876,\n",
       " 105.96872766347329,\n",
       " 96.23510907079141,\n",
       " 171.92010711550188,\n",
       " 112.16344937059776,\n",
       " 126.70826464985137,\n",
       " 122.03236548154152,\n",
       " 88.94800272836761,\n",
       " 195.06366856180233,\n",
       " 171.622430281884,\n",
       " 170.79768278482211,\n",
       " 160.33641767920162,\n",
       " 197.90114557099335,\n",
       " 211.03447493919444,\n",
       " 159.8258154419341,\n",
       " 158.76722176929323,\n",
       " 144.69735089106553,\n",
       " 177.40918235716137,\n",
       " 156.90540839809847,\n",
       " 137.7127355490243,\n",
       " 174.17177921250916,\n",
       " 83.71357019180623,\n",
       " 74.91536780352305,\n",
       " 186.92143950629608,\n",
       " 121.82388499025166,\n",
       " 183.6816659674546,\n",
       " 215.78343075217512,\n",
       " 173.03938208349686,\n",
       " 125.20289776380497,\n",
       " 155.18601830476211,\n",
       " 196.39126816136582,\n",
       " 161.7497978825844,\n",
       " 174.61889012647868,\n",
       " 65.54663048225044,\n",
       " 157.0248160476821,\n",
       " 62.013953587158035,\n",
       " 145.08070431203993,\n",
       " 204.4799135367354,\n",
       " 183.46892960179872,\n",
       " 169.5846061967458,\n",
       " 160.5596322556337,\n",
       " 198.4127872622094,\n",
       " 122.11691626931633,\n",
       " 147.88888449708367,\n",
       " 93.42186545782172,\n",
       " 105.40173478109776,\n",
       " 110.5826811846747,\n",
       " 143.86917056646578,\n",
       " 101.22077695947083,\n",
       " 148.40974160667747,\n",
       " 112.58156677270891,\n",
       " 64.71347947544407,\n",
       " 189.5665605856417,\n",
       " 123.98079768695281,\n",
       " 197.29410823363148,\n",
       " 72.28259185998074,\n",
       " 150.8796903877479,\n",
       " 122.09968749452402,\n",
       " 170.63292305955713,\n",
       " 67.69921288348104,\n",
       " 149.9891438233522,\n",
       " 189.93931500012005,\n",
       " 188.80454210176657,\n",
       " 166.75536743726886,\n",
       " 125.94392424460939,\n",
       " 80.72080708527228,\n",
       " 122.67435837866586,\n",
       " 115.81750608797329,\n",
       " 118.43471782667733,\n",
       " 110.72592709287412,\n",
       " 114.33456511012297,\n",
       " 100.78131459029609,\n",
       " 146.04379873794403,\n",
       " 194.2841641870514,\n",
       " 115.7590163024971,\n",
       " 167.9687777746371,\n",
       " 178.6367600945511,\n",
       " 195.39825566996254,\n",
       " 168.77034911371817,\n",
       " 89.96650422612986,\n",
       " 194.96716595179657,\n",
       " 125.13399841242413,\n",
       " 183.64540533770221,\n",
       " 142.02873027754407,\n",
       " 184.991755853192,\n",
       " 168.84706461417233,\n",
       " 159.48572439884825,\n",
       " 177.61870560362792,\n",
       " 79.3007810232649,\n",
       " 172.31957180265795,\n",
       " 152.5538351068324,\n",
       " 190.32822084034825,\n",
       " 142.3576744898982,\n",
       " 176.600428789626,\n",
       " 183.03758223792016,\n",
       " 165.51858291743346,\n",
       " 161.09118308421102,\n",
       " 183.2042590081598,\n",
       " 188.78826520519405,\n",
       " 95.3430762519607,\n",
       " 179.0625602293134,\n",
       " 125.20878058680984,\n",
       " 118.04152046407314,\n",
       " 193.67312720275402,\n",
       " 176.17849550343735,\n",
       " 164.27820380767363,\n",
       " 162.2653138848123,\n",
       " 125.7935363123882,\n",
       " 120.80737728943947,\n",
       " 208.89402371255466,\n",
       " 168.44531499828733,\n",
       " 78.79635701720501,\n",
       " 52.933077062864534,\n",
       " 174.4198573331745,\n",
       " 102.11441476501591,\n",
       " 135.4017529521238,\n",
       " 196.57738868191865,\n",
       " 127.61503447061828,\n",
       " 138.20086437144528,\n",
       " 165.43492219656562,\n",
       " 91.09763914095626,\n",
       " 179.45687401599008,\n",
       " 162.32562860316386,\n",
       " 130.6881983103069,\n",
       " 161.51411068527207,\n",
       " 149.78322420646572,\n",
       " 190.25376386433922,\n",
       " 175.99412333077444,\n",
       " 209.55887752446844,\n",
       " 56.44511671248712,\n",
       " 128.74667240200338,\n",
       " 140.62867410712596,\n",
       " 192.78451557645008,\n",
       " 190.47539757300817,\n",
       " 84.70669845912191,\n",
       " 126.5645608696394,\n",
       " 182.06050796465343,\n",
       " 208.34115922303175,\n",
       " 162.1540608456324,\n",
       " 112.39761701062665,\n",
       " 185.5956628018305,\n",
       " 170.51407443450358,\n",
       " 120.78962249742,\n",
       " 153.20488080614552,\n",
       " 169.34602573926276,\n",
       " 172.10023392706756,\n",
       " 143.0893403065432,\n",
       " 119.3252686220417,\n",
       " 145.03546702916725,\n",
       " 73.54042323738858,\n",
       " 131.3990284792499,\n",
       " 172.68962202453574,\n",
       " 182.56842851538042,\n",
       " 63.93600461625489,\n",
       " 124.95537578740365,\n",
       " 122.32755846393219,\n",
       " 70.88135173384464,\n",
       " 194.6268360909349,\n",
       " 105.47057148169274,\n",
       " 116.09942731745254,\n",
       " 185.77037358146475,\n",
       " 109.15381487476436,\n",
       " 58.97067227733653,\n",
       " 197.49325583510105,\n",
       " 172.64472309860318,\n",
       " 126.29319183408138,\n",
       " 111.17806718280472,\n",
       " 81.53392069338761,\n",
       " 123.39432590941928,\n",
       " 157.06707483318246,\n",
       " 189.92143090282906,\n",
       " 170.38070446293614,\n",
       " 184.56083588704786,\n",
       " 138.4038808227547,\n",
       " 170.56165414553294,\n",
       " 58.83621484947255,\n",
       " 107.72743244667221,\n",
       " 125.39181473229924,\n",
       " 207.00184707175782,\n",
       " 176.7181862573779,\n",
       " 128.20267780934688,\n",
       " 110.1624284409427,\n",
       " 175.20428977443635,\n",
       " 169.2167023983227,\n",
       " 120.1761763075121,\n",
       " 147.44031908594064,\n",
       " 125.89685343441269,\n",
       " 164.3261430996038,\n",
       " 216.688975576298,\n",
       " 154.84258772317511,\n",
       " 110.5522527681017,\n",
       " 96.96648934325393,\n",
       " 147.25021810997976,\n",
       " 79.6858090832753,\n",
       " 181.47750462561234,\n",
       " 161.4841523214461,\n",
       " 175.62685707151857,\n",
       " 182.7863765384666,\n",
       " 57.791774733943086,\n",
       " 170.54544773326867,\n",
       " 63.58967832813962,\n",
       " 151.32530416857955,\n",
       " 68.14729754121657,\n",
       " 155.69322416622532,\n",
       " 119.01286819191719,\n",
       " 179.39336685134242,\n",
       " 168.14188234563224,\n",
       " 129.5306808185457,\n",
       " 47.673895551441234,\n",
       " 161.14791796998955,\n",
       " 40.818125669411096,\n",
       " 161.2240191735624,\n",
       " 137.03165239991978,\n",
       " 206.9929920661403,\n",
       " 54.95163424733535,\n",
       " 179.44544458327047,\n",
       " 170.84009422892328,\n",
       " 161.4254455510234,\n",
       " 182.69232360783312,\n",
       " 182.2709928334863,\n",
       " 148.78112093397593,\n",
       " 80.7076429784921,\n",
       " 115.8267136584442,\n",
       " 85.38589989440067,\n",
       " 75.62843168051641,\n",
       " 157.46739449757396,\n",
       " 68.54325250797959,\n",
       " 180.54094989795692,\n",
       " 93.61985979424587,\n",
       " 74.91370000767155,\n",
       " 153.8449140669033,\n",
       " 80.07132798314431,\n",
       " 103.70719058065505,\n",
       " 64.8135336466496,\n",
       " 114.63980993064506,\n",
       " 163.44360047509406,\n",
       " 169.15617175542957,\n",
       " 194.275757796929,\n",
       " 168.18171533233894,\n",
       " 168.1384694308446,\n",
       " 131.7049523192029,\n",
       " 114.48926521147678,\n",
       " 97.45672018725938,\n",
       " 231.5197675997255,\n",
       " 123.4856566506957,\n",
       " 117.49339895924102,\n",
       " 187.7353365158894,\n",
       " 98.4002463550037,\n",
       " 156.71379077772391,\n",
       " 173.2845681275291,\n",
       " 180.487728889963,\n",
       " 62.82088427234667,\n",
       " 163.17368178662724,\n",
       " 169.92634304132736,\n",
       " 144.32509442639727,\n",
       " 157.87439844314096,\n",
       " 137.12573237945344,\n",
       " 166.19691483625894,\n",
       " 59.57733556504935,\n",
       " 107.45979805462858,\n",
       " 91.67756591152141,\n",
       " 191.49393104926614,\n",
       " 158.00610043985216,\n",
       " 182.90229637984547,\n",
       " 137.40685239588606,\n",
       " 108.58412189158004,\n",
       " 167.42229683588175,\n",
       " 76.13163503703635,\n",
       " 93.67055671644665,\n",
       " 131.5099549695988,\n",
       " 68.99410589733462,\n",
       " 176.3821760854944,\n",
       " 163.69842860303515,\n",
       " 170.96051022393726,\n",
       " 187.88334458596853,\n",
       " 116.56702301589345,\n",
       " 143.40050862596473,\n",
       " 79.46858049356821,\n",
       " 186.48251810531025,\n",
       " 188.8607277296337,\n",
       " 182.74265567874335,\n",
       " 108.70403132054577,\n",
       " 171.02719393167754,\n",
       " 203.86990020833966,\n",
       " 68.34561242656554,\n",
       " 163.02841555446872,\n",
       " 175.38110001350617,\n",
       " 173.57429466489083,\n",
       " 183.88227404492392,\n",
       " 150.79586862718986,\n",
       " 174.772420779452,\n",
       " 189.16881341871996,\n",
       " 173.5847643777267,\n",
       " 171.2025008814231,\n",
       " 232.08497789417027,\n",
       " 173.0634683423599,\n",
       " 125.62180148656394,\n",
       " 202.47011160944132,\n",
       " 119.5579116275682,\n",
       " 130.2622225922786,\n",
       " 94.6443205657611,\n",
       " 205.80278341310134,\n",
       " 165.59584928603928,\n",
       " 84.63463859510792,\n",
       " 126.26521337384801,\n",
       " 117.61985719835387,\n",
       " 177.09452125270343,\n",
       " 106.27041002145053,\n",
       " 207.7355066062048,\n",
       " 144.27647676792924,\n",
       " 103.68388585589646,\n",
       " 157.999291370704,\n",
       " 124.60575925880538,\n",
       " 87.34593652581627,\n",
       " 180.06511229405896,\n",
       " 131.59984086703855,\n",
       " 142.08375105261737,\n",
       " 166.79923207313155,\n",
       " 172.7535286741458,\n",
       " 191.40947834637063,\n",
       " 150.56261528112984,\n",
       " 166.677828361661,\n",
       " 101.99179701354096,\n",
       " 108.56770309691116,\n",
       " 101.51606864411242,\n",
       " 181.79634494543106,\n",
       " 170.2444656792795,\n",
       " 179.20563484470972,\n",
       " 164.26252062158025,\n",
       " 181.51695932740205,\n",
       " 100.40002690541863,\n",
       " 133.65799783185477,\n",
       " 118.47030148557795,\n",
       " 183.26208547170236,\n",
       " 111.24645248350501,\n",
       " 153.5613469465519,\n",
       " 206.35073751102237,\n",
       " 192.08400791188734,\n",
       " 124.4242491904403,\n",
       " 177.28709148337924,\n",
       " 214.10429416987708,\n",
       " 177.26702722137117,\n",
       " 119.56246777367731,\n",
       " 179.51259537881006,\n",
       " 55.7601449612337,\n",
       " 60.31923784127708,\n",
       " 171.78353165258486,\n",
       " 68.41941283113965,\n",
       " 163.47800109548902,\n",
       " 162.41194224390745,\n",
       " 135.43705698916548,\n",
       " 109.04353998766628,\n",
       " 155.92082256399814,\n",
       " 109.59965658298385,\n",
       " 144.03238697439747,\n",
       " 107.3112753270377,\n",
       " 159.66541627363026,\n",
       " 182.29016557923472,\n",
       " 164.32446508946734,\n",
       " 160.00310497824916,\n",
       " 169.03653097872714,\n",
       " 72.21656145915387,\n",
       " 212.8435030430874,\n",
       " 172.9229418087759,\n",
       " 182.76742774911938,\n",
       " 144.3091463767417,\n",
       " 97.68130366956645,\n",
       " 98.75213061302281,\n",
       " 106.69468341612391,\n",
       " 168.24446801874296,\n",
       " 170.26019365275963,\n",
       " 98.20535669069095,\n",
       " 157.62994548464934,\n",
       " 112.65903693408801,\n",
       " 82.34811171859084,\n",
       " 150.03058008700128,\n",
       " 57.64106504031913,\n",
       " 84.17166937846741,\n",
       " 171.46808418901645,\n",
       " 112.59769890501732,\n",
       " 169.2105461598527,\n",
       " 169.24044046796425,\n",
       " 150.16391663050763,\n",
       " 204.00067403291933,\n",
       " 197.59123748624285,\n",
       " 128.60546093656055,\n",
       " 177.69313503736458,\n",
       " 208.85119544705657,\n",
       " 95.59228435261039,\n",
       " 45.89573978403366,\n",
       " 131.2252618795177,\n",
       " 175.0126331466311,\n",
       " 122.55469334429591,\n",
       " 80.50124181799445,\n",
       " 98.40923610266098,\n",
       " 139.82150018660954,\n",
       " 169.23286834886574,\n",
       " 184.40911138439796,\n",
       " 165.33112855935784,\n",
       " 142.35458360311577,\n",
       " 161.92312580716322,\n",
       " 196.6419106769927,\n",
       " 168.93362601505962,\n",
       " 123.37954690016963,\n",
       " 162.72411257659957,\n",
       " 94.50383485189116,\n",
       " 156.086206003573,\n",
       " 47.47065278430487,\n",
       " 178.54819650677183,\n",
       " 172.0896989621598,\n",
       " 175.44528495476902,\n",
       " 161.40669991479433,\n",
       " 179.2109843398464,\n",
       " 84.74083350739983,\n",
       " 110.612940364999,\n",
       " 69.76454473335902]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploit_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exploit_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
