{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow.contrib.layers as layers\n",
    "from mujoco_py import load_model_from_xml, MjSim, MjViewer\n",
    "import policies\n",
    "import value_functions as vfuncs\n",
    "import utils_pg as utils\n",
    "\n",
    "# Environment setup\n",
    "#env = \"CartPole-v0\"\n",
    "#env=\"InvertedPendulum-v2\"\n",
    "#env = \"Hopper-v2\"\n",
    "env=\"HalfCheetah-v2\"\n",
    "\n",
    "# discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "# observation_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "max_ep_len = 1000\n",
    "num_traj = 3\n",
    "#traj_length = max_ep_len*(observation_dim + 2)\n",
    "latent_size = 25\n",
    "use_baseline = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feed forward network (multi-layer-perceptron, or mlp)\n",
    "# def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "#     '''\n",
    "#     Build a feed forward network\n",
    "#     '''\n",
    "#     Input = mlp_input\n",
    "#     with tf.variable_scope(scope):\n",
    "#         # Dense Layers\n",
    "#         for i in range(n_layers-1):\n",
    "#             dense = tf.layers.dense(inputs = Input, units = size, activation = tf.nn.relu, bias_initializer=tf.constant_initializer(1.0))\n",
    "#             Input = dense\n",
    "#         # Fully Connected Layer\n",
    "#         out = layers.fully_connected(inputs = Input, num_outputs = output_size, activation_fn=output_activation)\n",
    "#     return out\n",
    "\n",
    "def build_mlp(mlp_input,output_size,scope,n_layers,size,output_activation=None):\n",
    "\n",
    "        hidden1 = layers.fully_connected(mlp_input,\n",
    "                num_outputs=32,\n",
    "                weights_initializer=layers.xavier_initializer(uniform=True),\n",
    "                activation_fn=tf.nn.relu)\n",
    "        hidden2 = layers.fully_connected(hidden1,\n",
    "                num_outputs=32,\n",
    "                weights_initializer=layers.xavier_initializer(uniform=True),\n",
    "                activation_fn=tf.nn.relu)\n",
    "        mean_na = layers.fully_connected(hidden2,\n",
    "                num_outputs=output_size,\n",
    "                weights_initializer=layers.xavier_initializer(uniform=True),\n",
    "                activation_fn=None)\n",
    "\n",
    "        return mean_na "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner():\n",
    "    def __init__(self, env, max_ep_len, num_traj,latent_size ):\n",
    "        self.env = gym.make(env)\n",
    "        self.discrete = isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.num_traj = num_traj\n",
    "        self.traj_length = self.max_ep_len*(self.observation_dim + 1 + self.env.action_space.shape[0]) # TO Change\n",
    "        self.use_baseline = True\n",
    "        self.latent_size = latent_size\n",
    "        self.feature_size = self.observation_dim + 1 + self.env.action_space.shape[0] # HC\n",
    "        self.lr = 3e-3\n",
    "        self.num_layers = 1\n",
    "        self.layers_size = 16\n",
    "        self.num_mdps = 10\n",
    "        self.model = self.env.env.model\n",
    "        self.sim = MjSim(self.model)\n",
    "        self.gamma = .95\n",
    "        self.decoder_len = 16\n",
    "\n",
    "        # build model\n",
    "        #self.ConstructGraph()\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        self.observation_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        if(self.discrete):\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.int32, shape=(None))\n",
    "            self.action_placeholder_exploit = tf.placeholder(tf.int32, shape=(None))\n",
    "        else:\n",
    "            self.action_placeholder_explore = tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "            self.action_placeholder_exploit= tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "        \n",
    "        ##########################\n",
    "        self.n     = tf.shape(self.observation_placeholder_explore)[0] \n",
    "        self.oldlogstd_a  = tf.placeholder(name=\"oldlogstd\", shape=[self.action_dim], dtype=tf.float32)\n",
    "        ##########################\n",
    "        \n",
    "        self.baseline_target_placeholder = tf.placeholder(tf.float32, shape= None)\n",
    "        self.advantage_placeholder_explore = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "        #self.encoder_input_placeholder = tf.placeholder(tf.float32, shape= (self.num_traj,self.traj_length))\n",
    "        self.encoder_input_placeholder = tf.placeholder(tf.float32, [None, None, self.feature_size])\n",
    "        self.decoder_input_placeholder = tf.placeholder(tf.float32, shape= (1,self.latent_size))\n",
    "        self.sequence_length_placeholder = tf.placeholder(tf.int32, [None, ])\n",
    "        \n",
    "        self.observation_placeholder_exploit = tf.placeholder(tf.float32, shape=(None,self.observation_dim))\n",
    "        #TODO\n",
    "        self.advantage_placeholder_exploit = tf.placeholder(tf.float32, shape=(None))\n",
    "        \n",
    "        \n",
    "    def build_policy_explore(self, scope = \"policy_explore\"):\n",
    "        if (self.discrete):\n",
    "            self.action_logits = build_mlp(self.observation_placeholder_explore,self.action_dim,scope = scope,n_layers=self.num_layers,size = self.layers_size,output_activation=None)\n",
    "            self.explore_action = tf.multinomial(self.action_logits,1)\n",
    "            self.explore_action = tf.squeeze(self.explore_action, axis=1)\n",
    "            self.explore_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.action_logits, labels = self.action_placeholder_explore)\n",
    "\n",
    "        else:   \n",
    "            action_means = build_mlp(self.observation_placeholder_explore,self.action_dim,scope,n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "            \n",
    "            self.logstd_a     = tf.get_variable(\"logstd\", [self.action_dim], initializer=tf.zeros_initializer())\n",
    "            self.logstd_na    = tf.ones(shape=(self.n,self.action_dim), dtype=tf.float32) * self.logstd_a\n",
    "            self.oldlogstd_na = tf.ones(shape=(self.n,self.action_dim), dtype=tf.float32) * self.oldlogstd_a\n",
    "            \n",
    "            self.explore_logprob  = utils.gauss_log_prob(mu=action_means, logstd=self.logstd_na, x=self.action_placeholder_explore)\n",
    "            self.explore_action =   action_means + tf.multiply(tf.exp(self.logstd_na),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            \n",
    "#             init = tf.constant(np.random.rand(1, 2))\n",
    "#             log_std = tf.get_variable(\"log_std\", [self.action_dim])\n",
    "#             self.explore_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "#             mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "#             self.explore_logprob =  mvn.log_prob(value = self.action_placeholder_explore, name='log_prob')\n",
    "    \n",
    "    \n",
    "    def build_policy_exploit(self, scope = \"policy_exploit\"):\n",
    "        if(self.discrete):\n",
    "            #self.exploit_action_logits = (tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(self.observation_placeholder_exploit,self.d_W1) + self.d_B1), self.d_W2) + self.d_B2),self.d_W3) + self.d_B3)\n",
    "            self.exploit_action_logits = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            self.exploit_action = tf.multinomial(self.exploit_action_logits,1)\n",
    "            self.exploit_action = tf.squeeze(self.exploit_action, axis=1)\n",
    "            self.exploit_logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.exploit_action_logits, labels = self.action_placeholder_exploit)\n",
    "        else:\n",
    "            \n",
    "            action_means = tf.matmul(self.observation_placeholder_exploit,self.d_W3) + self.d_B3\n",
    "            init = tf.constant(np.random.rand(1, 2))\n",
    "            log_std = tf.get_variable(\"exploit_log_prob\", [self.action_dim])\n",
    "            self.exploit_action =   action_means + tf.multiply(tf.exp(log_std),tf.random_normal(shape = (self.action_dim,1),mean=0,stddev=1))\n",
    "            mvn = tf.contrib.distributions.MultivariateNormalDiag(action_means, tf.exp(log_std))\n",
    "            self.exploit_logprob =  mvn.log_prob(value = self.action_placeholder_exploit, name='exploit_log_prob')\n",
    "\n",
    "        #self.loss_grads_exploit = self.exploit_logprob * self.advantage_placeholder_exploit\n",
    "        \n",
    "    def NNEncoder(self, scope = \"NNEncoder\"):\n",
    "        self.Z = build_mlp(self.encoder_input_placeholder,self.latent_size,scope = scope,n_layers=3,size = 60,output_activation=None)\n",
    "    \n",
    " \n",
    "\n",
    "    # input [num_traj, length, features (obs + action + reward) ]\n",
    "    def LSTMEncoder(self, scope = \"LSTMEncoder\"):\n",
    "        self.hidden_size = 64\n",
    "        initializer = tf.random_uniform_initializer(-1, 1)\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, self.feature_size, initializer=initializer)\n",
    "        cell_out = tf.contrib.rnn.OutputProjectionWrapper(cell, self.latent_size)\n",
    "        self.output, _ = tf.nn.dynamic_rnn(cell_out,self.encoder_input_placeholder,self.sequence_length_placeholder,dtype=tf.float32,)\n",
    "        batch_size = tf.shape(self.output)[0]\n",
    "        max_length = tf.shape(self.output)[1]\n",
    "        out_size = int(self.output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (self.sequence_length_placeholder - 1)\n",
    "        flat = tf.reshape(self.output, [-1, out_size])\n",
    "        self.Z = tf.reduce_mean(tf.gather(flat, index), axis = 0)\n",
    "        \n",
    "    \n",
    "    def Decoder(self, decoder_out_dim, scope):\n",
    "        return build_mlp(self.decoder_input_placeholder,decoder_out_dim,scope = scope,n_layers=1,size = 64,output_activation=None)\n",
    "    \n",
    "    \n",
    "    def sample_paths_explore(self, env,mdp_num,Test = False, num_episodes = None):\n",
    "        paths = []\n",
    "        self.length = []\n",
    "        episode_rewards = []\n",
    "        #self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        for i in range(self.num_traj):\n",
    "            pad = False\n",
    "            state = env.reset()\n",
    "            new_states,states,new_actions, actions, rewards,new_rewards = [], [], [], [], [],[]\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                if (pad):\n",
    "                    states.append([0]*self.observation_dim)\n",
    "                    if(self.discrete):\n",
    "                        actions.append(0)\n",
    "                    else:\n",
    "                        actions.append([0]*self.action_dim)\n",
    "                    rewards.append(0)\n",
    "                else:\n",
    "                    states.append(state)\n",
    "                    new_states.append(state)\n",
    "                    action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\n",
    "                    #action = self.policyfn.sample_action(state) #NEW\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    actions.append(action)\n",
    "                    new_actions.append(action)\n",
    "                    rewards.append(reward)\n",
    "                    new_rewards.append(reward)\n",
    "                    episode_reward += reward\n",
    "                    if (done or step == self.max_ep_len-1):\n",
    "                        self.length.append(step + 1)\n",
    "                        pad = True \n",
    "                        \n",
    "            episode_rewards.append(episode_reward)            \n",
    "            #print(\"explore\",np.array(actions))\n",
    "            path = {\"new_obs\" : np.array(new_states),\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\"new_acs\" : np.array(new_actions),\n",
    "                                \"action\" : np.array(actions),\"new_rewards\":np.array(new_rewards)}\n",
    "            paths.append(path)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def sample_paths_exploit(self, env,Z,mdp_num,Test = False, num_episodes = None):\n",
    "        #self.sim.model.opt.gravity[-1] = -5 - mdp_num\n",
    "        \n",
    "        paths = []\n",
    "        num = 0\n",
    "        episode_rewards = []\n",
    "        for i in range(self.num_traj):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            for step in range(self.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.sess.run(self.exploit_action, feed_dict={self.observation_placeholder_exploit : state[None], self.decoder_input_placeholder: Z})[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                if (done or step == self.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "            #print(\"exploit\",np.array(actions))\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                                \"reward\" : np.array(rewards),\n",
    "                                \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    " \n",
    "        #print(\"exploit success: \", num)\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    def get_returns(self,paths, explore = False):\n",
    "        all_returns = []\n",
    "        m = 0\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            if(explore):\n",
    "                length = self.length[m]\n",
    "            else:\n",
    "                length = len(rewards)\n",
    "            for i in range(length):\n",
    "                path_returns = 0\n",
    "                k = 0\n",
    "                for j in range(i,length):\n",
    "                    path_returns = path_returns + rewards[j]*(self.gamma)**k\n",
    "                    k = k+1\n",
    "                returns.append(path_returns)\n",
    "            all_returns.append(returns)\n",
    "            m+=1\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "    \n",
    "    def stack_trajectories(self,paths):\n",
    "        trajectories = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            states = path[\"observation\"]\n",
    "            action = path[\"action\"]\n",
    "            \n",
    "            SAR = []\n",
    "            for i in range(len(states)):\n",
    "                SAR.append(list(states[i]) + list(action[i]) + [rewards[i]])\n",
    "            trajectories.append(SAR)\n",
    "        return np.array(trajectories)\n",
    "    \n",
    "    def addBaseline(self):\n",
    "        self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=self.num_layers, size = self.layers_size,output_activation=None)\n",
    "        self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "        baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "\n",
    "    def calculate_advantage(self,returns, observations):\n",
    "        if (self.use_baseline):\n",
    "            baseline = self.sess.run(self.baseline, {input_placeholder:observations})\n",
    "            adv = returns - baseline\n",
    "            adv = (adv - np.mean(adv))/np.std(adv)\n",
    "        else:\n",
    "            adv = returns\n",
    "        return adv\n",
    "    \n",
    "\n",
    "        \n",
    "    def ConstructGraph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.add_placeholders()\n",
    "        \n",
    "        self.build_policy_explore()\n",
    "        self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "        \n",
    "        #self.vf = vfuncs.NnValueFunction(session=self.sess,ob_dim=self.observation_dim)\n",
    "\n",
    "        \n",
    "        \n",
    "#         self.add_placeholders()\n",
    "        \n",
    "#         self.build_policy_explore()\n",
    "#         self.explore_policy_loss = -tf.reduce_sum(self.explore_logprob * self.advantage_placeholder_explore)\n",
    "#         self.loss_grads_explore = -self.explore_logprob * self.advantage_placeholder_explore\n",
    "#         self.tvars_explore = tf.trainable_variables()\n",
    "#         self.gradients_explore = tf.gradients(self.explore_policy_loss,self.tvars_explore)\n",
    "        \n",
    "        self.addBaseline()\n",
    "        \n",
    "#         self.baseline = build_mlp(self.observation_placeholder_explore,1,scope = \"baseline\",n_layers=1, size = 16,output_activation=None)\n",
    "#         self.baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder,self.baseline,scope = \"baseline\")\n",
    "#         baseline_adam_optimizer =  tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "#         self.update_baseline_op = baseline_adam_optimizer.minimize(self.baseline_loss)\n",
    "        \n",
    "#         #Encoder LSTM\n",
    "        self.LSTMEncoder()\n",
    "        \n",
    "        \n",
    "        #decoder weights\n",
    "        #self.d_W1 = self.Decoder(scope = \"W1\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W2 = self.Decoder(scope = \"W2\", decoder_out_dim = self.observation_dim*self.decoder_len)\n",
    "        #self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.decoder_len*action_dim)\n",
    "        self.d_W3 = self.Decoder(scope = \"W3\", decoder_out_dim = self.observation_dim*self.action_dim)\n",
    "        \n",
    "        #self.d_W1 = ((self.d_W1 - (tf.reduce_max(self.d_W1) + tf.reduce_min(self.d_W1))/2)/(tf.reduce_max(self.d_W1) - tf.reduce_min(self.d_W1)))*2 \n",
    "        #self.d_W2 = ((self.d_W2 - (tf.reduce_max(self.d_W2) + tf.reduce_min(self.d_W2))/2)/(tf.reduce_max(self.d_W2) - tf.reduce_min(self.d_W2)))*2 \n",
    "        #self.d_W3 = ((self.d_W3 - (tf.reduce_max(self.d_W3) + tf.reduce_min(self.d_W3))/2)/(tf.reduce_max(self.d_W3) - tf.reduce_min(self.d_W3)))*2 \n",
    "        \n",
    "        #self.d_W1 = tf.reshape(self.d_W1, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W2 = tf.reshape(self.d_W2, [self.observation_dim, self.decoder_len])\n",
    "        #self.d_W3 = tf.reshape(self.d_W3, [self.decoder_len, self.action_dim])\n",
    "        self.d_W3 = tf.reshape(self.d_W3, [self.observation_dim, self.action_dim])\n",
    "        \n",
    "        # decoder output bias\n",
    "        #self.d_B1 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B1\"), [self.decoder_len])\n",
    "        #self.d_B2 = tf.reshape(self.Decoder(decoder_out_dim = self.decoder_len, scope = \"B2\"), [self.decoder_len])\n",
    "        self.d_B3 = tf.reshape(self.Decoder(decoder_out_dim = self.action_dim, scope = \"B3\"), [self.action_dim])\n",
    "        #self.d_B1 = ((self.d_B1 - (tf.reduce_max(self.d_B1) + tf.reduce_min(self.d_B1))/2)/(tf.reduce_max(self.d_B1) - tf.reduce_min(self.d_B1)))*2 \n",
    "        #self.d_B2 = ((self.d_B2 - (tf.reduce_max(self.d_B2) + tf.reduce_min(self.d_B2))/2)/(tf.reduce_max(self.d_B2) - tf.reduce_min(self.d_B2)))*2 \n",
    "        \n",
    "        \n",
    "        # exploit policy\n",
    "        self.build_policy_exploit()\n",
    "        #self.d = [self.d_W1, self.d_B1, self.d_W2, self.d_B2, self.d_W3, self.d_B3]\n",
    "        self.exploit_policy_loss = -tf.reduce_sum(self.exploit_logprob * self.advantage_placeholder_exploit)\n",
    "        self.d = [self.d_W3, self.d_B3]\n",
    "        self.gradients_exploit = tf.gradients(self.exploit_policy_loss,self.d)\n",
    "        \n",
    "        # train encoder and decoder\n",
    "        adam_optimizer_exploit =  tf.train.AdamOptimizer(self.lr*0.3)\n",
    "        self.output_train_op = adam_optimizer_exploit.minimize(self.exploit_policy_loss)\n",
    "        # train original network\n",
    "        adam_optimizer_explore = tf.train.AdamOptimizer(self.lr)\n",
    "        self.input_train_op = adam_optimizer_explore.minimize(self.explore_policy_loss)\n",
    "\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.ConstructGraph()\n",
    "        # create tf session\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # initiliaze all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def train_step(self): \n",
    "        # sample num_traj*num_MDPs\n",
    "        for i in range(self.num_mdps):\n",
    "            explore_paths, explore_rewards = self.sample_paths_explore(self.env,i)\n",
    "            observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "            new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "            new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "            actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "            rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "            returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "            \n",
    "#             vtargs, vpreds, advantages_explore = [], [], []\n",
    "#             for path in explore_paths:\n",
    "#                 rew_t = path[\"new_rewards\"]\n",
    "#                 return_t = utils.discount(rew_t, self.gamma)\n",
    "#                 vpred_t = self.vf.predict(path[\"new_obs\"])\n",
    "#                 adv_t = return_t - vpred_t\n",
    "#                 advantages_explore.append(adv_t)\n",
    "#                 vtargs.append(return_t)\n",
    "#                 vpreds.append(vpred_t)\n",
    "                \n",
    "                \n",
    "#             #returns_explore = self.get_returns(explore_paths, explore = True)\n",
    "#             advantages_explore = np.concatenate(advantages_explore)\n",
    "#             advantages_explore = (advantages_explore - advantages_explore.mean()) / (advantages_explore.std() + 1e-8)\n",
    "#             vtarg_n = np.concatenate(vtargs)\n",
    "#             vpred_n = np.concatenate(vpreds)\n",
    "#             #update baseline\n",
    "#             #print(advantages_explore.shape,new_observations_explore.shape)\n",
    "#             self.vf.fit(new_observations_explore, vtarg_n)\n",
    "            \n",
    "#             surr_loss, oldmean_na, oldlogstd_a = self.policyfn.update_policy(\n",
    "#                     new_observations_explore, new_actions_explore, advantages_explore, self.lr)\n",
    "            \n",
    "            print(\"average reward explore: MDP\", i, np.mean(explore_rewards), len(explore_rewards))\n",
    "            \n",
    "            #print(returns_explore)\n",
    "#             print(\"average reward explore: MDP\", i, np.sum(explore_rewards)/num_traj, len(explore_rewards))\n",
    "\n",
    "            baseline_explore = self.sess.run(self.baseline, {self.observation_placeholder_explore:new_observations_explore})\n",
    "            adv = returns_explore - np.squeeze(baseline_explore)\n",
    "            advantages_explore = (adv - np.mean(adv))/(np.std(adv) + 1e-12)\n",
    "\n",
    "\n",
    "            # update the baseline\n",
    "\n",
    "            self.sess.run(self.update_baseline_op, {self.observation_placeholder_explore:new_observations_explore, \n",
    "                                           self.baseline_target_placeholder : returns_explore})\n",
    "\n",
    "            # calculate explore gradients\n",
    "    #         grads_explore = self.sess.run(self.gradients_explore, feed_dict={\n",
    "    #                     self.observation_placeholder_explore : observations_explore,\n",
    "    #                     self.action_placeholder_explore : actions_explore,\n",
    "    #                     self.advantage_placeholder_explore : returns_explore})\n",
    "            #print(\"explore\",grads_explore )\n",
    "            # form trajectory matrix\n",
    "\n",
    "            M = np.array(self.stack_trajectories(explore_paths))\n",
    "\n",
    "\n",
    "            #encoder LSTM\n",
    "            Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "            Z = np.reshape(Z,[1,len(Z)])\n",
    "#             #print(\"Z\",Z)\n",
    "\n",
    "#             # sample paths\n",
    "#             tvars = tf.trainable_variables()\n",
    "#             tvars_vals = self.sess.run(tvars[-5:-1])\n",
    "#             #print(tvars_vals)\n",
    "            for j in range(0):\n",
    "                exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,i)\n",
    "                # get observations, actions and rewards\n",
    "                observations_exploit = np.concatenate([path[\"observation\"] for path in exploit_paths])\n",
    "                actions_exploit = np.concatenate([path[\"action\"] for path in exploit_paths])\n",
    "                rewards_exploit = np.concatenate([path[\"reward\"] for path in exploit_paths])\n",
    "                returns_exploit = self.get_returns(exploit_paths)\n",
    "                print(\"average reward exploit: MDP\", i, np.mean(exploit_rewards), len(exploit_rewards))\n",
    "\n",
    "\n",
    "#             # exploit grads\n",
    "#     #         grads_exploit = self.sess.run(self.gradients_exploit,feed_dict={\n",
    "#     #                     self.observation_placeholder_exploit : observations_exploit,\n",
    "#     #                     self.action_placeholder_exploit : actions_exploit,\n",
    "#     #                     self.advantage_placeholder_exploit : returns_exploit,\n",
    "#     #                     self.decoder_input_placeholder: Z})\n",
    "\n",
    "            #train encoder and decoder network\n",
    "                self.sess.run(self.output_train_op, feed_dict={\n",
    "                                self.observation_placeholder_exploit : observations_exploit,\n",
    "                                self.action_placeholder_exploit : actions_exploit,\n",
    "                                self.advantage_placeholder_exploit : returns_exploit,\n",
    "                                self.decoder_input_placeholder: Z})\n",
    "\n",
    "            # find advantage for input network\n",
    "    #         advantage_explore = 0\n",
    "    #         for i in range(len(grads_exploit)):\n",
    "    #             l1 = grads_exploit[i]\n",
    "    #             l2 = grads_explore[i]\n",
    "    #             advantage_explore = advantage_explore + np.matmul(l1.flatten(), l2.flatten())\n",
    "\n",
    "            #train input policy\n",
    "            self.sess.run(self.input_train_op, feed_dict={\n",
    "                            self.observation_placeholder_explore : new_observations_explore,\n",
    "                            self.action_placeholder_explore : new_actions_explore,\n",
    "                            self.advantage_placeholder_explore : returns_explore})\n",
    "    \n",
    "    def test(self):\n",
    "        explore_paths, explore_rewards = self.sample_paths_explore(self.env,20*np.random.rand())\n",
    "        print(\"gravity: \",self.sim.model.opt.gravity )\n",
    "        M = self.stack_trajectories(explore_paths)\n",
    "        Z = self.sess.run(self.Z, feed_dict = {self.encoder_input_placeholder: M,self.sequence_length_placeholder: self.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        # sample paths\n",
    "        exploit_paths, exploit_rewards = self.sample_paths_exploit(self.env,Z,20*np.random.rand())\n",
    "        print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))\n",
    "        \n",
    "    def train(self):\n",
    "        self.initialize()\n",
    "        num_epochs = 200\n",
    "        for epoch in range(num_epochs):\n",
    "            #self.num_traj = 20\n",
    "            print(\"epoch number: \", epoch)\n",
    "            self.train_step()\n",
    "#             print(\"evaluating on new MDPs\")\n",
    "#             self.num_traj = 400\n",
    "#             self.test()\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "a = MetaLearner(env, max_ep_len, num_traj, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:  0\n",
      "average reward explore: MDP 0 -5583.598018273133 3\n",
      "average reward explore: MDP 1 -4809.484868915452 3\n",
      "average reward explore: MDP 2 -5052.846445813212 3\n",
      "average reward explore: MDP 3 -4319.391044290912 3\n",
      "average reward explore: MDP 4 -4319.466598941304 3\n",
      "average reward explore: MDP 5 -4082.2975601553658 3\n",
      "average reward explore: MDP 6 -3360.033697006263 3\n",
      "average reward explore: MDP 7 -3469.6460017439917 3\n",
      "average reward explore: MDP 8 -2793.539014690749 3\n",
      "average reward explore: MDP 9 -2902.800488821465 3\n",
      "epoch number:  1\n",
      "average reward explore: MDP 0 -2811.716964443236 3\n",
      "average reward explore: MDP 1 -2805.0019011205436 3\n",
      "average reward explore: MDP 2 -2833.3712956085637 3\n",
      "average reward explore: MDP 3 -2484.0084286247875 3\n",
      "average reward explore: MDP 4 -2762.3671141181526 3\n",
      "average reward explore: MDP 5 -2805.3309695782955 3\n",
      "average reward explore: MDP 6 -2825.4149105311176 3\n",
      "average reward explore: MDP 7 -2569.8552030291416 3\n",
      "average reward explore: MDP 8 -2900.0265055921463 3\n",
      "average reward explore: MDP 9 -2430.0315803448148 3\n",
      "epoch number:  2\n",
      "average reward explore: MDP 0 -2584.7677988727933 3\n",
      "average reward explore: MDP 1 -2775.5422323837797 3\n",
      "average reward explore: MDP 2 -2722.535721012749 3\n",
      "average reward explore: MDP 3 -2537.0638839312246 3\n",
      "average reward explore: MDP 4 -2510.751109290308 3\n",
      "average reward explore: MDP 5 -2499.387252822618 3\n",
      "average reward explore: MDP 6 -2772.7040159215408 3\n",
      "average reward explore: MDP 7 -2464.1519432549726 3\n",
      "average reward explore: MDP 8 -2798.0045774783744 3\n",
      "average reward explore: MDP 9 -2696.3576215855014 3\n",
      "epoch number:  3\n",
      "average reward explore: MDP 0 -2633.6377961597414 3\n",
      "average reward explore: MDP 1 -2695.732598855393 3\n",
      "average reward explore: MDP 2 -2761.5148810671603 3\n",
      "average reward explore: MDP 3 -2633.044923401525 3\n",
      "average reward explore: MDP 4 -2636.384555474319 3\n",
      "average reward explore: MDP 5 -2694.4841548046847 3\n",
      "average reward explore: MDP 6 -2551.426647205188 3\n",
      "average reward explore: MDP 7 -2555.3007783306116 3\n",
      "average reward explore: MDP 8 -2427.287839182975 3\n",
      "average reward explore: MDP 9 -2767.431875114322 3\n",
      "epoch number:  4\n",
      "average reward explore: MDP 0 -2445.826093091804 3\n",
      "average reward explore: MDP 1 -2509.399454387631 3\n",
      "average reward explore: MDP 2 -2533.7936918353503 3\n",
      "average reward explore: MDP 3 -2369.678628627638 3\n",
      "average reward explore: MDP 4 -2280.972895837643 3\n",
      "average reward explore: MDP 5 -2557.2169509430696 3\n",
      "average reward explore: MDP 6 -2046.4925749189886 3\n",
      "average reward explore: MDP 7 -2294.905744530567 3\n",
      "average reward explore: MDP 8 -2190.426666264532 3\n",
      "average reward explore: MDP 9 -2507.65721944819 3\n",
      "epoch number:  5\n",
      "average reward explore: MDP 0 -2544.3220065737605 3\n",
      "average reward explore: MDP 1 -2071.6081703680425 3\n",
      "average reward explore: MDP 2 -2245.1470998047066 3\n",
      "average reward explore: MDP 3 -2371.9876857575728 3\n",
      "average reward explore: MDP 4 -2575.3084724280834 3\n",
      "average reward explore: MDP 5 -2323.9400196502534 3\n",
      "average reward explore: MDP 6 -1879.9905825539554 3\n",
      "average reward explore: MDP 7 -1899.8909312533106 3\n",
      "average reward explore: MDP 8 -2819.1737309680175 3\n",
      "average reward explore: MDP 9 -2402.3548486080285 3\n",
      "epoch number:  6\n",
      "average reward explore: MDP 0 -2328.7749615906596 3\n",
      "average reward explore: MDP 1 -2286.3861347287584 3\n",
      "average reward explore: MDP 2 -2511.4260232874285 3\n",
      "average reward explore: MDP 3 -2310.8342000320617 3\n",
      "average reward explore: MDP 4 -2354.940418801674 3\n",
      "average reward explore: MDP 5 -2640.7844396746736 3\n",
      "average reward explore: MDP 6 -2145.3431553945716 3\n",
      "average reward explore: MDP 7 -2395.7313810409246 3\n",
      "average reward explore: MDP 8 -2122.9578597661884 3\n",
      "average reward explore: MDP 9 -2282.5974443683153 3\n",
      "epoch number:  7\n",
      "average reward explore: MDP 0 -2364.442703549888 3\n",
      "average reward explore: MDP 1 -2179.064184220244 3\n",
      "average reward explore: MDP 2 -1536.814911042896 3\n",
      "average reward explore: MDP 3 -2425.2002267936837 3\n",
      "average reward explore: MDP 4 -2309.3712898713907 3\n",
      "average reward explore: MDP 5 -2128.9539085185606 3\n",
      "average reward explore: MDP 6 -1996.4801922099043 3\n",
      "average reward explore: MDP 7 -2140.7688367982582 3\n",
      "average reward explore: MDP 8 -2289.7621191885028 3\n",
      "average reward explore: MDP 9 -2322.011897787908 3\n",
      "epoch number:  8\n",
      "average reward explore: MDP 0 -2168.696854517749 3\n",
      "average reward explore: MDP 1 -1978.534795584941 3\n",
      "average reward explore: MDP 2 -1769.5657675448047 3\n",
      "average reward explore: MDP 3 -2069.871756556533 3\n",
      "average reward explore: MDP 4 -2107.6369065086164 3\n",
      "average reward explore: MDP 5 -2189.525589275347 3\n",
      "average reward explore: MDP 6 -1967.5306385822757 3\n",
      "average reward explore: MDP 7 -1994.452539532396 3\n",
      "average reward explore: MDP 8 -2136.481541358098 3\n",
      "average reward explore: MDP 9 -2153.4924494490006 3\n",
      "epoch number:  9\n",
      "average reward explore: MDP 0 -2103.266399307819 3\n",
      "average reward explore: MDP 1 -2288.2256938912656 3\n",
      "average reward explore: MDP 2 -2287.395333806026 3\n",
      "average reward explore: MDP 3 -2106.5660253941774 3\n",
      "average reward explore: MDP 4 -2034.3545019727087 3\n",
      "average reward explore: MDP 5 -1947.5850680252859 3\n",
      "average reward explore: MDP 6 -1888.8317709500072 3\n",
      "average reward explore: MDP 7 -2131.586424660341 3\n",
      "average reward explore: MDP 8 -2123.8853240483536 3\n",
      "average reward explore: MDP 9 -2082.16001814571 3\n",
      "epoch number:  10\n",
      "average reward explore: MDP 0 -2078.710298772328 3\n",
      "average reward explore: MDP 1 -1738.0531951781056 3\n",
      "average reward explore: MDP 2 -1904.7782120130535 3\n",
      "average reward explore: MDP 3 -2139.94072994426 3\n",
      "average reward explore: MDP 4 -2123.1583618073273 3\n",
      "average reward explore: MDP 5 -1893.3652070304863 3\n",
      "average reward explore: MDP 6 -1743.5086415699673 3\n",
      "average reward explore: MDP 7 -2215.0789615425642 3\n",
      "average reward explore: MDP 8 -1827.4470475324651 3\n",
      "average reward explore: MDP 9 -1946.248546822564 3\n",
      "epoch number:  11\n",
      "average reward explore: MDP 0 -1925.2506110755928 3\n",
      "average reward explore: MDP 1 -1784.0285432657467 3\n",
      "average reward explore: MDP 2 -1992.4089725312554 3\n",
      "average reward explore: MDP 3 -2077.9718485260573 3\n",
      "average reward explore: MDP 4 -1946.9646503287304 3\n",
      "average reward explore: MDP 5 -1728.5803593464143 3\n",
      "average reward explore: MDP 6 -2313.4758151533547 3\n",
      "average reward explore: MDP 7 -1782.036013054763 3\n",
      "average reward explore: MDP 8 -1506.6907114698245 3\n",
      "average reward explore: MDP 9 -1764.406281772276 3\n",
      "epoch number:  12\n",
      "average reward explore: MDP 0 -2082.4841566351142 3\n",
      "average reward explore: MDP 1 -2035.835923039008 3\n",
      "average reward explore: MDP 2 -1805.8102464306194 3\n",
      "average reward explore: MDP 3 -1728.0353036054078 3\n",
      "average reward explore: MDP 4 -1786.8271081233327 3\n",
      "average reward explore: MDP 5 -1640.9533877073948 3\n",
      "average reward explore: MDP 6 -1901.5382025132365 3\n",
      "average reward explore: MDP 7 -1675.6804171515375 3\n",
      "average reward explore: MDP 8 -1825.107410909215 3\n",
      "average reward explore: MDP 9 -1581.2450146389544 3\n",
      "epoch number:  13\n",
      "average reward explore: MDP 0 -1859.7008399576744 3\n",
      "average reward explore: MDP 1 -1761.9681128860855 3\n",
      "average reward explore: MDP 2 -1906.3329099009763 3\n",
      "average reward explore: MDP 3 -1658.5406528734932 3\n",
      "average reward explore: MDP 4 -1806.144022603991 3\n",
      "average reward explore: MDP 5 -1734.0495299696538 3\n",
      "average reward explore: MDP 6 -1731.2063235858575 3\n",
      "average reward explore: MDP 7 -1932.7082338697367 3\n",
      "average reward explore: MDP 8 -1873.2706646233291 3\n",
      "average reward explore: MDP 9 -1779.0872121138161 3\n",
      "epoch number:  14\n",
      "average reward explore: MDP 0 -1491.6298814426273 3\n",
      "average reward explore: MDP 1 -1700.3087651115109 3\n",
      "average reward explore: MDP 2 -1654.2335409248583 3\n",
      "average reward explore: MDP 3 -2009.0918354101987 3\n",
      "average reward explore: MDP 4 -1832.3464897671008 3\n",
      "average reward explore: MDP 5 -1682.6715278776555 3\n",
      "average reward explore: MDP 6 -1865.386576243316 3\n",
      "average reward explore: MDP 7 -1679.8531902490129 3\n",
      "average reward explore: MDP 8 -1516.252634334438 3\n",
      "average reward explore: MDP 9 -1851.8636210752993 3\n",
      "epoch number:  15\n",
      "average reward explore: MDP 0 -1624.0108109743558 3\n",
      "average reward explore: MDP 1 -1463.8704269110547 3\n",
      "average reward explore: MDP 2 -1773.94383749364 3\n",
      "average reward explore: MDP 3 -1596.5548164882084 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: MDP 4 -1826.0604122986742 3\n",
      "average reward explore: MDP 5 -1546.386363674209 3\n",
      "average reward explore: MDP 6 -1672.3151797184728 3\n",
      "average reward explore: MDP 7 -1802.0984585680792 3\n",
      "average reward explore: MDP 8 -1537.5080173912918 3\n",
      "average reward explore: MDP 9 -1847.3424046732473 3\n",
      "epoch number:  16\n",
      "average reward explore: MDP 0 -1518.7034342635545 3\n",
      "average reward explore: MDP 1 -1538.667901087536 3\n",
      "average reward explore: MDP 2 -1593.1648463486981 3\n",
      "average reward explore: MDP 3 -1643.007981491491 3\n",
      "average reward explore: MDP 4 -1487.1683376686399 3\n",
      "average reward explore: MDP 5 -1566.0263053994931 3\n",
      "average reward explore: MDP 6 -1906.5072385402518 3\n",
      "average reward explore: MDP 7 -1557.702180744937 3\n",
      "average reward explore: MDP 8 -1695.9774074552006 3\n",
      "average reward explore: MDP 9 -1828.0057917363554 3\n",
      "epoch number:  17\n",
      "average reward explore: MDP 0 -1491.8090849397984 3\n",
      "average reward explore: MDP 1 -1577.3338122057776 3\n",
      "average reward explore: MDP 2 -1788.3712597237043 3\n",
      "average reward explore: MDP 3 -1659.019991223691 3\n",
      "average reward explore: MDP 4 -1642.1193933492734 3\n",
      "average reward explore: MDP 5 -1680.969608013121 3\n",
      "average reward explore: MDP 6 -1666.046174887828 3\n",
      "average reward explore: MDP 7 -1749.1634314411351 3\n",
      "average reward explore: MDP 8 -1592.261275702127 3\n",
      "average reward explore: MDP 9 -1542.2105669599107 3\n",
      "epoch number:  18\n",
      "average reward explore: MDP 0 -1159.6777573940888 3\n",
      "average reward explore: MDP 1 -1233.1015185344777 3\n",
      "average reward explore: MDP 2 -1670.968109453195 3\n",
      "average reward explore: MDP 3 -1608.7870389649145 3\n",
      "average reward explore: MDP 4 -1545.3562178362415 3\n",
      "average reward explore: MDP 5 -1474.6417409072235 3\n",
      "average reward explore: MDP 6 -1704.8653699709728 3\n",
      "average reward explore: MDP 7 -1642.1129493691053 3\n",
      "average reward explore: MDP 8 -1584.1090506568646 3\n",
      "average reward explore: MDP 9 -1330.024071764983 3\n",
      "epoch number:  19\n",
      "average reward explore: MDP 0 -1674.367497629951 3\n",
      "average reward explore: MDP 1 -1449.995266054595 3\n",
      "average reward explore: MDP 2 -1398.4235054715703 3\n",
      "average reward explore: MDP 3 -1305.06287649368 3\n",
      "average reward explore: MDP 4 -1435.5805172818102 3\n",
      "average reward explore: MDP 5 -1481.487070522501 3\n",
      "average reward explore: MDP 6 -1217.3525978532816 3\n",
      "average reward explore: MDP 7 -1297.816708716757 3\n",
      "average reward explore: MDP 8 -1290.9809181801847 3\n",
      "average reward explore: MDP 9 -1289.157154505976 3\n",
      "epoch number:  20\n",
      "average reward explore: MDP 0 -1221.2491424052757 3\n",
      "average reward explore: MDP 1 -1579.4107918583925 3\n",
      "average reward explore: MDP 2 -1213.3273912359427 3\n",
      "average reward explore: MDP 3 -1276.3183010155435 3\n",
      "average reward explore: MDP 4 -1296.2985204793565 3\n",
      "average reward explore: MDP 5 -1299.9328703673139 3\n",
      "average reward explore: MDP 6 -1263.8731914701646 3\n",
      "average reward explore: MDP 7 -1524.8488689152507 3\n",
      "average reward explore: MDP 8 -1332.4048960461657 3\n",
      "average reward explore: MDP 9 -1399.2803645561114 3\n",
      "epoch number:  21\n",
      "average reward explore: MDP 0 -1557.322932590261 3\n",
      "average reward explore: MDP 1 -1311.5084625263544 3\n",
      "average reward explore: MDP 2 -1651.68338202437 3\n",
      "average reward explore: MDP 3 -1405.6354818749403 3\n",
      "average reward explore: MDP 4 -1379.8317534228415 3\n",
      "average reward explore: MDP 5 -1499.9410288296287 3\n",
      "average reward explore: MDP 6 -1524.1290717139693 3\n",
      "average reward explore: MDP 7 -1368.2255613368086 3\n",
      "average reward explore: MDP 8 -1306.1892868174102 3\n",
      "average reward explore: MDP 9 -1508.452851484664 3\n",
      "epoch number:  22\n",
      "average reward explore: MDP 0 -1401.8509499121874 3\n",
      "average reward explore: MDP 1 -1301.5648820388017 3\n",
      "average reward explore: MDP 2 -1366.962058500764 3\n",
      "average reward explore: MDP 3 -1261.6836916724162 3\n",
      "average reward explore: MDP 4 -1131.2310092954428 3\n",
      "average reward explore: MDP 5 -1425.221677266014 3\n",
      "average reward explore: MDP 6 -1474.474527135894 3\n",
      "average reward explore: MDP 7 -1306.0930046802935 3\n",
      "average reward explore: MDP 8 -1347.5240875301895 3\n",
      "average reward explore: MDP 9 -1522.3984731797912 3\n",
      "epoch number:  23\n",
      "average reward explore: MDP 0 -1473.7157852775629 3\n",
      "average reward explore: MDP 1 -1126.6749186776021 3\n",
      "average reward explore: MDP 2 -1230.0495199730158 3\n",
      "average reward explore: MDP 3 -1229.463998564476 3\n",
      "average reward explore: MDP 4 -1204.8513303424068 3\n",
      "average reward explore: MDP 5 -1320.6186469860177 3\n",
      "average reward explore: MDP 6 -1462.9519005867385 3\n",
      "average reward explore: MDP 7 -1174.7842669265906 3\n",
      "average reward explore: MDP 8 -1438.8799988532646 3\n",
      "average reward explore: MDP 9 -1366.5811515165617 3\n",
      "epoch number:  24\n",
      "average reward explore: MDP 0 -1452.0182974873023 3\n",
      "average reward explore: MDP 1 -1440.7498012604854 3\n",
      "average reward explore: MDP 2 -1311.322023414841 3\n",
      "average reward explore: MDP 3 -1378.8766183488267 3\n",
      "average reward explore: MDP 4 -1399.8143942892787 3\n",
      "average reward explore: MDP 5 -1433.3310292534468 3\n",
      "average reward explore: MDP 6 -1443.3072050769617 3\n",
      "average reward explore: MDP 7 -1190.0427334562366 3\n",
      "average reward explore: MDP 8 -1147.8226010570827 3\n",
      "average reward explore: MDP 9 -1190.3467854849962 3\n",
      "epoch number:  25\n",
      "average reward explore: MDP 0 -1249.652301162674 3\n",
      "average reward explore: MDP 1 -1565.361393168575 3\n",
      "average reward explore: MDP 2 -1240.7085032832051 3\n",
      "average reward explore: MDP 3 -1261.568789404691 3\n",
      "average reward explore: MDP 4 -1269.5419604573362 3\n",
      "average reward explore: MDP 5 -1446.2001699326531 3\n",
      "average reward explore: MDP 6 -1474.0199684722681 3\n",
      "average reward explore: MDP 7 -1406.8903528987867 3\n",
      "average reward explore: MDP 8 -1501.201905542771 3\n",
      "average reward explore: MDP 9 -1410.44844840499 3\n",
      "epoch number:  26\n",
      "average reward explore: MDP 0 -1291.4849564281483 3\n",
      "average reward explore: MDP 1 -1331.4069609364851 3\n",
      "average reward explore: MDP 2 -1533.0488807125776 3\n",
      "average reward explore: MDP 3 -1482.7291374651747 3\n",
      "average reward explore: MDP 4 -1081.6152336757775 3\n",
      "average reward explore: MDP 5 -1504.6550619862435 3\n",
      "average reward explore: MDP 6 -1261.3313433912201 3\n",
      "average reward explore: MDP 7 -1277.8828646463098 3\n",
      "average reward explore: MDP 8 -1546.990240953926 3\n",
      "average reward explore: MDP 9 -1389.4772698251536 3\n",
      "epoch number:  27\n",
      "average reward explore: MDP 0 -1397.245780812921 3\n",
      "average reward explore: MDP 1 -1527.960103427583 3\n",
      "average reward explore: MDP 2 -1245.0169431096372 3\n",
      "average reward explore: MDP 3 -1127.0317586193617 3\n",
      "average reward explore: MDP 4 -1342.6583753537732 3\n",
      "average reward explore: MDP 5 -1258.4781306992957 3\n",
      "average reward explore: MDP 6 -1260.000348743224 3\n",
      "average reward explore: MDP 7 -1282.479577040361 3\n",
      "average reward explore: MDP 8 -1062.1828114410598 3\n",
      "average reward explore: MDP 9 -1221.8008369252268 3\n",
      "epoch number:  28\n",
      "average reward explore: MDP 0 -1165.8633169832663 3\n",
      "average reward explore: MDP 1 -1161.8094179111247 3\n",
      "average reward explore: MDP 2 -1187.7026893534376 3\n",
      "average reward explore: MDP 3 -1331.4110817425178 3\n",
      "average reward explore: MDP 4 -1279.9160771028116 3\n",
      "average reward explore: MDP 5 -1102.5231177386684 3\n",
      "average reward explore: MDP 6 -1123.2144049207361 3\n",
      "average reward explore: MDP 7 -1080.6978542310371 3\n",
      "average reward explore: MDP 8 -1014.0557168208005 3\n",
      "average reward explore: MDP 9 -1132.5796280356699 3\n",
      "epoch number:  29\n",
      "average reward explore: MDP 0 -1153.0573389816516 3\n",
      "average reward explore: MDP 1 -1108.139864076573 3\n",
      "average reward explore: MDP 2 -996.399659170895 3\n",
      "average reward explore: MDP 3 -1133.5522782112512 3\n",
      "average reward explore: MDP 4 -1184.2520500461442 3\n",
      "average reward explore: MDP 5 -1207.72824730966 3\n",
      "average reward explore: MDP 6 -1220.6225195594889 3\n",
      "average reward explore: MDP 7 -1184.3464988548967 3\n",
      "average reward explore: MDP 8 -1096.2050044030575 3\n",
      "average reward explore: MDP 9 -1250.7934797910968 3\n",
      "epoch number:  30\n",
      "average reward explore: MDP 0 -951.0810940598212 3\n",
      "average reward explore: MDP 1 -857.2924382833199 3\n",
      "average reward explore: MDP 2 -1066.7873747848885 3\n",
      "average reward explore: MDP 3 -1178.8558531246426 3\n",
      "average reward explore: MDP 4 -866.5031137555665 3\n",
      "average reward explore: MDP 5 -857.8470558781916 3\n",
      "average reward explore: MDP 6 -1104.6924456185789 3\n",
      "average reward explore: MDP 7 -1020.1968120880935 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: MDP 8 -957.7031939513639 3\n",
      "average reward explore: MDP 9 -936.28863295868 3\n",
      "epoch number:  31\n",
      "average reward explore: MDP 0 -1163.3645766083791 3\n",
      "average reward explore: MDP 1 -1089.9270415439887 3\n",
      "average reward explore: MDP 2 -1170.5760214093295 3\n",
      "average reward explore: MDP 3 -903.8212881217684 3\n",
      "average reward explore: MDP 4 -953.2166323120218 3\n",
      "average reward explore: MDP 5 -1078.7706610215116 3\n",
      "average reward explore: MDP 6 -960.1257415873464 3\n",
      "average reward explore: MDP 7 -1087.131039748478 3\n",
      "average reward explore: MDP 8 -1072.8576862173786 3\n",
      "average reward explore: MDP 9 -1038.2061491164777 3\n",
      "epoch number:  32\n",
      "average reward explore: MDP 0 -1031.4343127828072 3\n",
      "average reward explore: MDP 1 -1224.3072825629897 3\n",
      "average reward explore: MDP 2 -968.9139080161855 3\n",
      "average reward explore: MDP 3 -983.2912410336448 3\n",
      "average reward explore: MDP 4 -1018.2732000645166 3\n",
      "average reward explore: MDP 5 -1103.1050390110668 3\n",
      "average reward explore: MDP 6 -1026.672025193992 3\n",
      "average reward explore: MDP 7 -1010.8417523873057 3\n",
      "average reward explore: MDP 8 -1003.1673070411408 3\n",
      "average reward explore: MDP 9 -1040.1601694723688 3\n",
      "epoch number:  33\n",
      "average reward explore: MDP 0 -1069.9106178249012 3\n",
      "average reward explore: MDP 1 -1161.4729775263256 3\n",
      "average reward explore: MDP 2 -1022.2972729570938 3\n",
      "average reward explore: MDP 3 -1098.1421387057176 3\n",
      "average reward explore: MDP 4 -1003.7344402481227 3\n",
      "average reward explore: MDP 5 -986.8065142514588 3\n",
      "average reward explore: MDP 6 -1040.1077475050581 3\n",
      "average reward explore: MDP 7 -1123.6214955906378 3\n",
      "average reward explore: MDP 8 -941.3478938969887 3\n",
      "average reward explore: MDP 9 -870.0745747254949 3\n",
      "epoch number:  34\n",
      "average reward explore: MDP 0 -1063.8765399997362 3\n",
      "average reward explore: MDP 1 -1124.9610534990898 3\n",
      "average reward explore: MDP 2 -938.9682702090704 3\n",
      "average reward explore: MDP 3 -1070.9829134498557 3\n",
      "average reward explore: MDP 4 -1006.4529373932637 3\n",
      "average reward explore: MDP 5 -953.600964618844 3\n",
      "average reward explore: MDP 6 -998.0439051275343 3\n",
      "average reward explore: MDP 7 -912.5384665781941 3\n",
      "average reward explore: MDP 8 -1097.264526607534 3\n",
      "average reward explore: MDP 9 -960.9915888662049 3\n",
      "epoch number:  35\n",
      "average reward explore: MDP 0 -839.479545892637 3\n",
      "average reward explore: MDP 1 -1055.2497361922913 3\n",
      "average reward explore: MDP 2 -1049.6097226375875 3\n",
      "average reward explore: MDP 3 -908.540990818734 3\n",
      "average reward explore: MDP 4 -999.076258012928 3\n",
      "average reward explore: MDP 5 -888.0107661465522 3\n",
      "average reward explore: MDP 6 -1102.9426380755028 3\n",
      "average reward explore: MDP 7 -1022.9587599240626 3\n",
      "average reward explore: MDP 8 -1078.893865218574 3\n",
      "average reward explore: MDP 9 -963.8810042980435 3\n",
      "epoch number:  36\n",
      "average reward explore: MDP 0 -803.5876286721391 3\n",
      "average reward explore: MDP 1 -962.8732154889145 3\n",
      "average reward explore: MDP 2 -1005.3176441710306 3\n",
      "average reward explore: MDP 3 -935.0839760200457 3\n",
      "average reward explore: MDP 4 -841.0345894574493 3\n",
      "average reward explore: MDP 5 -824.8338872128444 3\n",
      "average reward explore: MDP 6 -893.6162838778761 3\n",
      "average reward explore: MDP 7 -932.5598950224139 3\n",
      "average reward explore: MDP 8 -964.0920131101562 3\n",
      "average reward explore: MDP 9 -830.513099491417 3\n",
      "epoch number:  37\n",
      "average reward explore: MDP 0 -969.6221196387891 3\n",
      "average reward explore: MDP 1 -1062.8884005251778 3\n",
      "average reward explore: MDP 2 -972.9667084888292 3\n",
      "average reward explore: MDP 3 -907.4148825820857 3\n",
      "average reward explore: MDP 4 -725.7255261057871 3\n",
      "average reward explore: MDP 5 -1053.5439141691259 3\n",
      "average reward explore: MDP 6 -978.26966723373 3\n",
      "average reward explore: MDP 7 -1049.5933232431337 3\n",
      "average reward explore: MDP 8 -879.9540395674129 3\n",
      "average reward explore: MDP 9 -819.9950686199103 3\n",
      "epoch number:  38\n",
      "average reward explore: MDP 0 -759.0772135185274 3\n",
      "average reward explore: MDP 1 -931.1263903739149 3\n",
      "average reward explore: MDP 2 -902.1487303275453 3\n",
      "average reward explore: MDP 3 -929.5669050443603 3\n",
      "average reward explore: MDP 4 -701.5826921822826 3\n",
      "average reward explore: MDP 5 -923.3753745970567 3\n",
      "average reward explore: MDP 6 -783.5715431260081 3\n",
      "average reward explore: MDP 7 -859.0137228908221 3\n",
      "average reward explore: MDP 8 -854.9744907604068 3\n",
      "average reward explore: MDP 9 -869.4277635545749 3\n",
      "epoch number:  39\n",
      "average reward explore: MDP 0 -919.6431490707843 3\n",
      "average reward explore: MDP 1 -821.9345018594428 3\n",
      "average reward explore: MDP 2 -860.4295142146933 3\n",
      "average reward explore: MDP 3 -976.3569193526037 3\n",
      "average reward explore: MDP 4 -773.0348104737294 3\n",
      "average reward explore: MDP 5 -910.9998849236895 3\n",
      "average reward explore: MDP 6 -760.6962453184016 3\n",
      "average reward explore: MDP 7 -879.4021457420052 3\n",
      "average reward explore: MDP 8 -909.9496467407439 3\n",
      "average reward explore: MDP 9 -975.342955152834 3\n",
      "epoch number:  40\n",
      "average reward explore: MDP 0 -940.048144798959 3\n",
      "average reward explore: MDP 1 -922.546987749556 3\n",
      "average reward explore: MDP 2 -892.2128865319688 3\n",
      "average reward explore: MDP 3 -770.5185179224787 3\n",
      "average reward explore: MDP 4 -826.5742494981041 3\n",
      "average reward explore: MDP 5 -943.0193096401061 3\n",
      "average reward explore: MDP 6 -903.8497706837519 3\n",
      "average reward explore: MDP 7 -949.9495837603412 3\n",
      "average reward explore: MDP 8 -961.2688989482303 3\n",
      "average reward explore: MDP 9 -898.5785276772436 3\n",
      "epoch number:  41\n",
      "average reward explore: MDP 0 -880.6102261054912 3\n",
      "average reward explore: MDP 1 -981.6873316870484 3\n",
      "average reward explore: MDP 2 -903.1157063684996 3\n",
      "average reward explore: MDP 3 -836.5680724370019 3\n",
      "average reward explore: MDP 4 -925.4829339869052 3\n",
      "average reward explore: MDP 5 -636.3568174525368 3\n",
      "average reward explore: MDP 6 -631.8757297266893 3\n",
      "average reward explore: MDP 7 -854.3720984713568 3\n",
      "average reward explore: MDP 8 -728.2490075046647 3\n",
      "average reward explore: MDP 9 -925.7852009705465 3\n",
      "epoch number:  42\n",
      "average reward explore: MDP 0 -828.6775942021076 3\n",
      "average reward explore: MDP 1 -676.7180089211845 3\n",
      "average reward explore: MDP 2 -874.6245299582084 3\n",
      "average reward explore: MDP 3 -900.191074945381 3\n",
      "average reward explore: MDP 4 -751.4883828097765 3\n",
      "average reward explore: MDP 5 -786.8917538941955 3\n",
      "average reward explore: MDP 6 -791.396900471715 3\n",
      "average reward explore: MDP 7 -797.7376786075083 3\n",
      "average reward explore: MDP 8 -802.4841584337637 3\n",
      "average reward explore: MDP 9 -742.1575238072177 3\n",
      "epoch number:  43\n",
      "average reward explore: MDP 0 -744.4040043748658 3\n",
      "average reward explore: MDP 1 -940.0630070380058 3\n",
      "average reward explore: MDP 2 -770.5530272817974 3\n",
      "average reward explore: MDP 3 -842.8064636802133 3\n",
      "average reward explore: MDP 4 -629.6852458346916 3\n",
      "average reward explore: MDP 5 -702.6472233987128 3\n",
      "average reward explore: MDP 6 -713.095564840609 3\n",
      "average reward explore: MDP 7 -770.9590976235078 3\n",
      "average reward explore: MDP 8 -780.4873687711429 3\n",
      "average reward explore: MDP 9 -790.9670118566988 3\n",
      "epoch number:  44\n",
      "average reward explore: MDP 0 -759.1739053114616 3\n",
      "average reward explore: MDP 1 -875.2693282081585 3\n",
      "average reward explore: MDP 2 -715.6651082691628 3\n",
      "average reward explore: MDP 3 -660.4229848263604 3\n",
      "average reward explore: MDP 4 -579.4062356573776 3\n",
      "average reward explore: MDP 5 -636.8323283881031 3\n",
      "average reward explore: MDP 6 -771.6555327875044 3\n",
      "average reward explore: MDP 7 -664.0637201772973 3\n",
      "average reward explore: MDP 8 -593.9001448633035 3\n",
      "average reward explore: MDP 9 -764.9088348114248 3\n",
      "epoch number:  45\n",
      "average reward explore: MDP 0 -653.857806202254 3\n",
      "average reward explore: MDP 1 -551.6216641152814 3\n",
      "average reward explore: MDP 2 -620.8683805795405 3\n",
      "average reward explore: MDP 3 -589.0031499066408 3\n",
      "average reward explore: MDP 4 -667.374626472165 3\n",
      "average reward explore: MDP 5 -495.6589344631115 3\n",
      "average reward explore: MDP 6 -491.6989205963007 3\n",
      "average reward explore: MDP 7 -559.274278938804 3\n",
      "average reward explore: MDP 8 -771.7647454237676 3\n",
      "average reward explore: MDP 9 -523.1120763741707 3\n",
      "epoch number:  46\n",
      "average reward explore: MDP 0 -674.7838415274332 3\n",
      "average reward explore: MDP 1 -722.3716157486597 3\n",
      "average reward explore: MDP 2 -892.9082061118517 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: MDP 3 -745.1757309710111 3\n",
      "average reward explore: MDP 4 -740.8456646609608 3\n",
      "average reward explore: MDP 5 -682.8863603943342 3\n",
      "average reward explore: MDP 6 -621.6983424070801 3\n",
      "average reward explore: MDP 7 -702.106861466635 3\n",
      "average reward explore: MDP 8 -715.570558367246 3\n",
      "average reward explore: MDP 9 -759.7638027534562 3\n",
      "epoch number:  47\n",
      "average reward explore: MDP 0 -759.9707099552478 3\n",
      "average reward explore: MDP 1 -690.5168044495318 3\n",
      "average reward explore: MDP 2 -839.4621635207321 3\n",
      "average reward explore: MDP 3 -633.4802516673328 3\n",
      "average reward explore: MDP 4 -672.4700706524433 3\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    explore_paths, explore_rewards = a.sample_paths_explore(a.env,200*np.random.rand())\n",
    "    print(\"gravity:\", a.sim.model.opt.gravity)\n",
    "    print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "    M = a.stack_trajectories(explore_paths)\n",
    "    Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "    Z = np.reshape(Z,[1,len(Z)])\n",
    "    exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1)\n",
    "    print(\"average reward exploit\", np.mean(exploit_rewards), len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a.lr = 3e-3\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "        print(\"epoch number: \", epoch)\n",
    "        a.train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_traj = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_paths, explore_rewards = a.sample_paths_explore(a.env,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: 151.57677928797034 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward explore:\", np.mean(explore_rewards), len(explore_rewards))\n",
    "M = a.stack_trajectories(explore_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit 18.577710931432883 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward exploit\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_traj = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gravity: [ 0.         0.        -5.4325712]\n",
      "average reward explore 3.5619071147766848\n",
      "average reward exploit 2.7301699712255383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-730f26c155e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mexplore_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplore_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_paths_explore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#print(\"average reward explore:\", np.mean(explore_rewards))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplore_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-fb36fe994242>\u001b[0m in \u001b[0;36msample_paths_explore\u001b[0;34m(self, env, mdp_num, Test, num_episodes)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;31m#action = self.sess.run(self.explore_action, feed_dict={self.observation_placeholder_explore : states[-1][None]})[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    maxrev2 = 0\n",
    "    maxrev1 = 0\n",
    "    traj = np.random.rand()*(i+1)\n",
    "    \n",
    "    for i in range(4):\n",
    "        explore_paths, explore_rewards = a.sample_paths_explore(a.env,traj)\n",
    "        #print(\"average reward explore:\", np.mean(explore_rewards))\n",
    "        M = a.stack_trajectories(explore_paths)\n",
    "        Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "        Z = np.reshape(Z,[1,len(Z)])\n",
    "        exploit_paths, explore_rewards = a.sample_paths_exploit(a.env,Z,traj)\n",
    "        if(np.mean(explore_rewards) > maxrev1):\n",
    "            maxrev1 = np.mean(explore_rewards) / num_traj\n",
    "        if(np.mean(exploit_rewards)>maxrev2):\n",
    "            maxrev2 = np.mean(exploit_rewards) / num_traj\n",
    "    print(\"gravity:\", a.sim.model.opt.gravity)\n",
    "    print(\"average reward explore\", maxrev1)\n",
    "    print(\"average reward exploit\", maxrev2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_paths, explore_rewards = a.sample_paths_explore(a.env,5)\n",
    "observations_explore = np.concatenate([path[\"observation\"] for path in explore_paths])\n",
    "new_observations_explore = np.concatenate([path[\"new_obs\"] for path in explore_paths])\n",
    "new_actions_explore = np.concatenate([path[\"new_acs\"] for path in explore_paths])\n",
    "actions_explore = np.concatenate([path[\"action\"] for path in explore_paths])\n",
    "rewards_explore = np.concatenate([path[\"reward\"] for path in explore_paths])\n",
    "returns_explore = a.get_returns(explore_paths, explore = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array(a.stack_trajectories(explore_paths))\n",
    "Z = a.sess.run(a.Z, feed_dict = {a.encoder_input_placeholder: M,a.sequence_length_placeholder: a.length })\n",
    "Z = np.reshape(Z,[1,len(Z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward explore: MDP 20488.85105031593 800\n"
     ]
    }
   ],
   "source": [
    " print(\"average reward explore: MDP\", np.sum(explore_rewards)/num_traj, len(explore_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit_paths, exploit_rewards = a.sample_paths_exploit(a.env,Z,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward exploit: MDP 644.4290518284578 400\n"
     ]
    }
   ],
   "source": [
    "print(\"average reward exploit: MDP\", np.sum(exploit_rewards) / num_traj, len(exploit_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_pendulum(env):\n",
    "    m = config['pendulum_mass_min'] + np.random.rand()*(config['pendulum_mass_max'] - config['pendulum_mass_min'])\n",
    "    l = config['pendulum_len_min'] + np.random.rand()*(config['pendulum_len_max'] - config['pendulum_len_min'])\n",
    "    env.m = m\n",
    "    env.l = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_hopper(env):\n",
    "    ts = config['torso_min'] + np.random.rand()*(config['torso_max'] - config['torso_min'])\n",
    "    f = config['friction_min'] + np.random.rand()*(config['friction_max'] - config['friction_min'])\n",
    "    \n",
    "    env.friction = f\n",
    "    env.torso_size = ts\n",
    "    env.apply_env_modifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "cfg_filename = 'hopper-config.yml'\n",
    "with open(cfg_filename,'r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Randomizer(gym.make('Hopper-v2'), randomize_hopper)\n",
    "env.unwrapped.__dict__.keys()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = config['pendulum_mass_min'] + np.random.rand()*(config['pendulum_mass_max'] - config['pendulum_mass_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'Hopper-v2'\n",
    "e = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.unwrapped.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.unwrapped.apply_env_modifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mujoco_py.cymj.PyMjOption at 0x1c23ba8ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sim.model.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[112.13464748855134,\n",
       " 107.88303586312765,\n",
       " 186.02669267001477,\n",
       " 135.2581011205201,\n",
       " 79.13080731610503,\n",
       " 121.000075027127,\n",
       " 173.12633008893275,\n",
       " 183.049128023303,\n",
       " 178.67252211302042,\n",
       " 87.14624506369364,\n",
       " 194.95350061967267,\n",
       " 99.25651092576477,\n",
       " 71.49508077704641,\n",
       " 88.63821085625852,\n",
       " 102.13457423812075,\n",
       " 158.97240001687823,\n",
       " 116.1121244537115,\n",
       " 107.73547738516186,\n",
       " 139.42624826594056,\n",
       " 66.64122421143784,\n",
       " 180.7967956127941,\n",
       " 81.9917085465781,\n",
       " 86.7164580284039,\n",
       " 216.9255537144621,\n",
       " 196.23999804809637,\n",
       " 106.97407952338774,\n",
       " 211.77799925827094,\n",
       " 138.13634311633172,\n",
       " 146.8420615449942,\n",
       " 128.17126276654236,\n",
       " 74.77819434837676,\n",
       " 183.75160687642273,\n",
       " 72.78452895798674,\n",
       " 120.41099605928771,\n",
       " 101.5334367194092,\n",
       " 168.35981035001126,\n",
       " 69.62303405123133,\n",
       " 121.26107855071695,\n",
       " 158.01986971006684,\n",
       " 200.6322013111029,\n",
       " 71.87886597554524,\n",
       " 186.44100396410624,\n",
       " 90.5373394288748,\n",
       " 162.02200487464742,\n",
       " 171.76682194626335,\n",
       " 64.66835988088545,\n",
       " 93.07526440100999,\n",
       " 166.23306413321885,\n",
       " 105.99062506542288,\n",
       " 143.228189242465,\n",
       " 109.4308873122448,\n",
       " 143.5163139595506,\n",
       " 115.2344747817427,\n",
       " 60.20145334942355,\n",
       " 79.41136565212034,\n",
       " 183.78021364424464,\n",
       " 162.80513615600867,\n",
       " 87.64483751806068,\n",
       " 179.35113237455454,\n",
       " 60.607104642814754,\n",
       " 158.86527932123138,\n",
       " 125.77884930841314,\n",
       " 57.846734325285844,\n",
       " 123.92092940759048,\n",
       " 64.76586275688906,\n",
       " 164.67320276421657,\n",
       " 217.5985315618744,\n",
       " 198.02047584112512,\n",
       " 174.78015485384395,\n",
       " 78.6406920984788,\n",
       " 82.74185193615898,\n",
       " 181.4236702099464,\n",
       " 164.5192539944514,\n",
       " 175.46044451685484,\n",
       " 112.41872354355482,\n",
       " 120.65323070257459,\n",
       " 67.70633670005066,\n",
       " 152.0323454228858,\n",
       " 59.228267657095785,\n",
       " 159.06147132614302,\n",
       " 138.10635790834223,\n",
       " 75.88459957410683,\n",
       " 191.71341643467542,\n",
       " 158.4351805403924,\n",
       " 159.69731797408917,\n",
       " 111.49673121212818,\n",
       " 123.54878963166887,\n",
       " 123.15187865769032,\n",
       " 204.0635926964165,\n",
       " 189.31065397658512,\n",
       " 111.47756666995318,\n",
       " 164.26777641403527,\n",
       " 186.38204078987383,\n",
       " 89.76715796335638,\n",
       " 146.0933463146793,\n",
       " 152.610803218257,\n",
       " 114.77645412380338,\n",
       " 108.81992797106655,\n",
       " 188.25841809794176,\n",
       " 138.05299981344763,\n",
       " 184.15135002671167,\n",
       " 165.51448940409804,\n",
       " 175.52528317336584,\n",
       " 158.04278077734477,\n",
       " 156.84343321851523,\n",
       " 162.88627891367096,\n",
       " 188.41426192056605,\n",
       " 166.88873086055196,\n",
       " 116.24395041654002,\n",
       " 118.9098479167613,\n",
       " 169.21638209609134,\n",
       " 125.60153708455401,\n",
       " 168.97418752440572,\n",
       " 82.57725942062146,\n",
       " 135.01744845514526,\n",
       " 155.72827335098413,\n",
       " 178.80459441292803,\n",
       " 149.18428778521994,\n",
       " 159.1717479508303,\n",
       " 166.51255884406552,\n",
       " 173.43399285166555,\n",
       " 114.84112110685228,\n",
       " 169.42098247486686,\n",
       " 171.53876987424545,\n",
       " 107.16562997614457,\n",
       " 151.592953157463,\n",
       " 160.23711460638478,\n",
       " 162.55928788792957,\n",
       " 139.3097392754981,\n",
       " 114.9124209995656,\n",
       " 56.30767436211967,\n",
       " 116.56671702163149,\n",
       " 111.5149260443636,\n",
       " 163.46270836733547,\n",
       " 209.81775366280502,\n",
       " 175.45418362699255,\n",
       " 176.27783490931438,\n",
       " 99.88984408856093,\n",
       " 170.4815846265511,\n",
       " 140.43457537901952,\n",
       " 160.7437604778002,\n",
       " 101.26544910160446,\n",
       " 186.79100277608921,\n",
       " 64.93570257422888,\n",
       " 127.39573549217515,\n",
       " 184.99808188972617,\n",
       " 155.48670478950834,\n",
       " 68.31059687806311,\n",
       " 116.59860309305891,\n",
       " 170.39817305577597,\n",
       " 180.91296974869903,\n",
       " 152.9999943228542,\n",
       " 55.2057682094539,\n",
       " 123.9159015413261,\n",
       " 153.07469346847233,\n",
       " 158.5662290602995,\n",
       " 167.96563872160917,\n",
       " 94.16104827522985,\n",
       " 164.29140176537447,\n",
       " 162.19781056445146,\n",
       " 66.93100045650992,\n",
       " 60.91080210686061,\n",
       " 155.67670483170477,\n",
       " 174.63819653598944,\n",
       " 175.62832402040308,\n",
       " 113.98475380740328,\n",
       " 151.025130814201,\n",
       " 184.08070546884474,\n",
       " 197.89987483640658,\n",
       " 166.34365956938348,\n",
       " 185.90693339067053,\n",
       " 155.405899623845,\n",
       " 108.73802281684786,\n",
       " 71.92198829803677,\n",
       " 177.67728172973392,\n",
       " 189.19418485702744,\n",
       " 179.90317459982495,\n",
       " 184.84496980886635,\n",
       " 84.99476214441425,\n",
       " 111.45349416019073,\n",
       " 95.96066575397037,\n",
       " 221.89746310923368,\n",
       " 167.29304647107165,\n",
       " 100.45878395802286,\n",
       " 64.92333651823472,\n",
       " 112.27092564166203,\n",
       " 90.06206676145489,\n",
       " 146.39087807704877,\n",
       " 137.81828611991392,\n",
       " 157.07481173296145,\n",
       " 119.59676340646334,\n",
       " 73.47632593638016,\n",
       " 105.2242473701803,\n",
       " 189.47087231516565,\n",
       " 158.89309952832096,\n",
       " 123.24530573630302,\n",
       " 168.74937121889718,\n",
       " 138.6119878719888,\n",
       " 168.0969789849356,\n",
       " 170.62416276935818,\n",
       " 167.754198574978,\n",
       " 108.95616635176627,\n",
       " 194.97695297709222,\n",
       " 194.29747925519328,\n",
       " 178.67396703671469,\n",
       " 165.46589124304427,\n",
       " 90.28994201137101,\n",
       " 188.74143598694346,\n",
       " 125.04081290630428,\n",
       " 154.65044266308902,\n",
       " 102.46636008012295,\n",
       " 100.11554168795548,\n",
       " 170.91308109496129,\n",
       " 155.19047732795164,\n",
       " 102.12738265789064,\n",
       " 73.16181212374306,\n",
       " 170.16072823260023,\n",
       " 148.1590655671541,\n",
       " 189.96344126682635,\n",
       " 121.24110411269352,\n",
       " 124.25682682196863,\n",
       " 137.60212823136854,\n",
       " 123.6499407440728,\n",
       " 144.70326934900717,\n",
       " 157.61354368678892,\n",
       " 165.1887064543368,\n",
       " 150.13792688997142,\n",
       " 170.50968679891525,\n",
       " 107.40474661209676,\n",
       " 187.92630362822896,\n",
       " 171.80751432624095,\n",
       " 87.54920454902833,\n",
       " 187.56078128096428,\n",
       " 126.9021969860226,\n",
       " 61.073377970628364,\n",
       " 79.8378169904285,\n",
       " 184.65590876042876,\n",
       " 138.11240117235025,\n",
       " 114.46582064662607,\n",
       " 202.9490539883215,\n",
       " 84.84737575084107,\n",
       " 119.61693920616204,\n",
       " 161.76096501574867,\n",
       " 73.79217743284339,\n",
       " 165.21248152372328,\n",
       " 195.88894726115467,\n",
       " 179.45762239125025,\n",
       " 119.25894091202768,\n",
       " 165.8499876712191,\n",
       " 163.02155204297222,\n",
       " 114.80144383470349,\n",
       " 172.95707879010018,\n",
       " 138.09567519558797,\n",
       " 78.01488266784014,\n",
       " 111.42568653879442,\n",
       " 78.6547113169232,\n",
       " 196.61864800042588,\n",
       " 163.91173960195496,\n",
       " 90.6724933743234,\n",
       " 109.21782422844883,\n",
       " 191.23924303173578,\n",
       " 179.57993763471634,\n",
       " 100.5618416601382,\n",
       " 146.5140892260237,\n",
       " 140.05267884602674,\n",
       " 187.28481930517745,\n",
       " 163.31293483537556,\n",
       " 91.10761390655676,\n",
       " 151.6882722797837,\n",
       " 170.50280782363396,\n",
       " 108.51150198344183,\n",
       " 187.9751469395834,\n",
       " 137.3987602658892,\n",
       " 152.60378420359584,\n",
       " 129.7790180146226,\n",
       " 184.5950702165,\n",
       " 145.10017115995348,\n",
       " 108.93162191604206,\n",
       " 159.94400042864663,\n",
       " 158.57705956080912,\n",
       " 114.75847535280559,\n",
       " 94.8289213532897,\n",
       " 101.24432769178549,\n",
       " 145.00535960776062,\n",
       " 175.15319391853885,\n",
       " 166.24847041107589,\n",
       " 183.22554145014652,\n",
       " 155.36464560915147,\n",
       " 83.55061695048877,\n",
       " 162.90929343888735,\n",
       " 110.61380141948368,\n",
       " 93.01847339476095,\n",
       " 111.36720440755657,\n",
       " 199.43240560635525,\n",
       " 154.39226877344186,\n",
       " 134.23219811822142,\n",
       " 175.0187659641553,\n",
       " 83.51763676516553,\n",
       " 155.94449481431639,\n",
       " 133.6998695570268,\n",
       " 86.23446070044483,\n",
       " 78.50949156371904,\n",
       " 173.79626581992414,\n",
       " 162.77855082722536,\n",
       " 83.42137359070993,\n",
       " 194.97309237125322,\n",
       " 111.04123889982446,\n",
       " 191.0403270548807,\n",
       " 131.39542236182626,\n",
       " 182.01060562678185,\n",
       " 160.35570079318518,\n",
       " 76.25971014896251,\n",
       " 102.66210583955788,\n",
       " 124.78694717791087,\n",
       " 186.31913235993025,\n",
       " 130.4241857577105,\n",
       " 125.86402233457912,\n",
       " 159.65124698187867,\n",
       " 171.98788428743958,\n",
       " 111.90435314227774,\n",
       " 169.05038055051875,\n",
       " 95.8220871358059,\n",
       " 116.53015500009899,\n",
       " 130.12229729853436,\n",
       " 155.93156973198674,\n",
       " 162.19998363677647,\n",
       " 118.9088840882428,\n",
       " 182.9131958811087,\n",
       " 159.37523039033306,\n",
       " 167.6324162759753,\n",
       " 206.03761570014734,\n",
       " 100.4322031371492,\n",
       " 104.7584895469109,\n",
       " 70.68655794797172,\n",
       " 84.18939568037818,\n",
       " 191.62844514800258,\n",
       " 81.3421780910568,\n",
       " 129.69266941008482,\n",
       " 122.2434285486572,\n",
       " 199.13639090661073,\n",
       " 128.48077611425407,\n",
       " 166.12630325695477,\n",
       " 161.8878810291055,\n",
       " 160.66829947187873,\n",
       " 136.93280377099546,\n",
       " 186.81124019953143,\n",
       " 104.55614575005836,\n",
       " 199.15663354338034,\n",
       " 105.17298089873758,\n",
       " 157.18968585175875,\n",
       " 178.91477795540197,\n",
       " 115.5543658473315,\n",
       " 68.0302818158207,\n",
       " 148.24724283982516,\n",
       " 45.13841070994898,\n",
       " 95.99318929577541,\n",
       " 167.8307533692741,\n",
       " 188.25355467850324,\n",
       " 98.69309220923157,\n",
       " 111.06260591420735,\n",
       " 178.52161755093746,\n",
       " 118.24823952768493,\n",
       " 201.7235873105987,\n",
       " 169.1956912836896,\n",
       " 179.33454365748116,\n",
       " 81.66916522624804,\n",
       " 61.246066543679454,\n",
       " 100.72542334922018,\n",
       " 150.11418318162004,\n",
       " 174.95435490395033,\n",
       " 89.50068796774528,\n",
       " 178.81266509462466,\n",
       " 84.55475555929331,\n",
       " 168.71265549272033,\n",
       " 192.7560020741052,\n",
       " 63.945376219342926,\n",
       " 171.7243591724856,\n",
       " 179.77963215746692,\n",
       " 105.46121172218646,\n",
       " 93.83646669804898,\n",
       " 112.06465270292784,\n",
       " 173.7027056113586,\n",
       " 92.76332763415257,\n",
       " 170.34634102703615,\n",
       " 95.51235672187876,\n",
       " 105.96872766347329,\n",
       " 96.23510907079141,\n",
       " 171.92010711550188,\n",
       " 112.16344937059776,\n",
       " 126.70826464985137,\n",
       " 122.03236548154152,\n",
       " 88.94800272836761,\n",
       " 195.06366856180233,\n",
       " 171.622430281884,\n",
       " 170.79768278482211,\n",
       " 160.33641767920162,\n",
       " 197.90114557099335,\n",
       " 211.03447493919444,\n",
       " 159.8258154419341,\n",
       " 158.76722176929323,\n",
       " 144.69735089106553,\n",
       " 177.40918235716137,\n",
       " 156.90540839809847,\n",
       " 137.7127355490243,\n",
       " 174.17177921250916,\n",
       " 83.71357019180623,\n",
       " 74.91536780352305,\n",
       " 186.92143950629608,\n",
       " 121.82388499025166,\n",
       " 183.6816659674546,\n",
       " 215.78343075217512,\n",
       " 173.03938208349686,\n",
       " 125.20289776380497,\n",
       " 155.18601830476211,\n",
       " 196.39126816136582,\n",
       " 161.7497978825844,\n",
       " 174.61889012647868,\n",
       " 65.54663048225044,\n",
       " 157.0248160476821,\n",
       " 62.013953587158035,\n",
       " 145.08070431203993,\n",
       " 204.4799135367354,\n",
       " 183.46892960179872,\n",
       " 169.5846061967458,\n",
       " 160.5596322556337,\n",
       " 198.4127872622094,\n",
       " 122.11691626931633,\n",
       " 147.88888449708367,\n",
       " 93.42186545782172,\n",
       " 105.40173478109776,\n",
       " 110.5826811846747,\n",
       " 143.86917056646578,\n",
       " 101.22077695947083,\n",
       " 148.40974160667747,\n",
       " 112.58156677270891,\n",
       " 64.71347947544407,\n",
       " 189.5665605856417,\n",
       " 123.98079768695281,\n",
       " 197.29410823363148,\n",
       " 72.28259185998074,\n",
       " 150.8796903877479,\n",
       " 122.09968749452402,\n",
       " 170.63292305955713,\n",
       " 67.69921288348104,\n",
       " 149.9891438233522,\n",
       " 189.93931500012005,\n",
       " 188.80454210176657,\n",
       " 166.75536743726886,\n",
       " 125.94392424460939,\n",
       " 80.72080708527228,\n",
       " 122.67435837866586,\n",
       " 115.81750608797329,\n",
       " 118.43471782667733,\n",
       " 110.72592709287412,\n",
       " 114.33456511012297,\n",
       " 100.78131459029609,\n",
       " 146.04379873794403,\n",
       " 194.2841641870514,\n",
       " 115.7590163024971,\n",
       " 167.9687777746371,\n",
       " 178.6367600945511,\n",
       " 195.39825566996254,\n",
       " 168.77034911371817,\n",
       " 89.96650422612986,\n",
       " 194.96716595179657,\n",
       " 125.13399841242413,\n",
       " 183.64540533770221,\n",
       " 142.02873027754407,\n",
       " 184.991755853192,\n",
       " 168.84706461417233,\n",
       " 159.48572439884825,\n",
       " 177.61870560362792,\n",
       " 79.3007810232649,\n",
       " 172.31957180265795,\n",
       " 152.5538351068324,\n",
       " 190.32822084034825,\n",
       " 142.3576744898982,\n",
       " 176.600428789626,\n",
       " 183.03758223792016,\n",
       " 165.51858291743346,\n",
       " 161.09118308421102,\n",
       " 183.2042590081598,\n",
       " 188.78826520519405,\n",
       " 95.3430762519607,\n",
       " 179.0625602293134,\n",
       " 125.20878058680984,\n",
       " 118.04152046407314,\n",
       " 193.67312720275402,\n",
       " 176.17849550343735,\n",
       " 164.27820380767363,\n",
       " 162.2653138848123,\n",
       " 125.7935363123882,\n",
       " 120.80737728943947,\n",
       " 208.89402371255466,\n",
       " 168.44531499828733,\n",
       " 78.79635701720501,\n",
       " 52.933077062864534,\n",
       " 174.4198573331745,\n",
       " 102.11441476501591,\n",
       " 135.4017529521238,\n",
       " 196.57738868191865,\n",
       " 127.61503447061828,\n",
       " 138.20086437144528,\n",
       " 165.43492219656562,\n",
       " 91.09763914095626,\n",
       " 179.45687401599008,\n",
       " 162.32562860316386,\n",
       " 130.6881983103069,\n",
       " 161.51411068527207,\n",
       " 149.78322420646572,\n",
       " 190.25376386433922,\n",
       " 175.99412333077444,\n",
       " 209.55887752446844,\n",
       " 56.44511671248712,\n",
       " 128.74667240200338,\n",
       " 140.62867410712596,\n",
       " 192.78451557645008,\n",
       " 190.47539757300817,\n",
       " 84.70669845912191,\n",
       " 126.5645608696394,\n",
       " 182.06050796465343,\n",
       " 208.34115922303175,\n",
       " 162.1540608456324,\n",
       " 112.39761701062665,\n",
       " 185.5956628018305,\n",
       " 170.51407443450358,\n",
       " 120.78962249742,\n",
       " 153.20488080614552,\n",
       " 169.34602573926276,\n",
       " 172.10023392706756,\n",
       " 143.0893403065432,\n",
       " 119.3252686220417,\n",
       " 145.03546702916725,\n",
       " 73.54042323738858,\n",
       " 131.3990284792499,\n",
       " 172.68962202453574,\n",
       " 182.56842851538042,\n",
       " 63.93600461625489,\n",
       " 124.95537578740365,\n",
       " 122.32755846393219,\n",
       " 70.88135173384464,\n",
       " 194.6268360909349,\n",
       " 105.47057148169274,\n",
       " 116.09942731745254,\n",
       " 185.77037358146475,\n",
       " 109.15381487476436,\n",
       " 58.97067227733653,\n",
       " 197.49325583510105,\n",
       " 172.64472309860318,\n",
       " 126.29319183408138,\n",
       " 111.17806718280472,\n",
       " 81.53392069338761,\n",
       " 123.39432590941928,\n",
       " 157.06707483318246,\n",
       " 189.92143090282906,\n",
       " 170.38070446293614,\n",
       " 184.56083588704786,\n",
       " 138.4038808227547,\n",
       " 170.56165414553294,\n",
       " 58.83621484947255,\n",
       " 107.72743244667221,\n",
       " 125.39181473229924,\n",
       " 207.00184707175782,\n",
       " 176.7181862573779,\n",
       " 128.20267780934688,\n",
       " 110.1624284409427,\n",
       " 175.20428977443635,\n",
       " 169.2167023983227,\n",
       " 120.1761763075121,\n",
       " 147.44031908594064,\n",
       " 125.89685343441269,\n",
       " 164.3261430996038,\n",
       " 216.688975576298,\n",
       " 154.84258772317511,\n",
       " 110.5522527681017,\n",
       " 96.96648934325393,\n",
       " 147.25021810997976,\n",
       " 79.6858090832753,\n",
       " 181.47750462561234,\n",
       " 161.4841523214461,\n",
       " 175.62685707151857,\n",
       " 182.7863765384666,\n",
       " 57.791774733943086,\n",
       " 170.54544773326867,\n",
       " 63.58967832813962,\n",
       " 151.32530416857955,\n",
       " 68.14729754121657,\n",
       " 155.69322416622532,\n",
       " 119.01286819191719,\n",
       " 179.39336685134242,\n",
       " 168.14188234563224,\n",
       " 129.5306808185457,\n",
       " 47.673895551441234,\n",
       " 161.14791796998955,\n",
       " 40.818125669411096,\n",
       " 161.2240191735624,\n",
       " 137.03165239991978,\n",
       " 206.9929920661403,\n",
       " 54.95163424733535,\n",
       " 179.44544458327047,\n",
       " 170.84009422892328,\n",
       " 161.4254455510234,\n",
       " 182.69232360783312,\n",
       " 182.2709928334863,\n",
       " 148.78112093397593,\n",
       " 80.7076429784921,\n",
       " 115.8267136584442,\n",
       " 85.38589989440067,\n",
       " 75.62843168051641,\n",
       " 157.46739449757396,\n",
       " 68.54325250797959,\n",
       " 180.54094989795692,\n",
       " 93.61985979424587,\n",
       " 74.91370000767155,\n",
       " 153.8449140669033,\n",
       " 80.07132798314431,\n",
       " 103.70719058065505,\n",
       " 64.8135336466496,\n",
       " 114.63980993064506,\n",
       " 163.44360047509406,\n",
       " 169.15617175542957,\n",
       " 194.275757796929,\n",
       " 168.18171533233894,\n",
       " 168.1384694308446,\n",
       " 131.7049523192029,\n",
       " 114.48926521147678,\n",
       " 97.45672018725938,\n",
       " 231.5197675997255,\n",
       " 123.4856566506957,\n",
       " 117.49339895924102,\n",
       " 187.7353365158894,\n",
       " 98.4002463550037,\n",
       " 156.71379077772391,\n",
       " 173.2845681275291,\n",
       " 180.487728889963,\n",
       " 62.82088427234667,\n",
       " 163.17368178662724,\n",
       " 169.92634304132736,\n",
       " 144.32509442639727,\n",
       " 157.87439844314096,\n",
       " 137.12573237945344,\n",
       " 166.19691483625894,\n",
       " 59.57733556504935,\n",
       " 107.45979805462858,\n",
       " 91.67756591152141,\n",
       " 191.49393104926614,\n",
       " 158.00610043985216,\n",
       " 182.90229637984547,\n",
       " 137.40685239588606,\n",
       " 108.58412189158004,\n",
       " 167.42229683588175,\n",
       " 76.13163503703635,\n",
       " 93.67055671644665,\n",
       " 131.5099549695988,\n",
       " 68.99410589733462,\n",
       " 176.3821760854944,\n",
       " 163.69842860303515,\n",
       " 170.96051022393726,\n",
       " 187.88334458596853,\n",
       " 116.56702301589345,\n",
       " 143.40050862596473,\n",
       " 79.46858049356821,\n",
       " 186.48251810531025,\n",
       " 188.8607277296337,\n",
       " 182.74265567874335,\n",
       " 108.70403132054577,\n",
       " 171.02719393167754,\n",
       " 203.86990020833966,\n",
       " 68.34561242656554,\n",
       " 163.02841555446872,\n",
       " 175.38110001350617,\n",
       " 173.57429466489083,\n",
       " 183.88227404492392,\n",
       " 150.79586862718986,\n",
       " 174.772420779452,\n",
       " 189.16881341871996,\n",
       " 173.5847643777267,\n",
       " 171.2025008814231,\n",
       " 232.08497789417027,\n",
       " 173.0634683423599,\n",
       " 125.62180148656394,\n",
       " 202.47011160944132,\n",
       " 119.5579116275682,\n",
       " 130.2622225922786,\n",
       " 94.6443205657611,\n",
       " 205.80278341310134,\n",
       " 165.59584928603928,\n",
       " 84.63463859510792,\n",
       " 126.26521337384801,\n",
       " 117.61985719835387,\n",
       " 177.09452125270343,\n",
       " 106.27041002145053,\n",
       " 207.7355066062048,\n",
       " 144.27647676792924,\n",
       " 103.68388585589646,\n",
       " 157.999291370704,\n",
       " 124.60575925880538,\n",
       " 87.34593652581627,\n",
       " 180.06511229405896,\n",
       " 131.59984086703855,\n",
       " 142.08375105261737,\n",
       " 166.79923207313155,\n",
       " 172.7535286741458,\n",
       " 191.40947834637063,\n",
       " 150.56261528112984,\n",
       " 166.677828361661,\n",
       " 101.99179701354096,\n",
       " 108.56770309691116,\n",
       " 101.51606864411242,\n",
       " 181.79634494543106,\n",
       " 170.2444656792795,\n",
       " 179.20563484470972,\n",
       " 164.26252062158025,\n",
       " 181.51695932740205,\n",
       " 100.40002690541863,\n",
       " 133.65799783185477,\n",
       " 118.47030148557795,\n",
       " 183.26208547170236,\n",
       " 111.24645248350501,\n",
       " 153.5613469465519,\n",
       " 206.35073751102237,\n",
       " 192.08400791188734,\n",
       " 124.4242491904403,\n",
       " 177.28709148337924,\n",
       " 214.10429416987708,\n",
       " 177.26702722137117,\n",
       " 119.56246777367731,\n",
       " 179.51259537881006,\n",
       " 55.7601449612337,\n",
       " 60.31923784127708,\n",
       " 171.78353165258486,\n",
       " 68.41941283113965,\n",
       " 163.47800109548902,\n",
       " 162.41194224390745,\n",
       " 135.43705698916548,\n",
       " 109.04353998766628,\n",
       " 155.92082256399814,\n",
       " 109.59965658298385,\n",
       " 144.03238697439747,\n",
       " 107.3112753270377,\n",
       " 159.66541627363026,\n",
       " 182.29016557923472,\n",
       " 164.32446508946734,\n",
       " 160.00310497824916,\n",
       " 169.03653097872714,\n",
       " 72.21656145915387,\n",
       " 212.8435030430874,\n",
       " 172.9229418087759,\n",
       " 182.76742774911938,\n",
       " 144.3091463767417,\n",
       " 97.68130366956645,\n",
       " 98.75213061302281,\n",
       " 106.69468341612391,\n",
       " 168.24446801874296,\n",
       " 170.26019365275963,\n",
       " 98.20535669069095,\n",
       " 157.62994548464934,\n",
       " 112.65903693408801,\n",
       " 82.34811171859084,\n",
       " 150.03058008700128,\n",
       " 57.64106504031913,\n",
       " 84.17166937846741,\n",
       " 171.46808418901645,\n",
       " 112.59769890501732,\n",
       " 169.2105461598527,\n",
       " 169.24044046796425,\n",
       " 150.16391663050763,\n",
       " 204.00067403291933,\n",
       " 197.59123748624285,\n",
       " 128.60546093656055,\n",
       " 177.69313503736458,\n",
       " 208.85119544705657,\n",
       " 95.59228435261039,\n",
       " 45.89573978403366,\n",
       " 131.2252618795177,\n",
       " 175.0126331466311,\n",
       " 122.55469334429591,\n",
       " 80.50124181799445,\n",
       " 98.40923610266098,\n",
       " 139.82150018660954,\n",
       " 169.23286834886574,\n",
       " 184.40911138439796,\n",
       " 165.33112855935784,\n",
       " 142.35458360311577,\n",
       " 161.92312580716322,\n",
       " 196.6419106769927,\n",
       " 168.93362601505962,\n",
       " 123.37954690016963,\n",
       " 162.72411257659957,\n",
       " 94.50383485189116,\n",
       " 156.086206003573,\n",
       " 47.47065278430487,\n",
       " 178.54819650677183,\n",
       " 172.0896989621598,\n",
       " 175.44528495476902,\n",
       " 161.40669991479433,\n",
       " 179.2109843398464,\n",
       " 84.74083350739983,\n",
       " 110.612940364999,\n",
       " 69.76454473335902]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploit_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exploit_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
